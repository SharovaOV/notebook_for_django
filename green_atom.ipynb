{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os           \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.sparse import csr_matrix # Разреженые матрицы\n",
    "from sklearn.feature_extraction.text import CountVectorizer # для мешка слов\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix \n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from sklearn.externals import joblib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA='data/aclImdb'\n",
    "PATH_TRAIN =os.path.join(PATH_DATA,'train')\n",
    "PATH_TEST=os.path.join(PATH_DATA,'test')\n",
    "RANDOM_STATE=64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader( patr_n):\n",
    "    df=pd.DataFrame(columns=('name','Text','Rat','pos'))\n",
    "    p=('neg','pos')\n",
    "    \n",
    "    for pn in tqdm(p):\n",
    "        dir_adr=os.path.join(patr_n,pn)\n",
    "        files=os.listdir(dir_adr)\n",
    "\n",
    "        for fl in files:\n",
    "            ls=[]           \n",
    "            t=fl[:-len('.txt')].split('_')           \n",
    "            f=open(os.path.join(dir_adr,fl), encoding='utf-8')\n",
    "            ls.append(t[0])\n",
    "\n",
    "            ls.append(*f)\n",
    "            ls.append(t[1])\n",
    "            ls.append(pn=='pos')\n",
    "            df.loc[df.shape[0]]=ls\n",
    "            f.close()\n",
    "            \n",
    "    df=df.sort_values('name').reindex(range(df.shape[0]),axis=0)\n",
    "    \n",
    "    return df.drop('name',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rat_split(df):\n",
    "    df=pd.merge(df,pd.get_dummies(df),right_index=True, left_index=True)\n",
    "    for i in range(1,11):\n",
    "        i_str=str(i)\n",
    "        if (i_str in df.columns)==False:\n",
    "            df[i_str]=0\n",
    "    return df.drop('Rat',axis=1)[['1','2','3','4','5','6','7','8','9','10']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redistribution(d_train,d_test):\n",
    "    X_train=d_train['Text']\n",
    "    X_test=d_test['Text']\n",
    "\n",
    "    y_pos_train=list(d_train['pos'])\n",
    "    y_pos_test=list(d_test['pos'])\n",
    "\n",
    "    y_rat_train=rat_split(d_train['Rat'])\n",
    "    y_rat_test=rat_split(d_test['Rat'])\n",
    "    \n",
    "    y_rat_train['5']=0\n",
    "    y_rat_test['5']=0\n",
    "    y_rat_train=y_rat_train\n",
    "    y_rat_test=y_rat_test\n",
    "    return X_train,X_test, y_pos_train,y_pos_test,y_rat_train,y_rat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_t,y_p):\n",
    "    expected_outputs = np.argmax(np.array(y_t), axis=1)\n",
    "    predicted_outputs= np.argmax(y_p, axis=1)\n",
    "    predicted_confusion_matrix = confusion_matrix(expected_outputs, predicted_outputs)\n",
    "    return predicted_confusion_matrix\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и оценка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:49<00:00, 54.71s/it]\n",
      "100%|██████████| 2/2 [01:47<00:00, 53.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%%time\n",
    "data_train=data_loader(PATH_TRAIN)\n",
    "\n",
    "data_test=data_loader(PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Rat</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>24904</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>You do realize that you've been watching the E...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>3</td>\n",
       "      <td>5100</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text    Rat    pos\n",
       "count                                               25000  25000  25000\n",
       "unique                                              24904      8      2\n",
       "top     You do realize that you've been watching the E...      1   True\n",
       "freq                                                    3   5100  12500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Rat</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>24801</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>5</td>\n",
       "      <td>5022</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Text    Rat    pos\n",
       "count                                               25000  25000  25000\n",
       "unique                                              24801      8      2\n",
       "top     Loved today's show!!! It was a variety and not...      1   True\n",
       "freq                                                    5   5022  12500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rat\n",
       "1     5100\n",
       "10    4732\n",
       "2     2284\n",
       "3     2420\n",
       "4     2696\n",
       "7     2496\n",
       "8     3009\n",
       "9     2263\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.groupby('Rat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rat\n",
       "1     5022\n",
       "10    4999\n",
       "2     2302\n",
       "3     2541\n",
       "4     2635\n",
       "7     2307\n",
       "8     2850\n",
       "9     2344\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.groupby('Rat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train,X_test, y_pos_train,y_pos_test,y_rat_train,y_rat_test=redistribution(data_train,data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_r_train=data_train['Rat']\n",
    "y_r_test=data_test['Rat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12500, 12500], dtype=int64), array([12500, 12500], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount( y_pos_train) ,np.bincount( y_pos_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv=CountVectorizer()\n",
    "X_train_sparse=cv.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74849"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test_sparce=cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование текстовых данных c учетом 2-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv2=CountVectorizer(ngram_range=(1,2))\n",
    "X_train_sparse2=cv2.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test_sparse2=cv2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1513832"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение на обучающую и тестовую выборку с другим соотношением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all=pd.concat((data_train,data_test),ignore_index=True,sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntests=0.3\n",
    "nTr=(1-ntests)*data_all.shape[0]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_all_train, data_all_test = train_test_split(data_all, test_size=ntests)\n",
    "X_train_n,X_test_n, y_pos_train_n,y_pos_test_n,y_rat_train_n,y_rat_test_n=redistribution(data_all_train, data_all_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv_n=CountVectorizer()\n",
    "X_train_n_sparse=cv_n.fit_transform(X_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87870"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_n.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_test_n_sparse=cv_n.transform(X_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 87870)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_n_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели для определения толерантности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей логистической регрессии и логистической регрессией со стохастическим градиентным спуском для определения настроения отзыва\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit=LogisticRegression(random_state=64,n_jobs=-1,solver='lbfgs')\n",
    "sgd_logit=SGDClassifier(max_iter=50,random_state=64,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=-1, penalty='l2', random_state=64,\n",
       "                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_sparse,y_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 480 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
       "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
       "              random_state=64, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparse,y_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit train: 0.98164 |logit test: 0.86384\n"
     ]
    }
   ],
   "source": [
    "print('logit train:',accuracy_score(y_pos_train,logit.predict(X_train_sparse)),'|logit test:',accuracy_score(y_pos_test,logit.predict(X_test_sparce)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd_logit train: 0.99468 |sgd_logit test: 0.8578\n"
     ]
    }
   ],
   "source": [
    "print('sgd_logit train:', accuracy_score(y_pos_train,sgd_logit.predict(X_train_sparse)),'|sgd_logit test:',accuracy_score(y_pos_test,sgd_logit.predict(X_test_sparce)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей логистической регрессии и логистической регрессией с градиентным спуском с би-граммами\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit2=LogisticRegression(random_state=64,n_jobs=-1,solver='lbfgs')\n",
    "sgd_logit2=SGDClassifier(max_iter=50,random_state=64,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=-1, penalty='l2', random_state=64,\n",
       "                   solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit2.fit(X_train_sparse2,y_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
       "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
       "              random_state=64, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit2.fit(X_train_sparse2,y_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit2 train: 0.99996 |logit2 test: 0.8964\n"
     ]
    }
   ],
   "source": [
    "print('logit2 train:',accuracy_score(y_pos_train,logit2.predict(X_train_sparse2)),'|logit2 test:',accuracy_score(y_pos_test,logit2.predict(X_test_sparse2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd_logit2 train: 1.0 |sgd_logit2 test: 0.8882\n"
     ]
    }
   ],
   "source": [
    "print('sgd_logit2 train:', accuracy_score(y_pos_train,sgd_logit2.predict(X_train_sparse2)),'|sgd_logit2 test:',accuracy_score(y_pos_test,sgd_logit2.predict(X_test_sparse2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### метод опорных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_l=SVC(kernel='linear',C=100,gamma=0.001,random_state=64, decision_function_shape = 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=64, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svm_l.fit(X_train_sparse,y_pos_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_svm=svm_l.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84236"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pos_test,y_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=15, p=2, metric='minkowski')\n",
    "knn.fit(X_train_sparse, y_pos_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64596"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pos_test,y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(criterion='gini', \n",
    "                              max_depth=4, \n",
    "                              random_state=64)\n",
    "tree.fit(X_train_sparse, y_pos_train)\n",
    "y_pred_tree=tree.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67916"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pos_test,y_pred_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=64, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logisticClass=LogisticRegression(random_state=64, C=100)\n",
    "logisticClass.fit(X_train_sparse,y_r_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred_le=logisticClass.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33748"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_r_test,y_pred_le)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=64, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logisticClass2=LogisticRegression(random_state=64, C=100)\n",
    "logisticClass2.fit(X_train_sparse2,y_r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_le2=logisticClass2.predict(X_test_sparse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40472"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_r_test,y_pred_le2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=SVC(kernel='linear',C=100000,gamma=0.001,random_state=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=64, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm.fit(X_train_sparse,y_r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ysvg_pred=svm.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36108"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_r_test,ysvg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_r = DecisionTreeClassifier(criterion='gini', \n",
    "                              max_depth=4, \n",
    "                              random_state=64)\n",
    "tree_r.fit(X_train_sparse, y_rat_train)\n",
    "y_pred_tree_r=tree_r.predict(X_test_sparce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05248"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_rat_test,y_pred_tree_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(X_tr,y_tr,X_val,y_val,ln=1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input - Layer\n",
    "    model.add(layers.Dense(50, activation = \"relu\", input_shape=(X_tr.shape[1],)))\n",
    "    \n",
    "    # Hidden - Layers\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(50, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(50, activation = \"relu\"))\n",
    "        # Output- Layer\n",
    "    if(ln==1):\n",
    "        model.add(layers.Dense(ln, activation = \"sigmoid\"))\n",
    "    else:\n",
    "        model.add(layers.Dense(ln, activation = \"softmax\"))\n",
    "    model.summary()\n",
    "  \n",
    "    loss=\"binary_crossentropy\" if ln==1 else \"categorical_crossentropy\"\n",
    "    optimizer = \"adam\" if ln==1 else \"sgd\"\n",
    "    epoh=800 if ln==1 else 600\n",
    "    epochs=2 if ln==1 else 45\n",
    "    metrics=[\"accuracy\"] \n",
    "    model.compile(\n",
    "     optimizer = optimizer,\n",
    "     loss = loss,\n",
    "     metrics = metrics\n",
    "    )\n",
    "    results = model.fit(\n",
    "     X_tr, y_tr,\n",
    "     epochs= epochs,\n",
    "     batch_size = epoh,\n",
    "     validation_data = (X_val, y_val)\n",
    "    )\n",
    "    \n",
    "    print(\"Test-Accuracy:\", np.mean(results.history[\"val_accuracy\"]))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение настроения отзыва"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                4412950   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 4,418,101\n",
      "Trainable params: 4,418,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "35000/35000 [==============================] - ETA: 1:44 - loss: 0.6932 - accuracy: 0.51 - ETA: 1:06 - loss: 0.6930 - accuracy: 0.51 - ETA: 51s - loss: 0.6908 - accuracy: 0.5458 - ETA: 43s - loss: 0.6876 - accuracy: 0.571 - ETA: 37s - loss: 0.6845 - accuracy: 0.588 - ETA: 34s - loss: 0.6792 - accuracy: 0.610 - ETA: 32s - loss: 0.6736 - accuracy: 0.629 - ETA: 30s - loss: 0.6702 - accuracy: 0.640 - ETA: 28s - loss: 0.6646 - accuracy: 0.653 - ETA: 26s - loss: 0.6586 - accuracy: 0.662 - ETA: 25s - loss: 0.6512 - accuracy: 0.672 - ETA: 24s - loss: 0.6444 - accuracy: 0.680 - ETA: 23s - loss: 0.6383 - accuracy: 0.687 - ETA: 21s - loss: 0.6316 - accuracy: 0.694 - ETA: 20s - loss: 0.6252 - accuracy: 0.701 - ETA: 19s - loss: 0.6187 - accuracy: 0.707 - ETA: 18s - loss: 0.6113 - accuracy: 0.714 - ETA: 17s - loss: 0.6054 - accuracy: 0.719 - ETA: 17s - loss: 0.5992 - accuracy: 0.723 - ETA: 16s - loss: 0.5926 - accuracy: 0.728 - ETA: 15s - loss: 0.5855 - accuracy: 0.732 - ETA: 14s - loss: 0.5799 - accuracy: 0.736 - ETA: 13s - loss: 0.5732 - accuracy: 0.741 - ETA: 12s - loss: 0.5669 - accuracy: 0.745 - ETA: 12s - loss: 0.5609 - accuracy: 0.749 - ETA: 11s - loss: 0.5560 - accuracy: 0.752 - ETA: 10s - loss: 0.5501 - accuracy: 0.756 - ETA: 10s - loss: 0.5441 - accuracy: 0.759 - ETA: 9s - loss: 0.5382 - accuracy: 0.763 - ETA: 8s - loss: 0.5335 - accuracy: 0.76 - ETA: 8s - loss: 0.5272 - accuracy: 0.77 - ETA: 7s - loss: 0.5216 - accuracy: 0.77 - ETA: 6s - loss: 0.5172 - accuracy: 0.77 - ETA: 6s - loss: 0.5130 - accuracy: 0.77 - ETA: 5s - loss: 0.5087 - accuracy: 0.78 - ETA: 4s - loss: 0.5037 - accuracy: 0.78 - ETA: 4s - loss: 0.4992 - accuracy: 0.78 - ETA: 3s - loss: 0.4950 - accuracy: 0.78 - ETA: 2s - loss: 0.4902 - accuracy: 0.79 - ETA: 2s - loss: 0.4856 - accuracy: 0.79 - ETA: 1s - loss: 0.4821 - accuracy: 0.79 - ETA: 1s - loss: 0.4785 - accuracy: 0.79 - ETA: 0s - loss: 0.4752 - accuracy: 0.80 - 36s 1ms/step - loss: 0.4731 - accuracy: 0.8017 - val_loss: 0.2931 - val_accuracy: 0.8933\n",
      "Epoch 2/2\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 0.2358 - accuracy: 0.935 - ETA: 23s - loss: 0.2331 - accuracy: 0.932 - ETA: 23s - loss: 0.2380 - accuracy: 0.925 - ETA: 22s - loss: 0.2385 - accuracy: 0.925 - ETA: 21s - loss: 0.2346 - accuracy: 0.925 - ETA: 21s - loss: 0.2287 - accuracy: 0.928 - ETA: 20s - loss: 0.2297 - accuracy: 0.926 - ETA: 19s - loss: 0.2285 - accuracy: 0.926 - ETA: 19s - loss: 0.2267 - accuracy: 0.927 - ETA: 18s - loss: 0.2265 - accuracy: 0.928 - ETA: 18s - loss: 0.2269 - accuracy: 0.926 - ETA: 17s - loss: 0.2260 - accuracy: 0.926 - ETA: 16s - loss: 0.2247 - accuracy: 0.926 - ETA: 16s - loss: 0.2251 - accuracy: 0.925 - ETA: 15s - loss: 0.2239 - accuracy: 0.926 - ETA: 15s - loss: 0.2246 - accuracy: 0.926 - ETA: 14s - loss: 0.2227 - accuracy: 0.926 - ETA: 14s - loss: 0.2227 - accuracy: 0.925 - ETA: 13s - loss: 0.2225 - accuracy: 0.926 - ETA: 12s - loss: 0.2217 - accuracy: 0.925 - ETA: 12s - loss: 0.2204 - accuracy: 0.926 - ETA: 11s - loss: 0.2203 - accuracy: 0.926 - ETA: 11s - loss: 0.2206 - accuracy: 0.926 - ETA: 10s - loss: 0.2189 - accuracy: 0.926 - ETA: 10s - loss: 0.2177 - accuracy: 0.926 - ETA: 9s - loss: 0.2161 - accuracy: 0.926 - ETA: 9s - loss: 0.2159 - accuracy: 0.92 - ETA: 8s - loss: 0.2150 - accuracy: 0.92 - ETA: 7s - loss: 0.2144 - accuracy: 0.92 - ETA: 7s - loss: 0.2150 - accuracy: 0.92 - ETA: 6s - loss: 0.2139 - accuracy: 0.92 - ETA: 6s - loss: 0.2139 - accuracy: 0.92 - ETA: 5s - loss: 0.2133 - accuracy: 0.92 - ETA: 5s - loss: 0.2134 - accuracy: 0.92 - ETA: 4s - loss: 0.2136 - accuracy: 0.92 - ETA: 4s - loss: 0.2133 - accuracy: 0.92 - ETA: 3s - loss: 0.2139 - accuracy: 0.92 - ETA: 3s - loss: 0.2135 - accuracy: 0.92 - ETA: 2s - loss: 0.2128 - accuracy: 0.92 - ETA: 2s - loss: 0.2128 - accuracy: 0.92 - ETA: 1s - loss: 0.2123 - accuracy: 0.92 - ETA: 0s - loss: 0.2116 - accuracy: 0.92 - ETA: 0s - loss: 0.2112 - accuracy: 0.92 - 31s 895us/step - loss: 0.2112 - accuracy: 0.9267 - val_loss: 0.2807 - val_accuracy: 0.8951\n",
      "Test-Accuracy: 0.8941666781902313\n"
     ]
    }
   ],
   "source": [
    "model_pos=set_model(X_train_n_sparse,y_pos_train_n,X_test_n_sparse,y_pos_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_m=model_pos.predict(X_test_n_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                4393550   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 4,399,160\n",
      "Trainable params: 4,399,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/45\n",
      "35000/35000 [==============================] - ETA: 27s - loss: 2.3260 - accuracy: 0.071 - ETA: 23s - loss: 2.3221 - accuracy: 0.074 - ETA: 21s - loss: 2.3171 - accuracy: 0.075 - ETA: 20s - loss: 2.3137 - accuracy: 0.083 - ETA: 19s - loss: 2.3111 - accuracy: 0.091 - ETA: 18s - loss: 2.3093 - accuracy: 0.091 - ETA: 18s - loss: 2.3076 - accuracy: 0.096 - ETA: 17s - loss: 2.3060 - accuracy: 0.102 - ETA: 17s - loss: 2.3049 - accuracy: 0.108 - ETA: 17s - loss: 2.3035 - accuracy: 0.111 - ETA: 16s - loss: 2.3023 - accuracy: 0.115 - ETA: 16s - loss: 2.3009 - accuracy: 0.117 - ETA: 15s - loss: 2.2999 - accuracy: 0.121 - ETA: 15s - loss: 2.2991 - accuracy: 0.125 - ETA: 15s - loss: 2.2985 - accuracy: 0.127 - ETA: 14s - loss: 2.2977 - accuracy: 0.129 - ETA: 14s - loss: 2.2968 - accuracy: 0.131 - ETA: 14s - loss: 2.2961 - accuracy: 0.133 - ETA: 13s - loss: 2.2959 - accuracy: 0.134 - ETA: 13s - loss: 2.2948 - accuracy: 0.137 - ETA: 13s - loss: 2.2945 - accuracy: 0.139 - ETA: 12s - loss: 2.2935 - accuracy: 0.142 - ETA: 12s - loss: 2.2926 - accuracy: 0.143 - ETA: 12s - loss: 2.2919 - accuracy: 0.145 - ETA: 11s - loss: 2.2910 - accuracy: 0.146 - ETA: 11s - loss: 2.2904 - accuracy: 0.149 - ETA: 11s - loss: 2.2898 - accuracy: 0.149 - ETA: 10s - loss: 2.2888 - accuracy: 0.152 - ETA: 10s - loss: 2.2881 - accuracy: 0.153 - ETA: 10s - loss: 2.2873 - accuracy: 0.154 - ETA: 9s - loss: 2.2864 - accuracy: 0.155 - ETA: 9s - loss: 2.2858 - accuracy: 0.15 - ETA: 9s - loss: 2.2849 - accuracy: 0.15 - ETA: 8s - loss: 2.2839 - accuracy: 0.15 - ETA: 8s - loss: 2.2833 - accuracy: 0.16 - ETA: 7s - loss: 2.2829 - accuracy: 0.16 - ETA: 7s - loss: 2.2823 - accuracy: 0.16 - ETA: 7s - loss: 2.2817 - accuracy: 0.16 - ETA: 6s - loss: 2.2807 - accuracy: 0.16 - ETA: 6s - loss: 2.2799 - accuracy: 0.16 - ETA: 6s - loss: 2.2790 - accuracy: 0.16 - ETA: 5s - loss: 2.2782 - accuracy: 0.16 - ETA: 5s - loss: 2.2772 - accuracy: 0.16 - ETA: 5s - loss: 2.2765 - accuracy: 0.16 - ETA: 4s - loss: 2.2760 - accuracy: 0.16 - ETA: 4s - loss: 2.2752 - accuracy: 0.16 - ETA: 4s - loss: 2.2751 - accuracy: 0.16 - ETA: 3s - loss: 2.2745 - accuracy: 0.17 - ETA: 3s - loss: 2.2741 - accuracy: 0.17 - ETA: 3s - loss: 2.2736 - accuracy: 0.17 - ETA: 2s - loss: 2.2727 - accuracy: 0.17 - ETA: 2s - loss: 2.2722 - accuracy: 0.17 - ETA: 1s - loss: 2.2715 - accuracy: 0.17 - ETA: 1s - loss: 2.2712 - accuracy: 0.17 - ETA: 1s - loss: 2.2704 - accuracy: 0.17 - ETA: 0s - loss: 2.2699 - accuracy: 0.17 - ETA: 0s - loss: 2.2693 - accuracy: 0.17 - ETA: 0s - loss: 2.2687 - accuracy: 0.17 - 31s 896us/step - loss: 2.2684 - accuracy: 0.1774 - val_loss: 2.2105 - val_accuracy: 0.2107\n",
      "Epoch 2/45\n",
      "35000/35000 [==============================] - ETA: 21s - loss: 2.2380 - accuracy: 0.186 - ETA: 21s - loss: 2.2375 - accuracy: 0.195 - ETA: 20s - loss: 2.2297 - accuracy: 0.206 - ETA: 20s - loss: 2.2308 - accuracy: 0.200 - ETA: 20s - loss: 2.2306 - accuracy: 0.200 - ETA: 19s - loss: 2.2267 - accuracy: 0.205 - ETA: 19s - loss: 2.2271 - accuracy: 0.206 - ETA: 19s - loss: 2.2265 - accuracy: 0.204 - ETA: 18s - loss: 2.2249 - accuracy: 0.207 - ETA: 18s - loss: 2.2241 - accuracy: 0.208 - ETA: 18s - loss: 2.2242 - accuracy: 0.205 - ETA: 17s - loss: 2.2243 - accuracy: 0.205 - ETA: 17s - loss: 2.2234 - accuracy: 0.204 - ETA: 16s - loss: 2.2230 - accuracy: 0.205 - ETA: 16s - loss: 2.2216 - accuracy: 0.206 - ETA: 16s - loss: 2.2220 - accuracy: 0.206 - ETA: 16s - loss: 2.2209 - accuracy: 0.205 - ETA: 15s - loss: 2.2197 - accuracy: 0.206 - ETA: 15s - loss: 2.2194 - accuracy: 0.208 - ETA: 15s - loss: 2.2191 - accuracy: 0.207 - ETA: 15s - loss: 2.2188 - accuracy: 0.206 - ETA: 14s - loss: 2.2180 - accuracy: 0.208 - ETA: 14s - loss: 2.2170 - accuracy: 0.208 - ETA: 14s - loss: 2.2159 - accuracy: 0.209 - ETA: 13s - loss: 2.2161 - accuracy: 0.208 - ETA: 13s - loss: 2.2146 - accuracy: 0.209 - ETA: 13s - loss: 2.2139 - accuracy: 0.209 - ETA: 12s - loss: 2.2127 - accuracy: 0.211 - ETA: 12s - loss: 2.2119 - accuracy: 0.210 - ETA: 11s - loss: 2.2108 - accuracy: 0.211 - ETA: 11s - loss: 2.2100 - accuracy: 0.211 - ETA: 11s - loss: 2.2097 - accuracy: 0.212 - ETA: 10s - loss: 2.2096 - accuracy: 0.211 - ETA: 10s - loss: 2.2088 - accuracy: 0.212 - ETA: 9s - loss: 2.2079 - accuracy: 0.213 - ETA: 9s - loss: 2.2071 - accuracy: 0.21 - ETA: 9s - loss: 2.2061 - accuracy: 0.21 - ETA: 8s - loss: 2.2054 - accuracy: 0.21 - ETA: 8s - loss: 2.2049 - accuracy: 0.21 - ETA: 7s - loss: 2.2046 - accuracy: 0.21 - ETA: 7s - loss: 2.2045 - accuracy: 0.21 - ETA: 7s - loss: 2.2040 - accuracy: 0.21 - ETA: 6s - loss: 2.2033 - accuracy: 0.21 - ETA: 6s - loss: 2.2030 - accuracy: 0.21 - ETA: 5s - loss: 2.2025 - accuracy: 0.21 - ETA: 5s - loss: 2.2017 - accuracy: 0.21 - ETA: 4s - loss: 2.2013 - accuracy: 0.21 - ETA: 4s - loss: 2.2007 - accuracy: 0.21 - ETA: 4s - loss: 2.2000 - accuracy: 0.21 - ETA: 3s - loss: 2.1994 - accuracy: 0.21 - ETA: 3s - loss: 2.1991 - accuracy: 0.21 - ETA: 2s - loss: 2.1984 - accuracy: 0.21 - ETA: 2s - loss: 2.1976 - accuracy: 0.21 - ETA: 1s - loss: 2.1972 - accuracy: 0.21 - ETA: 1s - loss: 2.1969 - accuracy: 0.21 - ETA: 1s - loss: 2.1961 - accuracy: 0.21 - ETA: 0s - loss: 2.1960 - accuracy: 0.21 - ETA: 0s - loss: 2.1950 - accuracy: 0.21 - 36s 1ms/step - loss: 2.1948 - accuracy: 0.2181 - val_loss: 2.1281 - val_accuracy: 0.2237\n",
      "Epoch 3/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 2.1740 - accuracy: 0.221 - ETA: 23s - loss: 2.1613 - accuracy: 0.235 - ETA: 22s - loss: 2.1598 - accuracy: 0.228 - ETA: 22s - loss: 2.1560 - accuracy: 0.221 - ETA: 22s - loss: 2.1540 - accuracy: 0.227 - ETA: 21s - loss: 2.1509 - accuracy: 0.233 - ETA: 21s - loss: 2.1517 - accuracy: 0.232 - ETA: 21s - loss: 2.1524 - accuracy: 0.229 - ETA: 21s - loss: 2.1508 - accuracy: 0.228 - ETA: 21s - loss: 2.1487 - accuracy: 0.233 - ETA: 20s - loss: 2.1486 - accuracy: 0.231 - ETA: 20s - loss: 2.1484 - accuracy: 0.232 - ETA: 19s - loss: 2.1489 - accuracy: 0.233 - ETA: 19s - loss: 2.1493 - accuracy: 0.233 - ETA: 19s - loss: 2.1511 - accuracy: 0.231 - ETA: 18s - loss: 2.1494 - accuracy: 0.233 - ETA: 18s - loss: 2.1505 - accuracy: 0.232 - ETA: 18s - loss: 2.1509 - accuracy: 0.231 - ETA: 17s - loss: 2.1497 - accuracy: 0.232 - ETA: 17s - loss: 2.1487 - accuracy: 0.233 - ETA: 16s - loss: 2.1474 - accuracy: 0.234 - ETA: 16s - loss: 2.1481 - accuracy: 0.233 - ETA: 15s - loss: 2.1480 - accuracy: 0.232 - ETA: 15s - loss: 2.1482 - accuracy: 0.232 - ETA: 15s - loss: 2.1490 - accuracy: 0.231 - ETA: 14s - loss: 2.1485 - accuracy: 0.232 - ETA: 14s - loss: 2.1481 - accuracy: 0.233 - ETA: 13s - loss: 2.1486 - accuracy: 0.232 - ETA: 13s - loss: 2.1482 - accuracy: 0.232 - ETA: 12s - loss: 2.1478 - accuracy: 0.232 - ETA: 12s - loss: 2.1467 - accuracy: 0.232 - ETA: 12s - loss: 2.1455 - accuracy: 0.232 - ETA: 11s - loss: 2.1451 - accuracy: 0.233 - ETA: 11s - loss: 2.1444 - accuracy: 0.233 - ETA: 10s - loss: 2.1441 - accuracy: 0.232 - ETA: 10s - loss: 2.1436 - accuracy: 0.232 - ETA: 9s - loss: 2.1434 - accuracy: 0.233 - ETA: 9s - loss: 2.1426 - accuracy: 0.23 - ETA: 8s - loss: 2.1422 - accuracy: 0.23 - ETA: 8s - loss: 2.1419 - accuracy: 0.23 - ETA: 7s - loss: 2.1413 - accuracy: 0.23 - ETA: 7s - loss: 2.1408 - accuracy: 0.23 - ETA: 7s - loss: 2.1407 - accuracy: 0.23 - ETA: 6s - loss: 2.1405 - accuracy: 0.23 - ETA: 6s - loss: 2.1407 - accuracy: 0.23 - ETA: 5s - loss: 2.1404 - accuracy: 0.23 - ETA: 5s - loss: 2.1394 - accuracy: 0.23 - ETA: 4s - loss: 2.1399 - accuracy: 0.23 - ETA: 4s - loss: 2.1394 - accuracy: 0.23 - ETA: 3s - loss: 2.1391 - accuracy: 0.23 - ETA: 3s - loss: 2.1392 - accuracy: 0.23 - ETA: 2s - loss: 2.1390 - accuracy: 0.23 - ETA: 2s - loss: 2.1386 - accuracy: 0.23 - ETA: 1s - loss: 2.1382 - accuracy: 0.23 - ETA: 1s - loss: 2.1375 - accuracy: 0.23 - ETA: 1s - loss: 2.1372 - accuracy: 0.23 - ETA: 0s - loss: 2.1366 - accuracy: 0.23 - ETA: 0s - loss: 2.1362 - accuracy: 0.23 - 35s 1ms/step - loss: 2.1361 - accuracy: 0.2341 - val_loss: 2.0764 - val_accuracy: 0.2815\n",
      "Epoch 4/45\n",
      "35000/35000 [==============================] - ETA: 20s - loss: 2.1170 - accuracy: 0.231 - ETA: 22s - loss: 2.1063 - accuracy: 0.257 - ETA: 22s - loss: 2.1030 - accuracy: 0.250 - ETA: 21s - loss: 2.1018 - accuracy: 0.260 - ETA: 20s - loss: 2.1038 - accuracy: 0.255 - ETA: 20s - loss: 2.1014 - accuracy: 0.256 - ETA: 19s - loss: 2.1010 - accuracy: 0.254 - ETA: 19s - loss: 2.1035 - accuracy: 0.253 - ETA: 18s - loss: 2.1036 - accuracy: 0.254 - ETA: 18s - loss: 2.1059 - accuracy: 0.253 - ETA: 17s - loss: 2.1067 - accuracy: 0.250 - ETA: 17s - loss: 2.1066 - accuracy: 0.249 - ETA: 17s - loss: 2.1070 - accuracy: 0.248 - ETA: 16s - loss: 2.1061 - accuracy: 0.248 - ETA: 16s - loss: 2.1041 - accuracy: 0.248 - ETA: 16s - loss: 2.1026 - accuracy: 0.249 - ETA: 15s - loss: 2.1035 - accuracy: 0.249 - ETA: 15s - loss: 2.1042 - accuracy: 0.248 - ETA: 14s - loss: 2.1047 - accuracy: 0.246 - ETA: 14s - loss: 2.1055 - accuracy: 0.246 - ETA: 14s - loss: 2.1061 - accuracy: 0.246 - ETA: 13s - loss: 2.1057 - accuracy: 0.247 - ETA: 13s - loss: 2.1047 - accuracy: 0.248 - ETA: 13s - loss: 2.1032 - accuracy: 0.250 - ETA: 12s - loss: 2.1014 - accuracy: 0.250 - ETA: 12s - loss: 2.1020 - accuracy: 0.250 - ETA: 11s - loss: 2.1008 - accuracy: 0.251 - ETA: 11s - loss: 2.0995 - accuracy: 0.252 - ETA: 11s - loss: 2.1006 - accuracy: 0.251 - ETA: 10s - loss: 2.1000 - accuracy: 0.251 - ETA: 10s - loss: 2.0993 - accuracy: 0.250 - ETA: 10s - loss: 2.0992 - accuracy: 0.250 - ETA: 9s - loss: 2.0990 - accuracy: 0.250 - ETA: 9s - loss: 2.0979 - accuracy: 0.25 - ETA: 8s - loss: 2.0974 - accuracy: 0.25 - ETA: 8s - loss: 2.0968 - accuracy: 0.25 - ETA: 8s - loss: 2.0971 - accuracy: 0.25 - ETA: 7s - loss: 2.0968 - accuracy: 0.25 - ETA: 7s - loss: 2.0964 - accuracy: 0.25 - ETA: 7s - loss: 2.0963 - accuracy: 0.25 - ETA: 6s - loss: 2.0957 - accuracy: 0.25 - ETA: 6s - loss: 2.0953 - accuracy: 0.25 - ETA: 5s - loss: 2.0948 - accuracy: 0.25 - ETA: 5s - loss: 2.0941 - accuracy: 0.25 - ETA: 5s - loss: 2.0933 - accuracy: 0.25 - ETA: 4s - loss: 2.0930 - accuracy: 0.25 - ETA: 4s - loss: 2.0924 - accuracy: 0.25 - ETA: 3s - loss: 2.0924 - accuracy: 0.25 - ETA: 3s - loss: 2.0917 - accuracy: 0.25 - ETA: 3s - loss: 2.0916 - accuracy: 0.25 - ETA: 2s - loss: 2.0914 - accuracy: 0.25 - ETA: 2s - loss: 2.0909 - accuracy: 0.25 - ETA: 2s - loss: 2.0905 - accuracy: 0.25 - ETA: 1s - loss: 2.0905 - accuracy: 0.25 - ETA: 1s - loss: 2.0902 - accuracy: 0.25 - ETA: 0s - loss: 2.0896 - accuracy: 0.25 - ETA: 0s - loss: 2.0893 - accuracy: 0.25 - ETA: 0s - loss: 2.0891 - accuracy: 0.25 - 31s 876us/step - loss: 2.0889 - accuracy: 0.2555 - val_loss: 2.0337 - val_accuracy: 0.2897\n",
      "Epoch 5/45\n",
      "35000/35000 [==============================] - ETA: 19s - loss: 2.0565 - accuracy: 0.268 - ETA: 19s - loss: 2.0592 - accuracy: 0.260 - ETA: 19s - loss: 2.0603 - accuracy: 0.257 - ETA: 20s - loss: 2.0628 - accuracy: 0.251 - ETA: 20s - loss: 2.0604 - accuracy: 0.259 - ETA: 20s - loss: 2.0601 - accuracy: 0.258 - ETA: 20s - loss: 2.0659 - accuracy: 0.256 - ETA: 19s - loss: 2.0670 - accuracy: 0.255 - ETA: 19s - loss: 2.0658 - accuracy: 0.255 - ETA: 19s - loss: 2.0675 - accuracy: 0.256 - ETA: 18s - loss: 2.0662 - accuracy: 0.258 - ETA: 18s - loss: 2.0651 - accuracy: 0.261 - ETA: 17s - loss: 2.0645 - accuracy: 0.262 - ETA: 17s - loss: 2.0634 - accuracy: 0.263 - ETA: 16s - loss: 2.0625 - accuracy: 0.263 - ETA: 16s - loss: 2.0629 - accuracy: 0.264 - ETA: 16s - loss: 2.0616 - accuracy: 0.263 - ETA: 16s - loss: 2.0622 - accuracy: 0.264 - ETA: 15s - loss: 2.0625 - accuracy: 0.263 - ETA: 15s - loss: 2.0611 - accuracy: 0.265 - ETA: 15s - loss: 2.0611 - accuracy: 0.266 - ETA: 14s - loss: 2.0607 - accuracy: 0.265 - ETA: 14s - loss: 2.0608 - accuracy: 0.264 - ETA: 13s - loss: 2.0597 - accuracy: 0.265 - ETA: 13s - loss: 2.0599 - accuracy: 0.264 - ETA: 13s - loss: 2.0607 - accuracy: 0.263 - ETA: 12s - loss: 2.0615 - accuracy: 0.262 - ETA: 12s - loss: 2.0612 - accuracy: 0.261 - ETA: 11s - loss: 2.0607 - accuracy: 0.261 - ETA: 11s - loss: 2.0615 - accuracy: 0.261 - ETA: 11s - loss: 2.0611 - accuracy: 0.261 - ETA: 10s - loss: 2.0617 - accuracy: 0.261 - ETA: 10s - loss: 2.0614 - accuracy: 0.261 - ETA: 10s - loss: 2.0607 - accuracy: 0.262 - ETA: 9s - loss: 2.0603 - accuracy: 0.262 - ETA: 9s - loss: 2.0601 - accuracy: 0.26 - ETA: 8s - loss: 2.0600 - accuracy: 0.26 - ETA: 8s - loss: 2.0596 - accuracy: 0.26 - ETA: 8s - loss: 2.0599 - accuracy: 0.26 - ETA: 7s - loss: 2.0591 - accuracy: 0.26 - ETA: 7s - loss: 2.0586 - accuracy: 0.26 - ETA: 6s - loss: 2.0587 - accuracy: 0.26 - ETA: 6s - loss: 2.0592 - accuracy: 0.26 - ETA: 5s - loss: 2.0589 - accuracy: 0.26 - ETA: 5s - loss: 2.0588 - accuracy: 0.26 - ETA: 5s - loss: 2.0577 - accuracy: 0.26 - ETA: 4s - loss: 2.0571 - accuracy: 0.26 - ETA: 4s - loss: 2.0564 - accuracy: 0.26 - ETA: 3s - loss: 2.0561 - accuracy: 0.26 - ETA: 3s - loss: 2.0560 - accuracy: 0.26 - ETA: 3s - loss: 2.0558 - accuracy: 0.26 - ETA: 2s - loss: 2.0555 - accuracy: 0.26 - ETA: 2s - loss: 2.0555 - accuracy: 0.26 - ETA: 1s - loss: 2.0550 - accuracy: 0.26 - ETA: 1s - loss: 2.0546 - accuracy: 0.26 - ETA: 0s - loss: 2.0541 - accuracy: 0.26 - ETA: 0s - loss: 2.0540 - accuracy: 0.26 - ETA: 0s - loss: 2.0538 - accuracy: 0.26 - 33s 947us/step - loss: 2.0536 - accuracy: 0.2668 - val_loss: 2.0027 - val_accuracy: 0.2903\n",
      "Epoch 6/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 2.0966 - accuracy: 0.223 - ETA: 23s - loss: 2.0615 - accuracy: 0.248 - ETA: 23s - loss: 2.0535 - accuracy: 0.251 - ETA: 23s - loss: 2.0525 - accuracy: 0.256 - ETA: 23s - loss: 2.0511 - accuracy: 0.257 - ETA: 22s - loss: 2.0517 - accuracy: 0.256 - ETA: 22s - loss: 2.0484 - accuracy: 0.260 - ETA: 21s - loss: 2.0476 - accuracy: 0.256 - ETA: 21s - loss: 2.0488 - accuracy: 0.257 - ETA: 21s - loss: 2.0446 - accuracy: 0.261 - ETA: 20s - loss: 2.0439 - accuracy: 0.261 - ETA: 20s - loss: 2.0441 - accuracy: 0.262 - ETA: 19s - loss: 2.0463 - accuracy: 0.261 - ETA: 19s - loss: 2.0472 - accuracy: 0.261 - ETA: 18s - loss: 2.0468 - accuracy: 0.259 - ETA: 18s - loss: 2.0449 - accuracy: 0.261 - ETA: 17s - loss: 2.0433 - accuracy: 0.263 - ETA: 17s - loss: 2.0435 - accuracy: 0.263 - ETA: 17s - loss: 2.0419 - accuracy: 0.265 - ETA: 16s - loss: 2.0409 - accuracy: 0.265 - ETA: 16s - loss: 2.0396 - accuracy: 0.267 - ETA: 15s - loss: 2.0394 - accuracy: 0.267 - ETA: 15s - loss: 2.0396 - accuracy: 0.267 - ETA: 14s - loss: 2.0385 - accuracy: 0.267 - ETA: 14s - loss: 2.0375 - accuracy: 0.267 - ETA: 13s - loss: 2.0377 - accuracy: 0.267 - ETA: 13s - loss: 2.0371 - accuracy: 0.268 - ETA: 12s - loss: 2.0370 - accuracy: 0.268 - ETA: 12s - loss: 2.0362 - accuracy: 0.269 - ETA: 12s - loss: 2.0362 - accuracy: 0.268 - ETA: 11s - loss: 2.0351 - accuracy: 0.269 - ETA: 11s - loss: 2.0338 - accuracy: 0.269 - ETA: 10s - loss: 2.0337 - accuracy: 0.269 - ETA: 10s - loss: 2.0333 - accuracy: 0.269 - ETA: 9s - loss: 2.0327 - accuracy: 0.269 - ETA: 9s - loss: 2.0325 - accuracy: 0.26 - ETA: 9s - loss: 2.0326 - accuracy: 0.26 - ETA: 8s - loss: 2.0323 - accuracy: 0.26 - ETA: 8s - loss: 2.0315 - accuracy: 0.27 - ETA: 7s - loss: 2.0305 - accuracy: 0.27 - ETA: 7s - loss: 2.0303 - accuracy: 0.27 - ETA: 6s - loss: 2.0301 - accuracy: 0.27 - ETA: 6s - loss: 2.0294 - accuracy: 0.27 - ETA: 6s - loss: 2.0290 - accuracy: 0.27 - ETA: 5s - loss: 2.0295 - accuracy: 0.27 - ETA: 5s - loss: 2.0295 - accuracy: 0.27 - ETA: 4s - loss: 2.0289 - accuracy: 0.27 - ETA: 4s - loss: 2.0278 - accuracy: 0.27 - ETA: 3s - loss: 2.0277 - accuracy: 0.27 - ETA: 3s - loss: 2.0271 - accuracy: 0.27 - ETA: 3s - loss: 2.0277 - accuracy: 0.27 - ETA: 2s - loss: 2.0271 - accuracy: 0.27 - ETA: 2s - loss: 2.0262 - accuracy: 0.27 - ETA: 1s - loss: 2.0261 - accuracy: 0.27 - ETA: 1s - loss: 2.0252 - accuracy: 0.27 - ETA: 0s - loss: 2.0256 - accuracy: 0.27 - ETA: 0s - loss: 2.0251 - accuracy: 0.27 - ETA: 0s - loss: 2.0246 - accuracy: 0.27 - 33s 949us/step - loss: 2.0245 - accuracy: 0.2735 - val_loss: 1.9768 - val_accuracy: 0.2997\n",
      "Epoch 7/45\n",
      "35000/35000 [==============================] - ETA: 21s - loss: 2.0363 - accuracy: 0.245 - ETA: 22s - loss: 2.0257 - accuracy: 0.256 - ETA: 22s - loss: 2.0245 - accuracy: 0.266 - ETA: 22s - loss: 2.0158 - accuracy: 0.270 - ETA: 21s - loss: 2.0173 - accuracy: 0.267 - ETA: 21s - loss: 2.0173 - accuracy: 0.267 - ETA: 20s - loss: 2.0125 - accuracy: 0.270 - ETA: 20s - loss: 2.0109 - accuracy: 0.273 - ETA: 20s - loss: 2.0093 - accuracy: 0.275 - ETA: 19s - loss: 2.0092 - accuracy: 0.277 - ETA: 19s - loss: 2.0069 - accuracy: 0.277 - ETA: 18s - loss: 2.0053 - accuracy: 0.279 - ETA: 18s - loss: 2.0025 - accuracy: 0.281 - ETA: 18s - loss: 2.0038 - accuracy: 0.280 - ETA: 17s - loss: 2.0048 - accuracy: 0.279 - ETA: 17s - loss: 2.0017 - accuracy: 0.282 - ETA: 16s - loss: 2.0014 - accuracy: 0.282 - ETA: 16s - loss: 2.0022 - accuracy: 0.282 - ETA: 16s - loss: 2.0022 - accuracy: 0.282 - ETA: 15s - loss: 2.0008 - accuracy: 0.284 - ETA: 15s - loss: 1.9998 - accuracy: 0.284 - ETA: 14s - loss: 1.9991 - accuracy: 0.284 - ETA: 14s - loss: 2.0002 - accuracy: 0.283 - ETA: 14s - loss: 2.0011 - accuracy: 0.283 - ETA: 13s - loss: 2.0003 - accuracy: 0.283 - ETA: 13s - loss: 2.0003 - accuracy: 0.283 - ETA: 12s - loss: 2.0000 - accuracy: 0.284 - ETA: 12s - loss: 2.0000 - accuracy: 0.283 - ETA: 11s - loss: 1.9996 - accuracy: 0.283 - ETA: 11s - loss: 1.9999 - accuracy: 0.283 - ETA: 11s - loss: 1.9979 - accuracy: 0.284 - ETA: 10s - loss: 1.9988 - accuracy: 0.284 - ETA: 10s - loss: 1.9987 - accuracy: 0.284 - ETA: 9s - loss: 1.9982 - accuracy: 0.285 - ETA: 9s - loss: 1.9982 - accuracy: 0.28 - ETA: 9s - loss: 1.9981 - accuracy: 0.28 - ETA: 8s - loss: 1.9978 - accuracy: 0.28 - ETA: 8s - loss: 1.9979 - accuracy: 0.28 - ETA: 7s - loss: 1.9982 - accuracy: 0.28 - ETA: 7s - loss: 1.9979 - accuracy: 0.28 - ETA: 7s - loss: 1.9973 - accuracy: 0.28 - ETA: 6s - loss: 1.9968 - accuracy: 0.28 - ETA: 6s - loss: 1.9963 - accuracy: 0.28 - ETA: 5s - loss: 1.9965 - accuracy: 0.28 - ETA: 5s - loss: 1.9960 - accuracy: 0.28 - ETA: 5s - loss: 1.9962 - accuracy: 0.28 - ETA: 4s - loss: 1.9965 - accuracy: 0.28 - ETA: 4s - loss: 1.9962 - accuracy: 0.28 - ETA: 3s - loss: 1.9964 - accuracy: 0.28 - ETA: 3s - loss: 1.9962 - accuracy: 0.28 - ETA: 3s - loss: 1.9962 - accuracy: 0.28 - ETA: 2s - loss: 1.9960 - accuracy: 0.28 - ETA: 2s - loss: 1.9961 - accuracy: 0.28 - ETA: 1s - loss: 1.9956 - accuracy: 0.28 - ETA: 1s - loss: 1.9948 - accuracy: 0.28 - ETA: 0s - loss: 1.9945 - accuracy: 0.28 - ETA: 0s - loss: 1.9939 - accuracy: 0.28 - ETA: 0s - loss: 1.9943 - accuracy: 0.28 - 33s 941us/step - loss: 1.9946 - accuracy: 0.2885 - val_loss: 1.9472 - val_accuracy: 0.3089\n",
      "Epoch 8/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.9641 - accuracy: 0.275 - ETA: 22s - loss: 1.9660 - accuracy: 0.288 - ETA: 22s - loss: 1.9619 - accuracy: 0.295 - ETA: 22s - loss: 1.9566 - accuracy: 0.300 - ETA: 22s - loss: 1.9658 - accuracy: 0.295 - ETA: 21s - loss: 1.9629 - accuracy: 0.299 - ETA: 21s - loss: 1.9629 - accuracy: 0.301 - ETA: 20s - loss: 1.9706 - accuracy: 0.292 - ETA: 20s - loss: 1.9695 - accuracy: 0.294 - ETA: 20s - loss: 1.9703 - accuracy: 0.294 - ETA: 19s - loss: 1.9686 - accuracy: 0.296 - ETA: 19s - loss: 1.9714 - accuracy: 0.296 - ETA: 18s - loss: 1.9715 - accuracy: 0.295 - ETA: 18s - loss: 1.9720 - accuracy: 0.294 - ETA: 17s - loss: 1.9705 - accuracy: 0.294 - ETA: 17s - loss: 1.9706 - accuracy: 0.294 - ETA: 17s - loss: 1.9695 - accuracy: 0.294 - ETA: 17s - loss: 1.9704 - accuracy: 0.294 - ETA: 16s - loss: 1.9687 - accuracy: 0.295 - ETA: 16s - loss: 1.9685 - accuracy: 0.295 - ETA: 15s - loss: 1.9692 - accuracy: 0.293 - ETA: 15s - loss: 1.9702 - accuracy: 0.293 - ETA: 14s - loss: 1.9687 - accuracy: 0.294 - ETA: 14s - loss: 1.9689 - accuracy: 0.294 - ETA: 13s - loss: 1.9694 - accuracy: 0.294 - ETA: 13s - loss: 1.9701 - accuracy: 0.294 - ETA: 13s - loss: 1.9705 - accuracy: 0.294 - ETA: 12s - loss: 1.9705 - accuracy: 0.294 - ETA: 12s - loss: 1.9700 - accuracy: 0.294 - ETA: 11s - loss: 1.9702 - accuracy: 0.294 - ETA: 11s - loss: 1.9691 - accuracy: 0.295 - ETA: 10s - loss: 1.9689 - accuracy: 0.295 - ETA: 10s - loss: 1.9691 - accuracy: 0.296 - ETA: 10s - loss: 1.9696 - accuracy: 0.296 - ETA: 9s - loss: 1.9688 - accuracy: 0.296 - ETA: 9s - loss: 1.9691 - accuracy: 0.29 - ETA: 8s - loss: 1.9691 - accuracy: 0.29 - ETA: 8s - loss: 1.9684 - accuracy: 0.29 - ETA: 8s - loss: 1.9682 - accuracy: 0.29 - ETA: 7s - loss: 1.9683 - accuracy: 0.29 - ETA: 7s - loss: 1.9687 - accuracy: 0.29 - ETA: 6s - loss: 1.9686 - accuracy: 0.29 - ETA: 6s - loss: 1.9679 - accuracy: 0.29 - ETA: 5s - loss: 1.9674 - accuracy: 0.29 - ETA: 5s - loss: 1.9670 - accuracy: 0.29 - ETA: 5s - loss: 1.9668 - accuracy: 0.29 - ETA: 4s - loss: 1.9670 - accuracy: 0.29 - ETA: 4s - loss: 1.9667 - accuracy: 0.29 - ETA: 3s - loss: 1.9659 - accuracy: 0.29 - ETA: 3s - loss: 1.9660 - accuracy: 0.29 - ETA: 3s - loss: 1.9657 - accuracy: 0.29 - ETA: 2s - loss: 1.9658 - accuracy: 0.29 - ETA: 2s - loss: 1.9656 - accuracy: 0.29 - ETA: 1s - loss: 1.9651 - accuracy: 0.29 - ETA: 1s - loss: 1.9655 - accuracy: 0.29 - ETA: 0s - loss: 1.9654 - accuracy: 0.29 - ETA: 0s - loss: 1.9660 - accuracy: 0.29 - ETA: 0s - loss: 1.9661 - accuracy: 0.29 - 33s 941us/step - loss: 1.9663 - accuracy: 0.2950 - val_loss: 1.9262 - val_accuracy: 0.3227\n",
      "Epoch 9/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 1.9289 - accuracy: 0.315 - ETA: 23s - loss: 1.9335 - accuracy: 0.310 - ETA: 22s - loss: 1.9543 - accuracy: 0.293 - ETA: 22s - loss: 1.9522 - accuracy: 0.293 - ETA: 22s - loss: 1.9610 - accuracy: 0.290 - ETA: 21s - loss: 1.9664 - accuracy: 0.286 - ETA: 21s - loss: 1.9660 - accuracy: 0.288 - ETA: 20s - loss: 1.9639 - accuracy: 0.291 - ETA: 20s - loss: 1.9594 - accuracy: 0.293 - ETA: 20s - loss: 1.9587 - accuracy: 0.293 - ETA: 19s - loss: 1.9592 - accuracy: 0.293 - ETA: 19s - loss: 1.9556 - accuracy: 0.296 - ETA: 18s - loss: 1.9526 - accuracy: 0.299 - ETA: 18s - loss: 1.9546 - accuracy: 0.299 - ETA: 17s - loss: 1.9544 - accuracy: 0.300 - ETA: 17s - loss: 1.9557 - accuracy: 0.298 - ETA: 17s - loss: 1.9560 - accuracy: 0.297 - ETA: 16s - loss: 1.9551 - accuracy: 0.297 - ETA: 16s - loss: 1.9562 - accuracy: 0.297 - ETA: 15s - loss: 1.9522 - accuracy: 0.299 - ETA: 15s - loss: 1.9526 - accuracy: 0.300 - ETA: 14s - loss: 1.9532 - accuracy: 0.300 - ETA: 14s - loss: 1.9518 - accuracy: 0.300 - ETA: 14s - loss: 1.9526 - accuracy: 0.299 - ETA: 13s - loss: 1.9531 - accuracy: 0.299 - ETA: 13s - loss: 1.9534 - accuracy: 0.299 - ETA: 12s - loss: 1.9527 - accuracy: 0.300 - ETA: 12s - loss: 1.9524 - accuracy: 0.300 - ETA: 12s - loss: 1.9517 - accuracy: 0.301 - ETA: 11s - loss: 1.9509 - accuracy: 0.300 - ETA: 11s - loss: 1.9522 - accuracy: 0.300 - ETA: 10s - loss: 1.9504 - accuracy: 0.301 - ETA: 10s - loss: 1.9503 - accuracy: 0.302 - ETA: 10s - loss: 1.9489 - accuracy: 0.303 - ETA: 9s - loss: 1.9481 - accuracy: 0.303 - ETA: 9s - loss: 1.9486 - accuracy: 0.30 - ETA: 8s - loss: 1.9479 - accuracy: 0.30 - ETA: 8s - loss: 1.9486 - accuracy: 0.30 - ETA: 8s - loss: 1.9475 - accuracy: 0.30 - ETA: 7s - loss: 1.9467 - accuracy: 0.30 - ETA: 7s - loss: 1.9463 - accuracy: 0.30 - ETA: 6s - loss: 1.9455 - accuracy: 0.30 - ETA: 6s - loss: 1.9448 - accuracy: 0.30 - ETA: 6s - loss: 1.9452 - accuracy: 0.30 - ETA: 5s - loss: 1.9446 - accuracy: 0.30 - ETA: 5s - loss: 1.9446 - accuracy: 0.30 - ETA: 4s - loss: 1.9448 - accuracy: 0.30 - ETA: 4s - loss: 1.9449 - accuracy: 0.30 - ETA: 3s - loss: 1.9449 - accuracy: 0.30 - ETA: 3s - loss: 1.9447 - accuracy: 0.30 - ETA: 3s - loss: 1.9441 - accuracy: 0.30 - ETA: 2s - loss: 1.9437 - accuracy: 0.30 - ETA: 2s - loss: 1.9434 - accuracy: 0.30 - ETA: 1s - loss: 1.9432 - accuracy: 0.30 - ETA: 1s - loss: 1.9427 - accuracy: 0.30 - ETA: 0s - loss: 1.9425 - accuracy: 0.30 - ETA: 0s - loss: 1.9430 - accuracy: 0.30 - ETA: 0s - loss: 1.9425 - accuracy: 0.30 - 33s 949us/step - loss: 1.9426 - accuracy: 0.3076 - val_loss: 1.8920 - val_accuracy: 0.3255\n",
      "Epoch 10/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.9190 - accuracy: 0.325 - ETA: 23s - loss: 1.9183 - accuracy: 0.325 - ETA: 23s - loss: 1.9278 - accuracy: 0.315 - ETA: 23s - loss: 1.9320 - accuracy: 0.310 - ETA: 22s - loss: 1.9339 - accuracy: 0.306 - ETA: 22s - loss: 1.9383 - accuracy: 0.301 - ETA: 21s - loss: 1.9372 - accuracy: 0.300 - ETA: 21s - loss: 1.9327 - accuracy: 0.304 - ETA: 20s - loss: 1.9345 - accuracy: 0.307 - ETA: 20s - loss: 1.9361 - accuracy: 0.306 - ETA: 19s - loss: 1.9337 - accuracy: 0.307 - ETA: 19s - loss: 1.9356 - accuracy: 0.306 - ETA: 19s - loss: 1.9338 - accuracy: 0.306 - ETA: 18s - loss: 1.9353 - accuracy: 0.305 - ETA: 18s - loss: 1.9334 - accuracy: 0.306 - ETA: 17s - loss: 1.9317 - accuracy: 0.308 - ETA: 17s - loss: 1.9322 - accuracy: 0.307 - ETA: 16s - loss: 1.9300 - accuracy: 0.308 - ETA: 16s - loss: 1.9290 - accuracy: 0.308 - ETA: 16s - loss: 1.9269 - accuracy: 0.308 - ETA: 15s - loss: 1.9285 - accuracy: 0.308 - ETA: 15s - loss: 1.9283 - accuracy: 0.309 - ETA: 14s - loss: 1.9268 - accuracy: 0.310 - ETA: 14s - loss: 1.9288 - accuracy: 0.309 - ETA: 13s - loss: 1.9289 - accuracy: 0.308 - ETA: 13s - loss: 1.9271 - accuracy: 0.310 - ETA: 13s - loss: 1.9272 - accuracy: 0.310 - ETA: 12s - loss: 1.9273 - accuracy: 0.310 - ETA: 12s - loss: 1.9266 - accuracy: 0.311 - ETA: 12s - loss: 1.9260 - accuracy: 0.311 - ETA: 11s - loss: 1.9253 - accuracy: 0.311 - ETA: 11s - loss: 1.9265 - accuracy: 0.310 - ETA: 10s - loss: 1.9251 - accuracy: 0.311 - ETA: 10s - loss: 1.9241 - accuracy: 0.312 - ETA: 9s - loss: 1.9230 - accuracy: 0.313 - ETA: 9s - loss: 1.9229 - accuracy: 0.31 - ETA: 9s - loss: 1.9231 - accuracy: 0.31 - ETA: 8s - loss: 1.9230 - accuracy: 0.31 - ETA: 8s - loss: 1.9230 - accuracy: 0.31 - ETA: 7s - loss: 1.9222 - accuracy: 0.31 - ETA: 7s - loss: 1.9230 - accuracy: 0.31 - ETA: 6s - loss: 1.9228 - accuracy: 0.31 - ETA: 6s - loss: 1.9212 - accuracy: 0.31 - ETA: 6s - loss: 1.9206 - accuracy: 0.31 - ETA: 5s - loss: 1.9210 - accuracy: 0.31 - ETA: 5s - loss: 1.9206 - accuracy: 0.31 - ETA: 4s - loss: 1.9211 - accuracy: 0.31 - ETA: 4s - loss: 1.9202 - accuracy: 0.31 - ETA: 3s - loss: 1.9200 - accuracy: 0.31 - ETA: 3s - loss: 1.9197 - accuracy: 0.31 - ETA: 3s - loss: 1.9190 - accuracy: 0.31 - ETA: 2s - loss: 1.9184 - accuracy: 0.31 - ETA: 2s - loss: 1.9188 - accuracy: 0.31 - ETA: 1s - loss: 1.9176 - accuracy: 0.31 - ETA: 1s - loss: 1.9177 - accuracy: 0.31 - ETA: 0s - loss: 1.9178 - accuracy: 0.31 - ETA: 0s - loss: 1.9181 - accuracy: 0.31 - ETA: 0s - loss: 1.9181 - accuracy: 0.31 - 33s 945us/step - loss: 1.9181 - accuracy: 0.3137 - val_loss: 1.8682 - val_accuracy: 0.3334\n",
      "Epoch 11/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.8805 - accuracy: 0.351 - ETA: 23s - loss: 1.9001 - accuracy: 0.330 - ETA: 22s - loss: 1.9073 - accuracy: 0.327 - ETA: 22s - loss: 1.9024 - accuracy: 0.328 - ETA: 22s - loss: 1.9082 - accuracy: 0.328 - ETA: 21s - loss: 1.9033 - accuracy: 0.330 - ETA: 21s - loss: 1.8962 - accuracy: 0.333 - ETA: 20s - loss: 1.8949 - accuracy: 0.333 - ETA: 20s - loss: 1.8980 - accuracy: 0.329 - ETA: 20s - loss: 1.8975 - accuracy: 0.329 - ETA: 19s - loss: 1.8990 - accuracy: 0.328 - ETA: 19s - loss: 1.8968 - accuracy: 0.328 - ETA: 18s - loss: 1.8983 - accuracy: 0.325 - ETA: 18s - loss: 1.8993 - accuracy: 0.324 - ETA: 17s - loss: 1.8987 - accuracy: 0.323 - ETA: 17s - loss: 1.9005 - accuracy: 0.322 - ETA: 17s - loss: 1.8992 - accuracy: 0.323 - ETA: 16s - loss: 1.8979 - accuracy: 0.323 - ETA: 16s - loss: 1.8977 - accuracy: 0.324 - ETA: 15s - loss: 1.8983 - accuracy: 0.323 - ETA: 15s - loss: 1.8972 - accuracy: 0.324 - ETA: 15s - loss: 1.8962 - accuracy: 0.324 - ETA: 14s - loss: 1.8973 - accuracy: 0.323 - ETA: 14s - loss: 1.8969 - accuracy: 0.324 - ETA: 13s - loss: 1.8989 - accuracy: 0.322 - ETA: 13s - loss: 1.8990 - accuracy: 0.322 - ETA: 13s - loss: 1.8984 - accuracy: 0.322 - ETA: 12s - loss: 1.8974 - accuracy: 0.322 - ETA: 12s - loss: 1.8976 - accuracy: 0.322 - ETA: 11s - loss: 1.8977 - accuracy: 0.321 - ETA: 11s - loss: 1.8972 - accuracy: 0.322 - ETA: 11s - loss: 1.8979 - accuracy: 0.322 - ETA: 10s - loss: 1.8973 - accuracy: 0.322 - ETA: 10s - loss: 1.8957 - accuracy: 0.323 - ETA: 9s - loss: 1.8950 - accuracy: 0.323 - ETA: 9s - loss: 1.8955 - accuracy: 0.32 - ETA: 8s - loss: 1.8951 - accuracy: 0.32 - ETA: 8s - loss: 1.8949 - accuracy: 0.32 - ETA: 8s - loss: 1.8945 - accuracy: 0.32 - ETA: 7s - loss: 1.8944 - accuracy: 0.32 - ETA: 7s - loss: 1.8944 - accuracy: 0.32 - ETA: 6s - loss: 1.8944 - accuracy: 0.32 - ETA: 6s - loss: 1.8941 - accuracy: 0.32 - ETA: 6s - loss: 1.8947 - accuracy: 0.32 - ETA: 5s - loss: 1.8947 - accuracy: 0.32 - ETA: 5s - loss: 1.8948 - accuracy: 0.32 - ETA: 4s - loss: 1.8946 - accuracy: 0.32 - ETA: 4s - loss: 1.8943 - accuracy: 0.32 - ETA: 3s - loss: 1.8935 - accuracy: 0.32 - ETA: 3s - loss: 1.8938 - accuracy: 0.32 - ETA: 3s - loss: 1.8937 - accuracy: 0.32 - ETA: 2s - loss: 1.8932 - accuracy: 0.32 - ETA: 2s - loss: 1.8931 - accuracy: 0.32 - ETA: 1s - loss: 1.8928 - accuracy: 0.32 - ETA: 1s - loss: 1.8934 - accuracy: 0.32 - ETA: 0s - loss: 1.8927 - accuracy: 0.32 - ETA: 0s - loss: 1.8924 - accuracy: 0.32 - ETA: 0s - loss: 1.8925 - accuracy: 0.32 - 33s 945us/step - loss: 1.8926 - accuracy: 0.3230 - val_loss: 1.8377 - val_accuracy: 0.3457\n",
      "Epoch 12/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 21s - loss: 1.8927 - accuracy: 0.341 - ETA: 22s - loss: 1.8861 - accuracy: 0.335 - ETA: 22s - loss: 1.8808 - accuracy: 0.337 - ETA: 21s - loss: 1.8771 - accuracy: 0.336 - ETA: 21s - loss: 1.8698 - accuracy: 0.335 - ETA: 21s - loss: 1.8697 - accuracy: 0.332 - ETA: 20s - loss: 1.8712 - accuracy: 0.332 - ETA: 20s - loss: 1.8762 - accuracy: 0.329 - ETA: 20s - loss: 1.8733 - accuracy: 0.330 - ETA: 19s - loss: 1.8734 - accuracy: 0.330 - ETA: 19s - loss: 1.8746 - accuracy: 0.328 - ETA: 18s - loss: 1.8749 - accuracy: 0.327 - ETA: 18s - loss: 1.8723 - accuracy: 0.330 - ETA: 18s - loss: 1.8709 - accuracy: 0.331 - ETA: 17s - loss: 1.8682 - accuracy: 0.332 - ETA: 17s - loss: 1.8660 - accuracy: 0.333 - ETA: 16s - loss: 1.8666 - accuracy: 0.332 - ETA: 16s - loss: 1.8672 - accuracy: 0.333 - ETA: 16s - loss: 1.8684 - accuracy: 0.333 - ETA: 15s - loss: 1.8674 - accuracy: 0.334 - ETA: 15s - loss: 1.8683 - accuracy: 0.334 - ETA: 14s - loss: 1.8697 - accuracy: 0.333 - ETA: 14s - loss: 1.8698 - accuracy: 0.333 - ETA: 14s - loss: 1.8693 - accuracy: 0.333 - ETA: 13s - loss: 1.8682 - accuracy: 0.332 - ETA: 13s - loss: 1.8675 - accuracy: 0.332 - ETA: 13s - loss: 1.8679 - accuracy: 0.332 - ETA: 12s - loss: 1.8681 - accuracy: 0.332 - ETA: 12s - loss: 1.8682 - accuracy: 0.331 - ETA: 11s - loss: 1.8678 - accuracy: 0.332 - ETA: 11s - loss: 1.8682 - accuracy: 0.332 - ETA: 11s - loss: 1.8678 - accuracy: 0.332 - ETA: 10s - loss: 1.8671 - accuracy: 0.333 - ETA: 10s - loss: 1.8679 - accuracy: 0.331 - ETA: 9s - loss: 1.8670 - accuracy: 0.332 - ETA: 9s - loss: 1.8670 - accuracy: 0.33 - ETA: 9s - loss: 1.8680 - accuracy: 0.33 - ETA: 8s - loss: 1.8681 - accuracy: 0.33 - ETA: 8s - loss: 1.8683 - accuracy: 0.33 - ETA: 7s - loss: 1.8679 - accuracy: 0.33 - ETA: 7s - loss: 1.8675 - accuracy: 0.33 - ETA: 6s - loss: 1.8672 - accuracy: 0.33 - ETA: 6s - loss: 1.8671 - accuracy: 0.33 - ETA: 6s - loss: 1.8671 - accuracy: 0.33 - ETA: 5s - loss: 1.8674 - accuracy: 0.33 - ETA: 5s - loss: 1.8676 - accuracy: 0.33 - ETA: 4s - loss: 1.8675 - accuracy: 0.33 - ETA: 4s - loss: 1.8666 - accuracy: 0.33 - ETA: 3s - loss: 1.8666 - accuracy: 0.33 - ETA: 3s - loss: 1.8668 - accuracy: 0.33 - ETA: 3s - loss: 1.8659 - accuracy: 0.33 - ETA: 2s - loss: 1.8659 - accuracy: 0.33 - ETA: 2s - loss: 1.8655 - accuracy: 0.33 - ETA: 1s - loss: 1.8657 - accuracy: 0.33 - ETA: 1s - loss: 1.8649 - accuracy: 0.33 - ETA: 0s - loss: 1.8651 - accuracy: 0.33 - ETA: 0s - loss: 1.8654 - accuracy: 0.33 - ETA: 0s - loss: 1.8656 - accuracy: 0.33 - 33s 954us/step - loss: 1.8662 - accuracy: 0.3323 - val_loss: 1.8177 - val_accuracy: 0.3530\n",
      "Epoch 13/45\n",
      "35000/35000 [==============================] - ETA: 21s - loss: 1.8188 - accuracy: 0.345 - ETA: 22s - loss: 1.8356 - accuracy: 0.337 - ETA: 22s - loss: 1.8416 - accuracy: 0.342 - ETA: 21s - loss: 1.8407 - accuracy: 0.340 - ETA: 21s - loss: 1.8445 - accuracy: 0.344 - ETA: 21s - loss: 1.8561 - accuracy: 0.335 - ETA: 20s - loss: 1.8558 - accuracy: 0.339 - ETA: 20s - loss: 1.8469 - accuracy: 0.343 - ETA: 20s - loss: 1.8543 - accuracy: 0.337 - ETA: 19s - loss: 1.8536 - accuracy: 0.335 - ETA: 19s - loss: 1.8534 - accuracy: 0.333 - ETA: 18s - loss: 1.8551 - accuracy: 0.331 - ETA: 18s - loss: 1.8500 - accuracy: 0.334 - ETA: 18s - loss: 1.8528 - accuracy: 0.331 - ETA: 17s - loss: 1.8544 - accuracy: 0.329 - ETA: 17s - loss: 1.8536 - accuracy: 0.330 - ETA: 16s - loss: 1.8529 - accuracy: 0.330 - ETA: 16s - loss: 1.8522 - accuracy: 0.330 - ETA: 16s - loss: 1.8495 - accuracy: 0.331 - ETA: 15s - loss: 1.8485 - accuracy: 0.332 - ETA: 15s - loss: 1.8485 - accuracy: 0.332 - ETA: 15s - loss: 1.8483 - accuracy: 0.332 - ETA: 14s - loss: 1.8468 - accuracy: 0.333 - ETA: 14s - loss: 1.8475 - accuracy: 0.332 - ETA: 13s - loss: 1.8484 - accuracy: 0.332 - ETA: 13s - loss: 1.8471 - accuracy: 0.332 - ETA: 13s - loss: 1.8460 - accuracy: 0.333 - ETA: 12s - loss: 1.8452 - accuracy: 0.333 - ETA: 12s - loss: 1.8455 - accuracy: 0.333 - ETA: 11s - loss: 1.8459 - accuracy: 0.333 - ETA: 11s - loss: 1.8452 - accuracy: 0.333 - ETA: 11s - loss: 1.8449 - accuracy: 0.334 - ETA: 10s - loss: 1.8447 - accuracy: 0.335 - ETA: 10s - loss: 1.8447 - accuracy: 0.335 - ETA: 9s - loss: 1.8451 - accuracy: 0.335 - ETA: 9s - loss: 1.8453 - accuracy: 0.33 - ETA: 9s - loss: 1.8451 - accuracy: 0.33 - ETA: 8s - loss: 1.8453 - accuracy: 0.33 - ETA: 8s - loss: 1.8448 - accuracy: 0.33 - ETA: 7s - loss: 1.8447 - accuracy: 0.33 - ETA: 7s - loss: 1.8442 - accuracy: 0.33 - ETA: 6s - loss: 1.8436 - accuracy: 0.33 - ETA: 6s - loss: 1.8427 - accuracy: 0.33 - ETA: 6s - loss: 1.8435 - accuracy: 0.33 - ETA: 5s - loss: 1.8430 - accuracy: 0.33 - ETA: 5s - loss: 1.8422 - accuracy: 0.33 - ETA: 4s - loss: 1.8427 - accuracy: 0.33 - ETA: 4s - loss: 1.8427 - accuracy: 0.33 - ETA: 3s - loss: 1.8420 - accuracy: 0.33 - ETA: 3s - loss: 1.8415 - accuracy: 0.33 - ETA: 3s - loss: 1.8416 - accuracy: 0.33 - ETA: 2s - loss: 1.8405 - accuracy: 0.33 - ETA: 2s - loss: 1.8411 - accuracy: 0.33 - ETA: 1s - loss: 1.8409 - accuracy: 0.33 - ETA: 1s - loss: 1.8408 - accuracy: 0.33 - ETA: 0s - loss: 1.8406 - accuracy: 0.33 - ETA: 0s - loss: 1.8401 - accuracy: 0.33 - ETA: 0s - loss: 1.8397 - accuracy: 0.33 - 33s 947us/step - loss: 1.8395 - accuracy: 0.3380 - val_loss: 1.7831 - val_accuracy: 0.3520\n",
      "Epoch 14/45\n",
      "35000/35000 [==============================] - ETA: 21s - loss: 1.8330 - accuracy: 0.325 - ETA: 22s - loss: 1.8235 - accuracy: 0.328 - ETA: 22s - loss: 1.8248 - accuracy: 0.330 - ETA: 22s - loss: 1.8282 - accuracy: 0.329 - ETA: 21s - loss: 1.8265 - accuracy: 0.336 - ETA: 21s - loss: 1.8306 - accuracy: 0.335 - ETA: 20s - loss: 1.8303 - accuracy: 0.336 - ETA: 20s - loss: 1.8307 - accuracy: 0.338 - ETA: 20s - loss: 1.8344 - accuracy: 0.336 - ETA: 19s - loss: 1.8305 - accuracy: 0.339 - ETA: 19s - loss: 1.8296 - accuracy: 0.339 - ETA: 19s - loss: 1.8292 - accuracy: 0.339 - ETA: 18s - loss: 1.8309 - accuracy: 0.337 - ETA: 18s - loss: 1.8283 - accuracy: 0.339 - ETA: 17s - loss: 1.8280 - accuracy: 0.341 - ETA: 17s - loss: 1.8259 - accuracy: 0.341 - ETA: 17s - loss: 1.8259 - accuracy: 0.342 - ETA: 16s - loss: 1.8269 - accuracy: 0.342 - ETA: 16s - loss: 1.8247 - accuracy: 0.343 - ETA: 15s - loss: 1.8260 - accuracy: 0.342 - ETA: 15s - loss: 1.8261 - accuracy: 0.342 - ETA: 15s - loss: 1.8253 - accuracy: 0.342 - ETA: 14s - loss: 1.8243 - accuracy: 0.343 - ETA: 14s - loss: 1.8233 - accuracy: 0.344 - ETA: 14s - loss: 1.8242 - accuracy: 0.343 - ETA: 13s - loss: 1.8253 - accuracy: 0.342 - ETA: 13s - loss: 1.8260 - accuracy: 0.343 - ETA: 12s - loss: 1.8266 - accuracy: 0.342 - ETA: 12s - loss: 1.8253 - accuracy: 0.343 - ETA: 11s - loss: 1.8244 - accuracy: 0.343 - ETA: 11s - loss: 1.8238 - accuracy: 0.343 - ETA: 11s - loss: 1.8234 - accuracy: 0.343 - ETA: 10s - loss: 1.8227 - accuracy: 0.343 - ETA: 10s - loss: 1.8224 - accuracy: 0.343 - ETA: 9s - loss: 1.8218 - accuracy: 0.343 - ETA: 9s - loss: 1.8216 - accuracy: 0.34 - ETA: 8s - loss: 1.8213 - accuracy: 0.34 - ETA: 8s - loss: 1.8199 - accuracy: 0.34 - ETA: 8s - loss: 1.8191 - accuracy: 0.34 - ETA: 7s - loss: 1.8190 - accuracy: 0.34 - ETA: 7s - loss: 1.8196 - accuracy: 0.34 - ETA: 6s - loss: 1.8186 - accuracy: 0.34 - ETA: 6s - loss: 1.8189 - accuracy: 0.34 - ETA: 6s - loss: 1.8192 - accuracy: 0.34 - ETA: 5s - loss: 1.8188 - accuracy: 0.34 - ETA: 5s - loss: 1.8178 - accuracy: 0.34 - ETA: 4s - loss: 1.8168 - accuracy: 0.34 - ETA: 4s - loss: 1.8162 - accuracy: 0.34 - ETA: 3s - loss: 1.8164 - accuracy: 0.34 - ETA: 3s - loss: 1.8173 - accuracy: 0.34 - ETA: 3s - loss: 1.8169 - accuracy: 0.34 - ETA: 2s - loss: 1.8161 - accuracy: 0.34 - ETA: 2s - loss: 1.8160 - accuracy: 0.34 - ETA: 1s - loss: 1.8163 - accuracy: 0.34 - ETA: 1s - loss: 1.8165 - accuracy: 0.34 - ETA: 0s - loss: 1.8163 - accuracy: 0.34 - ETA: 0s - loss: 1.8156 - accuracy: 0.34 - ETA: 0s - loss: 1.8151 - accuracy: 0.34 - 33s 949us/step - loss: 1.8153 - accuracy: 0.3442 - val_loss: 1.7577 - val_accuracy: 0.3594\n",
      "Epoch 15/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 21s - loss: 1.7894 - accuracy: 0.363 - ETA: 22s - loss: 1.8071 - accuracy: 0.344 - ETA: 22s - loss: 1.8125 - accuracy: 0.340 - ETA: 21s - loss: 1.8100 - accuracy: 0.340 - ETA: 21s - loss: 1.8190 - accuracy: 0.336 - ETA: 21s - loss: 1.8118 - accuracy: 0.339 - ETA: 20s - loss: 1.8097 - accuracy: 0.339 - ETA: 20s - loss: 1.8143 - accuracy: 0.339 - ETA: 20s - loss: 1.8111 - accuracy: 0.341 - ETA: 19s - loss: 1.8116 - accuracy: 0.340 - ETA: 19s - loss: 1.8100 - accuracy: 0.340 - ETA: 18s - loss: 1.8070 - accuracy: 0.342 - ETA: 18s - loss: 1.8064 - accuracy: 0.342 - ETA: 18s - loss: 1.8109 - accuracy: 0.340 - ETA: 17s - loss: 1.8082 - accuracy: 0.342 - ETA: 17s - loss: 1.8051 - accuracy: 0.345 - ETA: 16s - loss: 1.8046 - accuracy: 0.347 - ETA: 16s - loss: 1.8039 - accuracy: 0.347 - ETA: 16s - loss: 1.8038 - accuracy: 0.345 - ETA: 15s - loss: 1.8022 - accuracy: 0.346 - ETA: 15s - loss: 1.8025 - accuracy: 0.346 - ETA: 14s - loss: 1.8036 - accuracy: 0.346 - ETA: 14s - loss: 1.8025 - accuracy: 0.346 - ETA: 14s - loss: 1.8011 - accuracy: 0.347 - ETA: 13s - loss: 1.8015 - accuracy: 0.346 - ETA: 13s - loss: 1.8009 - accuracy: 0.346 - ETA: 12s - loss: 1.8002 - accuracy: 0.346 - ETA: 12s - loss: 1.7998 - accuracy: 0.346 - ETA: 12s - loss: 1.7994 - accuracy: 0.346 - ETA: 11s - loss: 1.8000 - accuracy: 0.345 - ETA: 11s - loss: 1.7995 - accuracy: 0.346 - ETA: 10s - loss: 1.7994 - accuracy: 0.345 - ETA: 10s - loss: 1.7987 - accuracy: 0.346 - ETA: 9s - loss: 1.7980 - accuracy: 0.347 - ETA: 9s - loss: 1.7969 - accuracy: 0.34 - ETA: 9s - loss: 1.7957 - accuracy: 0.34 - ETA: 8s - loss: 1.7961 - accuracy: 0.34 - ETA: 8s - loss: 1.7968 - accuracy: 0.34 - ETA: 7s - loss: 1.7970 - accuracy: 0.34 - ETA: 7s - loss: 1.7976 - accuracy: 0.34 - ETA: 7s - loss: 1.7965 - accuracy: 0.34 - ETA: 6s - loss: 1.7974 - accuracy: 0.34 - ETA: 6s - loss: 1.7981 - accuracy: 0.34 - ETA: 5s - loss: 1.7977 - accuracy: 0.34 - ETA: 5s - loss: 1.7966 - accuracy: 0.34 - ETA: 5s - loss: 1.7962 - accuracy: 0.34 - ETA: 4s - loss: 1.7954 - accuracy: 0.34 - ETA: 4s - loss: 1.7945 - accuracy: 0.34 - ETA: 3s - loss: 1.7945 - accuracy: 0.34 - ETA: 3s - loss: 1.7944 - accuracy: 0.34 - ETA: 3s - loss: 1.7948 - accuracy: 0.34 - ETA: 2s - loss: 1.7954 - accuracy: 0.34 - ETA: 2s - loss: 1.7949 - accuracy: 0.34 - ETA: 1s - loss: 1.7951 - accuracy: 0.34 - ETA: 1s - loss: 1.7944 - accuracy: 0.34 - ETA: 0s - loss: 1.7948 - accuracy: 0.34 - ETA: 0s - loss: 1.7950 - accuracy: 0.34 - ETA: 0s - loss: 1.7944 - accuracy: 0.34 - 33s 935us/step - loss: 1.7946 - accuracy: 0.3483 - val_loss: 1.7453 - val_accuracy: 0.3645\n",
      "Epoch 16/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.7997 - accuracy: 0.365 - ETA: 23s - loss: 1.8035 - accuracy: 0.346 - ETA: 22s - loss: 1.7974 - accuracy: 0.361 - ETA: 22s - loss: 1.8053 - accuracy: 0.352 - ETA: 22s - loss: 1.8026 - accuracy: 0.348 - ETA: 21s - loss: 1.7981 - accuracy: 0.351 - ETA: 21s - loss: 1.7941 - accuracy: 0.353 - ETA: 21s - loss: 1.7917 - accuracy: 0.352 - ETA: 20s - loss: 1.7917 - accuracy: 0.351 - ETA: 20s - loss: 1.7858 - accuracy: 0.354 - ETA: 19s - loss: 1.7802 - accuracy: 0.356 - ETA: 19s - loss: 1.7765 - accuracy: 0.358 - ETA: 19s - loss: 1.7759 - accuracy: 0.356 - ETA: 18s - loss: 1.7766 - accuracy: 0.355 - ETA: 18s - loss: 1.7775 - accuracy: 0.355 - ETA: 18s - loss: 1.7759 - accuracy: 0.355 - ETA: 17s - loss: 1.7770 - accuracy: 0.353 - ETA: 17s - loss: 1.7781 - accuracy: 0.353 - ETA: 16s - loss: 1.7795 - accuracy: 0.352 - ETA: 16s - loss: 1.7784 - accuracy: 0.353 - ETA: 16s - loss: 1.7797 - accuracy: 0.352 - ETA: 15s - loss: 1.7799 - accuracy: 0.353 - ETA: 15s - loss: 1.7778 - accuracy: 0.355 - ETA: 14s - loss: 1.7774 - accuracy: 0.355 - ETA: 14s - loss: 1.7781 - accuracy: 0.355 - ETA: 13s - loss: 1.7777 - accuracy: 0.355 - ETA: 13s - loss: 1.7788 - accuracy: 0.355 - ETA: 13s - loss: 1.7803 - accuracy: 0.354 - ETA: 12s - loss: 1.7788 - accuracy: 0.355 - ETA: 12s - loss: 1.7793 - accuracy: 0.356 - ETA: 11s - loss: 1.7803 - accuracy: 0.355 - ETA: 11s - loss: 1.7800 - accuracy: 0.355 - ETA: 10s - loss: 1.7803 - accuracy: 0.355 - ETA: 10s - loss: 1.7808 - accuracy: 0.354 - ETA: 10s - loss: 1.7806 - accuracy: 0.355 - ETA: 9s - loss: 1.7805 - accuracy: 0.354 - ETA: 9s - loss: 1.7810 - accuracy: 0.35 - ETA: 8s - loss: 1.7804 - accuracy: 0.35 - ETA: 8s - loss: 1.7798 - accuracy: 0.35 - ETA: 7s - loss: 1.7804 - accuracy: 0.35 - ETA: 7s - loss: 1.7811 - accuracy: 0.35 - ETA: 6s - loss: 1.7812 - accuracy: 0.35 - ETA: 6s - loss: 1.7810 - accuracy: 0.35 - ETA: 6s - loss: 1.7809 - accuracy: 0.35 - ETA: 5s - loss: 1.7808 - accuracy: 0.35 - ETA: 5s - loss: 1.7807 - accuracy: 0.35 - ETA: 4s - loss: 1.7801 - accuracy: 0.35 - ETA: 4s - loss: 1.7798 - accuracy: 0.35 - ETA: 3s - loss: 1.7797 - accuracy: 0.35 - ETA: 3s - loss: 1.7797 - accuracy: 0.35 - ETA: 3s - loss: 1.7790 - accuracy: 0.35 - ETA: 2s - loss: 1.7786 - accuracy: 0.35 - ETA: 2s - loss: 1.7777 - accuracy: 0.35 - ETA: 1s - loss: 1.7775 - accuracy: 0.35 - ETA: 1s - loss: 1.7775 - accuracy: 0.35 - ETA: 0s - loss: 1.7765 - accuracy: 0.35 - ETA: 0s - loss: 1.7764 - accuracy: 0.35 - ETA: 0s - loss: 1.7755 - accuracy: 0.35 - 33s 955us/step - loss: 1.7751 - accuracy: 0.3555 - val_loss: 1.7280 - val_accuracy: 0.3640\n",
      "Epoch 17/45\n",
      "35000/35000 [==============================] - ETA: 21s - loss: 1.7384 - accuracy: 0.361 - ETA: 22s - loss: 1.7512 - accuracy: 0.355 - ETA: 22s - loss: 1.7551 - accuracy: 0.351 - ETA: 22s - loss: 1.7538 - accuracy: 0.361 - ETA: 21s - loss: 1.7611 - accuracy: 0.360 - ETA: 21s - loss: 1.7727 - accuracy: 0.353 - ETA: 21s - loss: 1.7787 - accuracy: 0.348 - ETA: 20s - loss: 1.7807 - accuracy: 0.347 - ETA: 20s - loss: 1.7736 - accuracy: 0.350 - ETA: 20s - loss: 1.7763 - accuracy: 0.348 - ETA: 20s - loss: 1.7740 - accuracy: 0.350 - ETA: 19s - loss: 1.7729 - accuracy: 0.350 - ETA: 19s - loss: 1.7698 - accuracy: 0.350 - ETA: 19s - loss: 1.7734 - accuracy: 0.348 - ETA: 18s - loss: 1.7744 - accuracy: 0.347 - ETA: 18s - loss: 1.7720 - accuracy: 0.347 - ETA: 17s - loss: 1.7713 - accuracy: 0.347 - ETA: 17s - loss: 1.7709 - accuracy: 0.348 - ETA: 16s - loss: 1.7654 - accuracy: 0.353 - ETA: 16s - loss: 1.7633 - accuracy: 0.353 - ETA: 15s - loss: 1.7640 - accuracy: 0.352 - ETA: 15s - loss: 1.7647 - accuracy: 0.352 - ETA: 15s - loss: 1.7647 - accuracy: 0.352 - ETA: 14s - loss: 1.7640 - accuracy: 0.353 - ETA: 14s - loss: 1.7651 - accuracy: 0.352 - ETA: 13s - loss: 1.7642 - accuracy: 0.352 - ETA: 13s - loss: 1.7650 - accuracy: 0.352 - ETA: 12s - loss: 1.7637 - accuracy: 0.353 - ETA: 12s - loss: 1.7638 - accuracy: 0.353 - ETA: 11s - loss: 1.7634 - accuracy: 0.354 - ETA: 11s - loss: 1.7646 - accuracy: 0.353 - ETA: 11s - loss: 1.7637 - accuracy: 0.354 - ETA: 10s - loss: 1.7622 - accuracy: 0.354 - ETA: 10s - loss: 1.7635 - accuracy: 0.354 - ETA: 9s - loss: 1.7629 - accuracy: 0.355 - ETA: 9s - loss: 1.7635 - accuracy: 0.35 - ETA: 8s - loss: 1.7621 - accuracy: 0.35 - ETA: 8s - loss: 1.7616 - accuracy: 0.35 - ETA: 8s - loss: 1.7608 - accuracy: 0.35 - ETA: 7s - loss: 1.7605 - accuracy: 0.35 - ETA: 7s - loss: 1.7604 - accuracy: 0.35 - ETA: 6s - loss: 1.7595 - accuracy: 0.35 - ETA: 6s - loss: 1.7598 - accuracy: 0.35 - ETA: 6s - loss: 1.7606 - accuracy: 0.35 - ETA: 5s - loss: 1.7611 - accuracy: 0.35 - ETA: 5s - loss: 1.7607 - accuracy: 0.35 - ETA: 4s - loss: 1.7598 - accuracy: 0.35 - ETA: 4s - loss: 1.7597 - accuracy: 0.35 - ETA: 3s - loss: 1.7586 - accuracy: 0.35 - ETA: 3s - loss: 1.7584 - accuracy: 0.35 - ETA: 3s - loss: 1.7591 - accuracy: 0.35 - ETA: 2s - loss: 1.7580 - accuracy: 0.35 - ETA: 2s - loss: 1.7579 - accuracy: 0.35 - ETA: 1s - loss: 1.7581 - accuracy: 0.35 - ETA: 1s - loss: 1.7587 - accuracy: 0.35 - ETA: 0s - loss: 1.7586 - accuracy: 0.35 - ETA: 0s - loss: 1.7576 - accuracy: 0.35 - ETA: 0s - loss: 1.7574 - accuracy: 0.35 - 33s 950us/step - loss: 1.7570 - accuracy: 0.3583 - val_loss: 1.6928 - val_accuracy: 0.3775\n",
      "Epoch 18/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 1.7242 - accuracy: 0.373 - ETA: 22s - loss: 1.7274 - accuracy: 0.363 - ETA: 22s - loss: 1.7386 - accuracy: 0.358 - ETA: 22s - loss: 1.7428 - accuracy: 0.359 - ETA: 21s - loss: 1.7485 - accuracy: 0.353 - ETA: 21s - loss: 1.7395 - accuracy: 0.360 - ETA: 21s - loss: 1.7406 - accuracy: 0.361 - ETA: 21s - loss: 1.7361 - accuracy: 0.365 - ETA: 20s - loss: 1.7404 - accuracy: 0.362 - ETA: 20s - loss: 1.7401 - accuracy: 0.362 - ETA: 20s - loss: 1.7406 - accuracy: 0.362 - ETA: 19s - loss: 1.7429 - accuracy: 0.360 - ETA: 19s - loss: 1.7439 - accuracy: 0.359 - ETA: 18s - loss: 1.7440 - accuracy: 0.359 - ETA: 18s - loss: 1.7449 - accuracy: 0.360 - ETA: 17s - loss: 1.7467 - accuracy: 0.359 - ETA: 17s - loss: 1.7448 - accuracy: 0.361 - ETA: 17s - loss: 1.7442 - accuracy: 0.360 - ETA: 16s - loss: 1.7438 - accuracy: 0.360 - ETA: 16s - loss: 1.7451 - accuracy: 0.359 - ETA: 15s - loss: 1.7432 - accuracy: 0.361 - ETA: 15s - loss: 1.7441 - accuracy: 0.360 - ETA: 14s - loss: 1.7433 - accuracy: 0.360 - ETA: 14s - loss: 1.7414 - accuracy: 0.361 - ETA: 14s - loss: 1.7409 - accuracy: 0.361 - ETA: 13s - loss: 1.7389 - accuracy: 0.362 - ETA: 13s - loss: 1.7391 - accuracy: 0.361 - ETA: 12s - loss: 1.7399 - accuracy: 0.362 - ETA: 12s - loss: 1.7415 - accuracy: 0.360 - ETA: 11s - loss: 1.7408 - accuracy: 0.361 - ETA: 11s - loss: 1.7384 - accuracy: 0.362 - ETA: 11s - loss: 1.7388 - accuracy: 0.361 - ETA: 10s - loss: 1.7386 - accuracy: 0.362 - ETA: 10s - loss: 1.7394 - accuracy: 0.361 - ETA: 9s - loss: 1.7404 - accuracy: 0.361 - ETA: 9s - loss: 1.7400 - accuracy: 0.36 - ETA: 9s - loss: 1.7383 - accuracy: 0.36 - ETA: 8s - loss: 1.7376 - accuracy: 0.36 - ETA: 8s - loss: 1.7376 - accuracy: 0.36 - ETA: 7s - loss: 1.7376 - accuracy: 0.36 - ETA: 7s - loss: 1.7368 - accuracy: 0.36 - ETA: 6s - loss: 1.7373 - accuracy: 0.36 - ETA: 6s - loss: 1.7373 - accuracy: 0.36 - ETA: 6s - loss: 1.7384 - accuracy: 0.36 - ETA: 5s - loss: 1.7405 - accuracy: 0.36 - ETA: 5s - loss: 1.7402 - accuracy: 0.36 - ETA: 4s - loss: 1.7393 - accuracy: 0.36 - ETA: 4s - loss: 1.7396 - accuracy: 0.36 - ETA: 3s - loss: 1.7399 - accuracy: 0.36 - ETA: 3s - loss: 1.7393 - accuracy: 0.36 - ETA: 3s - loss: 1.7388 - accuracy: 0.36 - ETA: 2s - loss: 1.7380 - accuracy: 0.36 - ETA: 2s - loss: 1.7372 - accuracy: 0.36 - ETA: 1s - loss: 1.7364 - accuracy: 0.36 - ETA: 1s - loss: 1.7362 - accuracy: 0.36 - ETA: 0s - loss: 1.7364 - accuracy: 0.36 - ETA: 0s - loss: 1.7367 - accuracy: 0.36 - ETA: 0s - loss: 1.7371 - accuracy: 0.36 - 33s 953us/step - loss: 1.7369 - accuracy: 0.3635 - val_loss: 1.6918 - val_accuracy: 0.3715\n",
      "Epoch 19/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.6736 - accuracy: 0.396 - ETA: 23s - loss: 1.6782 - accuracy: 0.396 - ETA: 23s - loss: 1.7012 - accuracy: 0.376 - ETA: 23s - loss: 1.6947 - accuracy: 0.377 - ETA: 23s - loss: 1.6984 - accuracy: 0.374 - ETA: 22s - loss: 1.7031 - accuracy: 0.373 - ETA: 22s - loss: 1.7090 - accuracy: 0.371 - ETA: 21s - loss: 1.7062 - accuracy: 0.373 - ETA: 21s - loss: 1.7094 - accuracy: 0.372 - ETA: 20s - loss: 1.7096 - accuracy: 0.369 - ETA: 20s - loss: 1.7107 - accuracy: 0.368 - ETA: 20s - loss: 1.7124 - accuracy: 0.367 - ETA: 19s - loss: 1.7146 - accuracy: 0.368 - ETA: 19s - loss: 1.7193 - accuracy: 0.365 - ETA: 18s - loss: 1.7227 - accuracy: 0.365 - ETA: 18s - loss: 1.7256 - accuracy: 0.364 - ETA: 17s - loss: 1.7276 - accuracy: 0.364 - ETA: 17s - loss: 1.7260 - accuracy: 0.365 - ETA: 16s - loss: 1.7285 - accuracy: 0.364 - ETA: 16s - loss: 1.7290 - accuracy: 0.363 - ETA: 16s - loss: 1.7299 - accuracy: 0.363 - ETA: 15s - loss: 1.7305 - accuracy: 0.364 - ETA: 15s - loss: 1.7344 - accuracy: 0.361 - ETA: 14s - loss: 1.7356 - accuracy: 0.361 - ETA: 14s - loss: 1.7341 - accuracy: 0.362 - ETA: 13s - loss: 1.7336 - accuracy: 0.362 - ETA: 13s - loss: 1.7353 - accuracy: 0.361 - ETA: 12s - loss: 1.7327 - accuracy: 0.362 - ETA: 12s - loss: 1.7307 - accuracy: 0.362 - ETA: 12s - loss: 1.7279 - accuracy: 0.364 - ETA: 11s - loss: 1.7282 - accuracy: 0.364 - ETA: 11s - loss: 1.7278 - accuracy: 0.364 - ETA: 10s - loss: 1.7264 - accuracy: 0.365 - ETA: 10s - loss: 1.7277 - accuracy: 0.363 - ETA: 9s - loss: 1.7289 - accuracy: 0.364 - ETA: 9s - loss: 1.7291 - accuracy: 0.36 - ETA: 9s - loss: 1.7288 - accuracy: 0.36 - ETA: 8s - loss: 1.7279 - accuracy: 0.36 - ETA: 8s - loss: 1.7268 - accuracy: 0.36 - ETA: 7s - loss: 1.7274 - accuracy: 0.36 - ETA: 7s - loss: 1.7272 - accuracy: 0.36 - ETA: 6s - loss: 1.7283 - accuracy: 0.36 - ETA: 6s - loss: 1.7283 - accuracy: 0.36 - ETA: 6s - loss: 1.7295 - accuracy: 0.36 - ETA: 5s - loss: 1.7296 - accuracy: 0.36 - ETA: 5s - loss: 1.7294 - accuracy: 0.36 - ETA: 4s - loss: 1.7297 - accuracy: 0.36 - ETA: 4s - loss: 1.7293 - accuracy: 0.36 - ETA: 3s - loss: 1.7289 - accuracy: 0.36 - ETA: 3s - loss: 1.7292 - accuracy: 0.36 - ETA: 3s - loss: 1.7290 - accuracy: 0.36 - ETA: 2s - loss: 1.7279 - accuracy: 0.36 - ETA: 2s - loss: 1.7276 - accuracy: 0.36 - ETA: 1s - loss: 1.7277 - accuracy: 0.36 - ETA: 1s - loss: 1.7271 - accuracy: 0.36 - ETA: 0s - loss: 1.7263 - accuracy: 0.36 - ETA: 0s - loss: 1.7257 - accuracy: 0.36 - ETA: 0s - loss: 1.7258 - accuracy: 0.36 - 33s 949us/step - loss: 1.7259 - accuracy: 0.3653 - val_loss: 1.6893 - val_accuracy: 0.3799\n",
      "Epoch 20/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.7115 - accuracy: 0.375 - ETA: 24s - loss: 1.7423 - accuracy: 0.351 - ETA: 23s - loss: 1.7296 - accuracy: 0.364 - ETA: 22s - loss: 1.7165 - accuracy: 0.368 - ETA: 22s - loss: 1.7182 - accuracy: 0.364 - ETA: 22s - loss: 1.7256 - accuracy: 0.364 - ETA: 21s - loss: 1.7214 - accuracy: 0.365 - ETA: 21s - loss: 1.7190 - accuracy: 0.365 - ETA: 20s - loss: 1.7184 - accuracy: 0.365 - ETA: 20s - loss: 1.7226 - accuracy: 0.366 - ETA: 19s - loss: 1.7220 - accuracy: 0.364 - ETA: 19s - loss: 1.7198 - accuracy: 0.365 - ETA: 18s - loss: 1.7160 - accuracy: 0.365 - ETA: 18s - loss: 1.7132 - accuracy: 0.366 - ETA: 18s - loss: 1.7117 - accuracy: 0.368 - ETA: 17s - loss: 1.7154 - accuracy: 0.367 - ETA: 17s - loss: 1.7220 - accuracy: 0.365 - ETA: 16s - loss: 1.7227 - accuracy: 0.365 - ETA: 16s - loss: 1.7297 - accuracy: 0.362 - ETA: 15s - loss: 1.7267 - accuracy: 0.364 - ETA: 15s - loss: 1.7260 - accuracy: 0.364 - ETA: 15s - loss: 1.7245 - accuracy: 0.364 - ETA: 14s - loss: 1.7247 - accuracy: 0.363 - ETA: 14s - loss: 1.7252 - accuracy: 0.363 - ETA: 13s - loss: 1.7241 - accuracy: 0.364 - ETA: 13s - loss: 1.7231 - accuracy: 0.365 - ETA: 13s - loss: 1.7209 - accuracy: 0.366 - ETA: 12s - loss: 1.7212 - accuracy: 0.365 - ETA: 12s - loss: 1.7246 - accuracy: 0.364 - ETA: 11s - loss: 1.7266 - accuracy: 0.364 - ETA: 11s - loss: 1.7285 - accuracy: 0.363 - ETA: 10s - loss: 1.7277 - accuracy: 0.363 - ETA: 10s - loss: 1.7266 - accuracy: 0.364 - ETA: 10s - loss: 1.7263 - accuracy: 0.365 - ETA: 9s - loss: 1.7233 - accuracy: 0.366 - ETA: 9s - loss: 1.7223 - accuracy: 0.36 - ETA: 8s - loss: 1.7218 - accuracy: 0.36 - ETA: 8s - loss: 1.7213 - accuracy: 0.36 - ETA: 8s - loss: 1.7210 - accuracy: 0.36 - ETA: 7s - loss: 1.7187 - accuracy: 0.36 - ETA: 7s - loss: 1.7179 - accuracy: 0.36 - ETA: 6s - loss: 1.7174 - accuracy: 0.36 - ETA: 6s - loss: 1.7166 - accuracy: 0.36 - ETA: 6s - loss: 1.7163 - accuracy: 0.36 - ETA: 5s - loss: 1.7168 - accuracy: 0.36 - ETA: 5s - loss: 1.7157 - accuracy: 0.36 - ETA: 4s - loss: 1.7152 - accuracy: 0.36 - ETA: 4s - loss: 1.7146 - accuracy: 0.36 - ETA: 3s - loss: 1.7142 - accuracy: 0.36 - ETA: 3s - loss: 1.7135 - accuracy: 0.36 - ETA: 3s - loss: 1.7141 - accuracy: 0.36 - ETA: 2s - loss: 1.7153 - accuracy: 0.36 - ETA: 2s - loss: 1.7151 - accuracy: 0.36 - ETA: 1s - loss: 1.7152 - accuracy: 0.36 - ETA: 1s - loss: 1.7147 - accuracy: 0.36 - ETA: 0s - loss: 1.7152 - accuracy: 0.36 - ETA: 0s - loss: 1.7148 - accuracy: 0.36 - ETA: 0s - loss: 1.7160 - accuracy: 0.36 - 33s 948us/step - loss: 1.7159 - accuracy: 0.3685 - val_loss: 1.6698 - val_accuracy: 0.3734\n",
      "Epoch 21/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 1.6747 - accuracy: 0.378 - ETA: 23s - loss: 1.6880 - accuracy: 0.370 - ETA: 23s - loss: 1.7113 - accuracy: 0.360 - ETA: 23s - loss: 1.7113 - accuracy: 0.364 - ETA: 22s - loss: 1.7141 - accuracy: 0.364 - ETA: 22s - loss: 1.7121 - accuracy: 0.366 - ETA: 22s - loss: 1.7127 - accuracy: 0.367 - ETA: 21s - loss: 1.7130 - accuracy: 0.368 - ETA: 21s - loss: 1.7103 - accuracy: 0.369 - ETA: 20s - loss: 1.7110 - accuracy: 0.367 - ETA: 20s - loss: 1.7060 - accuracy: 0.370 - ETA: 19s - loss: 1.7067 - accuracy: 0.368 - ETA: 19s - loss: 1.7084 - accuracy: 0.365 - ETA: 18s - loss: 1.7077 - accuracy: 0.365 - ETA: 18s - loss: 1.7061 - accuracy: 0.366 - ETA: 18s - loss: 1.7015 - accuracy: 0.368 - ETA: 17s - loss: 1.7004 - accuracy: 0.368 - ETA: 17s - loss: 1.7014 - accuracy: 0.367 - ETA: 16s - loss: 1.6988 - accuracy: 0.369 - ETA: 16s - loss: 1.7004 - accuracy: 0.369 - ETA: 15s - loss: 1.6974 - accuracy: 0.371 - ETA: 15s - loss: 1.6990 - accuracy: 0.370 - ETA: 14s - loss: 1.6976 - accuracy: 0.371 - ETA: 14s - loss: 1.6968 - accuracy: 0.372 - ETA: 14s - loss: 1.6990 - accuracy: 0.371 - ETA: 13s - loss: 1.7006 - accuracy: 0.371 - ETA: 13s - loss: 1.7017 - accuracy: 0.370 - ETA: 12s - loss: 1.7043 - accuracy: 0.370 - ETA: 12s - loss: 1.7019 - accuracy: 0.371 - ETA: 11s - loss: 1.6995 - accuracy: 0.372 - ETA: 11s - loss: 1.6982 - accuracy: 0.373 - ETA: 11s - loss: 1.6969 - accuracy: 0.373 - ETA: 10s - loss: 1.6970 - accuracy: 0.374 - ETA: 10s - loss: 1.6969 - accuracy: 0.374 - ETA: 9s - loss: 1.6953 - accuracy: 0.375 - ETA: 9s - loss: 1.6938 - accuracy: 0.37 - ETA: 9s - loss: 1.6933 - accuracy: 0.37 - ETA: 8s - loss: 1.6932 - accuracy: 0.37 - ETA: 8s - loss: 1.6965 - accuracy: 0.37 - ETA: 7s - loss: 1.6962 - accuracy: 0.37 - ETA: 7s - loss: 1.6956 - accuracy: 0.37 - ETA: 6s - loss: 1.6964 - accuracy: 0.37 - ETA: 6s - loss: 1.6955 - accuracy: 0.37 - ETA: 6s - loss: 1.6948 - accuracy: 0.37 - ETA: 5s - loss: 1.6953 - accuracy: 0.37 - ETA: 5s - loss: 1.6965 - accuracy: 0.37 - ETA: 4s - loss: 1.6966 - accuracy: 0.37 - ETA: 4s - loss: 1.6971 - accuracy: 0.37 - ETA: 3s - loss: 1.6982 - accuracy: 0.37 - ETA: 3s - loss: 1.6990 - accuracy: 0.37 - ETA: 3s - loss: 1.6997 - accuracy: 0.37 - ETA: 2s - loss: 1.6998 - accuracy: 0.37 - ETA: 2s - loss: 1.7002 - accuracy: 0.37 - ETA: 1s - loss: 1.6997 - accuracy: 0.37 - ETA: 1s - loss: 1.6986 - accuracy: 0.37 - ETA: 0s - loss: 1.6985 - accuracy: 0.37 - ETA: 0s - loss: 1.6979 - accuracy: 0.37 - ETA: 0s - loss: 1.6974 - accuracy: 0.37 - 34s 966us/step - loss: 1.6979 - accuracy: 0.3755 - val_loss: 1.6835 - val_accuracy: 0.3712\n",
      "Epoch 22/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.7343 - accuracy: 0.363 - ETA: 24s - loss: 1.6898 - accuracy: 0.370 - ETA: 24s - loss: 1.6859 - accuracy: 0.372 - ETA: 23s - loss: 1.6731 - accuracy: 0.384 - ETA: 23s - loss: 1.6773 - accuracy: 0.380 - ETA: 23s - loss: 1.6775 - accuracy: 0.380 - ETA: 22s - loss: 1.6842 - accuracy: 0.376 - ETA: 22s - loss: 1.6840 - accuracy: 0.376 - ETA: 21s - loss: 1.6875 - accuracy: 0.374 - ETA: 21s - loss: 1.6903 - accuracy: 0.374 - ETA: 20s - loss: 1.6953 - accuracy: 0.372 - ETA: 20s - loss: 1.7004 - accuracy: 0.372 - ETA: 19s - loss: 1.7041 - accuracy: 0.371 - ETA: 19s - loss: 1.7027 - accuracy: 0.371 - ETA: 18s - loss: 1.6977 - accuracy: 0.373 - ETA: 18s - loss: 1.6969 - accuracy: 0.373 - ETA: 17s - loss: 1.6973 - accuracy: 0.374 - ETA: 17s - loss: 1.6988 - accuracy: 0.373 - ETA: 16s - loss: 1.7034 - accuracy: 0.372 - ETA: 16s - loss: 1.7045 - accuracy: 0.372 - ETA: 16s - loss: 1.7031 - accuracy: 0.373 - ETA: 15s - loss: 1.6993 - accuracy: 0.375 - ETA: 15s - loss: 1.6965 - accuracy: 0.375 - ETA: 14s - loss: 1.6938 - accuracy: 0.376 - ETA: 14s - loss: 1.6950 - accuracy: 0.374 - ETA: 14s - loss: 1.6936 - accuracy: 0.375 - ETA: 13s - loss: 1.6925 - accuracy: 0.375 - ETA: 13s - loss: 1.6926 - accuracy: 0.375 - ETA: 12s - loss: 1.6918 - accuracy: 0.375 - ETA: 12s - loss: 1.6916 - accuracy: 0.375 - ETA: 11s - loss: 1.6907 - accuracy: 0.375 - ETA: 11s - loss: 1.6903 - accuracy: 0.375 - ETA: 11s - loss: 1.6890 - accuracy: 0.376 - ETA: 10s - loss: 1.6889 - accuracy: 0.376 - ETA: 10s - loss: 1.6880 - accuracy: 0.377 - ETA: 9s - loss: 1.6897 - accuracy: 0.376 - ETA: 9s - loss: 1.6921 - accuracy: 0.37 - ETA: 9s - loss: 1.6938 - accuracy: 0.37 - ETA: 8s - loss: 1.6947 - accuracy: 0.37 - ETA: 8s - loss: 1.6942 - accuracy: 0.37 - ETA: 7s - loss: 1.6942 - accuracy: 0.37 - ETA: 7s - loss: 1.6933 - accuracy: 0.37 - ETA: 6s - loss: 1.6940 - accuracy: 0.37 - ETA: 6s - loss: 1.6939 - accuracy: 0.37 - ETA: 5s - loss: 1.6939 - accuracy: 0.37 - ETA: 5s - loss: 1.6928 - accuracy: 0.37 - ETA: 5s - loss: 1.6925 - accuracy: 0.37 - ETA: 4s - loss: 1.6914 - accuracy: 0.37 - ETA: 4s - loss: 1.6909 - accuracy: 0.37 - ETA: 3s - loss: 1.6912 - accuracy: 0.37 - ETA: 3s - loss: 1.6913 - accuracy: 0.37 - ETA: 2s - loss: 1.6908 - accuracy: 0.37 - ETA: 2s - loss: 1.6899 - accuracy: 0.37 - ETA: 1s - loss: 1.6907 - accuracy: 0.37 - ETA: 1s - loss: 1.6899 - accuracy: 0.37 - ETA: 1s - loss: 1.6897 - accuracy: 0.37 - ETA: 0s - loss: 1.6907 - accuracy: 0.37 - ETA: 0s - loss: 1.6914 - accuracy: 0.37 - 35s 993us/step - loss: 1.6912 - accuracy: 0.3748 - val_loss: 1.6329 - val_accuracy: 0.3912\n",
      "Epoch 23/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.6523 - accuracy: 0.401 - ETA: 24s - loss: 1.6582 - accuracy: 0.386 - ETA: 23s - loss: 1.6562 - accuracy: 0.388 - ETA: 23s - loss: 1.6577 - accuracy: 0.381 - ETA: 22s - loss: 1.6538 - accuracy: 0.384 - ETA: 22s - loss: 1.6472 - accuracy: 0.385 - ETA: 21s - loss: 1.6536 - accuracy: 0.382 - ETA: 21s - loss: 1.6582 - accuracy: 0.382 - ETA: 21s - loss: 1.6589 - accuracy: 0.381 - ETA: 20s - loss: 1.6629 - accuracy: 0.380 - ETA: 20s - loss: 1.6722 - accuracy: 0.379 - ETA: 19s - loss: 1.6787 - accuracy: 0.377 - ETA: 19s - loss: 1.6779 - accuracy: 0.378 - ETA: 19s - loss: 1.6755 - accuracy: 0.379 - ETA: 18s - loss: 1.6737 - accuracy: 0.379 - ETA: 18s - loss: 1.6762 - accuracy: 0.379 - ETA: 17s - loss: 1.6723 - accuracy: 0.381 - ETA: 17s - loss: 1.6716 - accuracy: 0.381 - ETA: 16s - loss: 1.6696 - accuracy: 0.382 - ETA: 16s - loss: 1.6708 - accuracy: 0.382 - ETA: 16s - loss: 1.6695 - accuracy: 0.382 - ETA: 15s - loss: 1.6720 - accuracy: 0.379 - ETA: 15s - loss: 1.6740 - accuracy: 0.380 - ETA: 14s - loss: 1.6758 - accuracy: 0.379 - ETA: 14s - loss: 1.6794 - accuracy: 0.379 - ETA: 14s - loss: 1.6823 - accuracy: 0.377 - ETA: 13s - loss: 1.6841 - accuracy: 0.377 - ETA: 13s - loss: 1.6831 - accuracy: 0.377 - ETA: 12s - loss: 1.6816 - accuracy: 0.377 - ETA: 12s - loss: 1.6808 - accuracy: 0.378 - ETA: 11s - loss: 1.6799 - accuracy: 0.378 - ETA: 11s - loss: 1.6822 - accuracy: 0.377 - ETA: 11s - loss: 1.6863 - accuracy: 0.376 - ETA: 10s - loss: 1.6869 - accuracy: 0.375 - ETA: 10s - loss: 1.6871 - accuracy: 0.375 - ETA: 9s - loss: 1.6866 - accuracy: 0.375 - ETA: 9s - loss: 1.6847 - accuracy: 0.37 - ETA: 8s - loss: 1.6852 - accuracy: 0.37 - ETA: 8s - loss: 1.6856 - accuracy: 0.37 - ETA: 8s - loss: 1.6849 - accuracy: 0.37 - ETA: 7s - loss: 1.6851 - accuracy: 0.37 - ETA: 7s - loss: 1.6843 - accuracy: 0.37 - ETA: 6s - loss: 1.6830 - accuracy: 0.37 - ETA: 6s - loss: 1.6833 - accuracy: 0.37 - ETA: 5s - loss: 1.6834 - accuracy: 0.37 - ETA: 5s - loss: 1.6834 - accuracy: 0.37 - ETA: 4s - loss: 1.6838 - accuracy: 0.37 - ETA: 4s - loss: 1.6844 - accuracy: 0.37 - ETA: 4s - loss: 1.6844 - accuracy: 0.37 - ETA: 3s - loss: 1.6845 - accuracy: 0.37 - ETA: 3s - loss: 1.6844 - accuracy: 0.37 - ETA: 2s - loss: 1.6826 - accuracy: 0.37 - ETA: 2s - loss: 1.6820 - accuracy: 0.37 - ETA: 1s - loss: 1.6815 - accuracy: 0.37 - ETA: 1s - loss: 1.6800 - accuracy: 0.37 - ETA: 1s - loss: 1.6796 - accuracy: 0.37 - ETA: 0s - loss: 1.6791 - accuracy: 0.37 - ETA: 0s - loss: 1.6795 - accuracy: 0.37 - 34s 973us/step - loss: 1.6795 - accuracy: 0.3783 - val_loss: 1.6730 - val_accuracy: 0.3730\n",
      "Epoch 24/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 1.7474 - accuracy: 0.368 - ETA: 22s - loss: 1.7195 - accuracy: 0.373 - ETA: 22s - loss: 1.6946 - accuracy: 0.378 - ETA: 21s - loss: 1.6670 - accuracy: 0.387 - ETA: 21s - loss: 1.6659 - accuracy: 0.380 - ETA: 21s - loss: 1.6604 - accuracy: 0.382 - ETA: 20s - loss: 1.6670 - accuracy: 0.381 - ETA: 20s - loss: 1.6713 - accuracy: 0.381 - ETA: 20s - loss: 1.6763 - accuracy: 0.378 - ETA: 19s - loss: 1.6838 - accuracy: 0.374 - ETA: 19s - loss: 1.6810 - accuracy: 0.377 - ETA: 19s - loss: 1.6814 - accuracy: 0.376 - ETA: 18s - loss: 1.6805 - accuracy: 0.377 - ETA: 18s - loss: 1.6802 - accuracy: 0.378 - ETA: 18s - loss: 1.6776 - accuracy: 0.380 - ETA: 17s - loss: 1.6792 - accuracy: 0.378 - ETA: 17s - loss: 1.6782 - accuracy: 0.377 - ETA: 16s - loss: 1.6766 - accuracy: 0.378 - ETA: 16s - loss: 1.6750 - accuracy: 0.377 - ETA: 16s - loss: 1.6717 - accuracy: 0.378 - ETA: 15s - loss: 1.6701 - accuracy: 0.379 - ETA: 15s - loss: 1.6691 - accuracy: 0.379 - ETA: 15s - loss: 1.6679 - accuracy: 0.380 - ETA: 14s - loss: 1.6664 - accuracy: 0.382 - ETA: 14s - loss: 1.6676 - accuracy: 0.381 - ETA: 13s - loss: 1.6690 - accuracy: 0.380 - ETA: 13s - loss: 1.6684 - accuracy: 0.380 - ETA: 13s - loss: 1.6666 - accuracy: 0.382 - ETA: 12s - loss: 1.6660 - accuracy: 0.381 - ETA: 12s - loss: 1.6665 - accuracy: 0.381 - ETA: 11s - loss: 1.6638 - accuracy: 0.382 - ETA: 11s - loss: 1.6644 - accuracy: 0.382 - ETA: 10s - loss: 1.6637 - accuracy: 0.382 - ETA: 10s - loss: 1.6629 - accuracy: 0.383 - ETA: 10s - loss: 1.6646 - accuracy: 0.381 - ETA: 9s - loss: 1.6632 - accuracy: 0.382 - ETA: 9s - loss: 1.6625 - accuracy: 0.38 - ETA: 8s - loss: 1.6629 - accuracy: 0.38 - ETA: 8s - loss: 1.6636 - accuracy: 0.38 - ETA: 7s - loss: 1.6647 - accuracy: 0.38 - ETA: 7s - loss: 1.6655 - accuracy: 0.38 - ETA: 7s - loss: 1.6651 - accuracy: 0.38 - ETA: 6s - loss: 1.6648 - accuracy: 0.38 - ETA: 6s - loss: 1.6654 - accuracy: 0.38 - ETA: 5s - loss: 1.6669 - accuracy: 0.38 - ETA: 5s - loss: 1.6657 - accuracy: 0.38 - ETA: 4s - loss: 1.6655 - accuracy: 0.38 - ETA: 4s - loss: 1.6645 - accuracy: 0.38 - ETA: 4s - loss: 1.6646 - accuracy: 0.38 - ETA: 3s - loss: 1.6642 - accuracy: 0.38 - ETA: 3s - loss: 1.6637 - accuracy: 0.38 - ETA: 2s - loss: 1.6626 - accuracy: 0.38 - ETA: 2s - loss: 1.6624 - accuracy: 0.38 - ETA: 1s - loss: 1.6621 - accuracy: 0.38 - ETA: 1s - loss: 1.6629 - accuracy: 0.38 - ETA: 1s - loss: 1.6647 - accuracy: 0.38 - ETA: 0s - loss: 1.6661 - accuracy: 0.38 - ETA: 0s - loss: 1.6659 - accuracy: 0.38 - 34s 969us/step - loss: 1.6662 - accuracy: 0.3815 - val_loss: 1.6145 - val_accuracy: 0.3975\n",
      "Epoch 25/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.6744 - accuracy: 0.361 - ETA: 22s - loss: 1.6652 - accuracy: 0.380 - ETA: 23s - loss: 1.6583 - accuracy: 0.378 - ETA: 23s - loss: 1.6563 - accuracy: 0.377 - ETA: 23s - loss: 1.6578 - accuracy: 0.378 - ETA: 22s - loss: 1.6601 - accuracy: 0.380 - ETA: 22s - loss: 1.6608 - accuracy: 0.380 - ETA: 21s - loss: 1.6728 - accuracy: 0.380 - ETA: 21s - loss: 1.6747 - accuracy: 0.380 - ETA: 20s - loss: 1.6828 - accuracy: 0.375 - ETA: 20s - loss: 1.6783 - accuracy: 0.376 - ETA: 19s - loss: 1.6757 - accuracy: 0.377 - ETA: 19s - loss: 1.6775 - accuracy: 0.376 - ETA: 18s - loss: 1.6743 - accuracy: 0.376 - ETA: 18s - loss: 1.6711 - accuracy: 0.378 - ETA: 18s - loss: 1.6692 - accuracy: 0.379 - ETA: 17s - loss: 1.6673 - accuracy: 0.381 - ETA: 17s - loss: 1.6702 - accuracy: 0.379 - ETA: 17s - loss: 1.6678 - accuracy: 0.380 - ETA: 16s - loss: 1.6669 - accuracy: 0.382 - ETA: 16s - loss: 1.6677 - accuracy: 0.382 - ETA: 15s - loss: 1.6690 - accuracy: 0.381 - ETA: 15s - loss: 1.6701 - accuracy: 0.382 - ETA: 14s - loss: 1.6715 - accuracy: 0.381 - ETA: 14s - loss: 1.6706 - accuracy: 0.381 - ETA: 14s - loss: 1.6722 - accuracy: 0.380 - ETA: 13s - loss: 1.6704 - accuracy: 0.381 - ETA: 13s - loss: 1.6702 - accuracy: 0.381 - ETA: 12s - loss: 1.6706 - accuracy: 0.381 - ETA: 12s - loss: 1.6727 - accuracy: 0.381 - ETA: 11s - loss: 1.6738 - accuracy: 0.381 - ETA: 11s - loss: 1.6715 - accuracy: 0.381 - ETA: 10s - loss: 1.6702 - accuracy: 0.382 - ETA: 10s - loss: 1.6692 - accuracy: 0.382 - ETA: 10s - loss: 1.6674 - accuracy: 0.383 - ETA: 9s - loss: 1.6668 - accuracy: 0.383 - ETA: 9s - loss: 1.6653 - accuracy: 0.38 - ETA: 8s - loss: 1.6654 - accuracy: 0.38 - ETA: 8s - loss: 1.6648 - accuracy: 0.38 - ETA: 7s - loss: 1.6637 - accuracy: 0.38 - ETA: 7s - loss: 1.6630 - accuracy: 0.38 - ETA: 6s - loss: 1.6614 - accuracy: 0.38 - ETA: 6s - loss: 1.6613 - accuracy: 0.38 - ETA: 6s - loss: 1.6609 - accuracy: 0.38 - ETA: 5s - loss: 1.6603 - accuracy: 0.38 - ETA: 5s - loss: 1.6605 - accuracy: 0.38 - ETA: 4s - loss: 1.6624 - accuracy: 0.38 - ETA: 4s - loss: 1.6629 - accuracy: 0.38 - ETA: 3s - loss: 1.6631 - accuracy: 0.38 - ETA: 3s - loss: 1.6622 - accuracy: 0.38 - ETA: 3s - loss: 1.6617 - accuracy: 0.38 - ETA: 2s - loss: 1.6608 - accuracy: 0.38 - ETA: 2s - loss: 1.6602 - accuracy: 0.38 - ETA: 1s - loss: 1.6591 - accuracy: 0.38 - ETA: 1s - loss: 1.6594 - accuracy: 0.38 - ETA: 1s - loss: 1.6598 - accuracy: 0.38 - ETA: 0s - loss: 1.6609 - accuracy: 0.38 - ETA: 0s - loss: 1.6615 - accuracy: 0.38 - 34s 962us/step - loss: 1.6617 - accuracy: 0.3829 - val_loss: 1.6933 - val_accuracy: 0.3579\n",
      "Epoch 26/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.7069 - accuracy: 0.370 - ETA: 22s - loss: 1.7283 - accuracy: 0.360 - ETA: 22s - loss: 1.7224 - accuracy: 0.360 - ETA: 22s - loss: 1.6931 - accuracy: 0.377 - ETA: 21s - loss: 1.6779 - accuracy: 0.382 - ETA: 21s - loss: 1.6701 - accuracy: 0.384 - ETA: 21s - loss: 1.6729 - accuracy: 0.380 - ETA: 20s - loss: 1.6675 - accuracy: 0.381 - ETA: 20s - loss: 1.6692 - accuracy: 0.379 - ETA: 20s - loss: 1.6655 - accuracy: 0.380 - ETA: 19s - loss: 1.6585 - accuracy: 0.387 - ETA: 19s - loss: 1.6554 - accuracy: 0.386 - ETA: 19s - loss: 1.6581 - accuracy: 0.385 - ETA: 18s - loss: 1.6650 - accuracy: 0.383 - ETA: 18s - loss: 1.6667 - accuracy: 0.384 - ETA: 18s - loss: 1.6649 - accuracy: 0.383 - ETA: 17s - loss: 1.6617 - accuracy: 0.384 - ETA: 17s - loss: 1.6631 - accuracy: 0.385 - ETA: 17s - loss: 1.6614 - accuracy: 0.386 - ETA: 16s - loss: 1.6612 - accuracy: 0.385 - ETA: 16s - loss: 1.6613 - accuracy: 0.384 - ETA: 15s - loss: 1.6602 - accuracy: 0.383 - ETA: 15s - loss: 1.6594 - accuracy: 0.383 - ETA: 15s - loss: 1.6577 - accuracy: 0.384 - ETA: 14s - loss: 1.6541 - accuracy: 0.385 - ETA: 14s - loss: 1.6521 - accuracy: 0.386 - ETA: 13s - loss: 1.6504 - accuracy: 0.386 - ETA: 13s - loss: 1.6502 - accuracy: 0.387 - ETA: 12s - loss: 1.6474 - accuracy: 0.389 - ETA: 12s - loss: 1.6478 - accuracy: 0.388 - ETA: 11s - loss: 1.6480 - accuracy: 0.388 - ETA: 11s - loss: 1.6509 - accuracy: 0.387 - ETA: 10s - loss: 1.6548 - accuracy: 0.385 - ETA: 10s - loss: 1.6539 - accuracy: 0.385 - ETA: 10s - loss: 1.6526 - accuracy: 0.386 - ETA: 9s - loss: 1.6507 - accuracy: 0.386 - ETA: 9s - loss: 1.6499 - accuracy: 0.38 - ETA: 8s - loss: 1.6502 - accuracy: 0.38 - ETA: 8s - loss: 1.6490 - accuracy: 0.38 - ETA: 7s - loss: 1.6486 - accuracy: 0.38 - ETA: 7s - loss: 1.6487 - accuracy: 0.38 - ETA: 7s - loss: 1.6494 - accuracy: 0.38 - ETA: 6s - loss: 1.6496 - accuracy: 0.38 - ETA: 6s - loss: 1.6497 - accuracy: 0.38 - ETA: 5s - loss: 1.6502 - accuracy: 0.38 - ETA: 5s - loss: 1.6497 - accuracy: 0.38 - ETA: 4s - loss: 1.6486 - accuracy: 0.38 - ETA: 4s - loss: 1.6479 - accuracy: 0.38 - ETA: 4s - loss: 1.6472 - accuracy: 0.38 - ETA: 3s - loss: 1.6472 - accuracy: 0.38 - ETA: 3s - loss: 1.6471 - accuracy: 0.38 - ETA: 2s - loss: 1.6457 - accuracy: 0.38 - ETA: 2s - loss: 1.6460 - accuracy: 0.38 - ETA: 1s - loss: 1.6477 - accuracy: 0.38 - ETA: 1s - loss: 1.6484 - accuracy: 0.38 - ETA: 1s - loss: 1.6488 - accuracy: 0.38 - ETA: 0s - loss: 1.6482 - accuracy: 0.38 - ETA: 0s - loss: 1.6474 - accuracy: 0.38 - 34s 964us/step - loss: 1.6481 - accuracy: 0.3883 - val_loss: 1.6028 - val_accuracy: 0.3989\n",
      "Epoch 27/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 1.6819 - accuracy: 0.366 - ETA: 22s - loss: 1.6497 - accuracy: 0.381 - ETA: 22s - loss: 1.6357 - accuracy: 0.383 - ETA: 22s - loss: 1.6372 - accuracy: 0.390 - ETA: 22s - loss: 1.6395 - accuracy: 0.389 - ETA: 22s - loss: 1.6520 - accuracy: 0.385 - ETA: 21s - loss: 1.6568 - accuracy: 0.384 - ETA: 21s - loss: 1.6582 - accuracy: 0.384 - ETA: 21s - loss: 1.6481 - accuracy: 0.387 - ETA: 21s - loss: 1.6432 - accuracy: 0.390 - ETA: 20s - loss: 1.6430 - accuracy: 0.388 - ETA: 20s - loss: 1.6448 - accuracy: 0.387 - ETA: 19s - loss: 1.6406 - accuracy: 0.387 - ETA: 19s - loss: 1.6404 - accuracy: 0.387 - ETA: 19s - loss: 1.6379 - accuracy: 0.387 - ETA: 18s - loss: 1.6371 - accuracy: 0.388 - ETA: 18s - loss: 1.6370 - accuracy: 0.389 - ETA: 17s - loss: 1.6401 - accuracy: 0.386 - ETA: 17s - loss: 1.6391 - accuracy: 0.387 - ETA: 16s - loss: 1.6386 - accuracy: 0.387 - ETA: 16s - loss: 1.6379 - accuracy: 0.386 - ETA: 15s - loss: 1.6382 - accuracy: 0.386 - ETA: 15s - loss: 1.6343 - accuracy: 0.387 - ETA: 15s - loss: 1.6340 - accuracy: 0.388 - ETA: 14s - loss: 1.6341 - accuracy: 0.388 - ETA: 14s - loss: 1.6337 - accuracy: 0.389 - ETA: 13s - loss: 1.6316 - accuracy: 0.389 - ETA: 13s - loss: 1.6344 - accuracy: 0.389 - ETA: 12s - loss: 1.6354 - accuracy: 0.389 - ETA: 12s - loss: 1.6382 - accuracy: 0.387 - ETA: 11s - loss: 1.6384 - accuracy: 0.388 - ETA: 11s - loss: 1.6374 - accuracy: 0.389 - ETA: 11s - loss: 1.6360 - accuracy: 0.389 - ETA: 10s - loss: 1.6363 - accuracy: 0.389 - ETA: 10s - loss: 1.6338 - accuracy: 0.390 - ETA: 9s - loss: 1.6331 - accuracy: 0.390 - ETA: 9s - loss: 1.6325 - accuracy: 0.39 - ETA: 8s - loss: 1.6325 - accuracy: 0.39 - ETA: 8s - loss: 1.6322 - accuracy: 0.39 - ETA: 7s - loss: 1.6325 - accuracy: 0.39 - ETA: 7s - loss: 1.6338 - accuracy: 0.39 - ETA: 7s - loss: 1.6364 - accuracy: 0.38 - ETA: 6s - loss: 1.6363 - accuracy: 0.38 - ETA: 6s - loss: 1.6371 - accuracy: 0.38 - ETA: 5s - loss: 1.6364 - accuracy: 0.38 - ETA: 5s - loss: 1.6360 - accuracy: 0.38 - ETA: 4s - loss: 1.6348 - accuracy: 0.38 - ETA: 4s - loss: 1.6351 - accuracy: 0.38 - ETA: 4s - loss: 1.6365 - accuracy: 0.38 - ETA: 3s - loss: 1.6392 - accuracy: 0.38 - ETA: 3s - loss: 1.6393 - accuracy: 0.38 - ETA: 2s - loss: 1.6389 - accuracy: 0.38 - ETA: 2s - loss: 1.6382 - accuracy: 0.38 - ETA: 1s - loss: 1.6388 - accuracy: 0.38 - ETA: 1s - loss: 1.6378 - accuracy: 0.38 - ETA: 1s - loss: 1.6366 - accuracy: 0.38 - ETA: 0s - loss: 1.6366 - accuracy: 0.38 - ETA: 0s - loss: 1.6363 - accuracy: 0.38 - 34s 965us/step - loss: 1.6365 - accuracy: 0.3881 - val_loss: 1.6022 - val_accuracy: 0.3919\n",
      "Epoch 28/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.5983 - accuracy: 0.401 - ETA: 22s - loss: 1.5751 - accuracy: 0.412 - ETA: 22s - loss: 1.6061 - accuracy: 0.400 - ETA: 22s - loss: 1.6125 - accuracy: 0.399 - ETA: 22s - loss: 1.6321 - accuracy: 0.393 - ETA: 22s - loss: 1.6449 - accuracy: 0.386 - ETA: 21s - loss: 1.6509 - accuracy: 0.384 - ETA: 21s - loss: 1.6453 - accuracy: 0.388 - ETA: 20s - loss: 1.6435 - accuracy: 0.386 - ETA: 20s - loss: 1.6448 - accuracy: 0.384 - ETA: 20s - loss: 1.6376 - accuracy: 0.387 - ETA: 19s - loss: 1.6402 - accuracy: 0.386 - ETA: 19s - loss: 1.6376 - accuracy: 0.386 - ETA: 18s - loss: 1.6343 - accuracy: 0.388 - ETA: 18s - loss: 1.6363 - accuracy: 0.389 - ETA: 18s - loss: 1.6358 - accuracy: 0.390 - ETA: 17s - loss: 1.6370 - accuracy: 0.389 - ETA: 17s - loss: 1.6330 - accuracy: 0.391 - ETA: 16s - loss: 1.6348 - accuracy: 0.391 - ETA: 16s - loss: 1.6390 - accuracy: 0.390 - ETA: 15s - loss: 1.6402 - accuracy: 0.390 - ETA: 15s - loss: 1.6418 - accuracy: 0.388 - ETA: 15s - loss: 1.6397 - accuracy: 0.390 - ETA: 14s - loss: 1.6415 - accuracy: 0.389 - ETA: 14s - loss: 1.6399 - accuracy: 0.390 - ETA: 13s - loss: 1.6396 - accuracy: 0.390 - ETA: 13s - loss: 1.6419 - accuracy: 0.388 - ETA: 12s - loss: 1.6413 - accuracy: 0.388 - ETA: 12s - loss: 1.6409 - accuracy: 0.388 - ETA: 12s - loss: 1.6396 - accuracy: 0.388 - ETA: 11s - loss: 1.6383 - accuracy: 0.389 - ETA: 11s - loss: 1.6381 - accuracy: 0.389 - ETA: 10s - loss: 1.6401 - accuracy: 0.388 - ETA: 10s - loss: 1.6433 - accuracy: 0.386 - ETA: 9s - loss: 1.6427 - accuracy: 0.387 - ETA: 9s - loss: 1.6422 - accuracy: 0.38 - ETA: 9s - loss: 1.6409 - accuracy: 0.38 - ETA: 8s - loss: 1.6422 - accuracy: 0.38 - ETA: 8s - loss: 1.6447 - accuracy: 0.38 - ETA: 7s - loss: 1.6472 - accuracy: 0.38 - ETA: 7s - loss: 1.6481 - accuracy: 0.38 - ETA: 7s - loss: 1.6478 - accuracy: 0.38 - ETA: 6s - loss: 1.6469 - accuracy: 0.38 - ETA: 6s - loss: 1.6467 - accuracy: 0.38 - ETA: 5s - loss: 1.6459 - accuracy: 0.38 - ETA: 5s - loss: 1.6470 - accuracy: 0.38 - ETA: 4s - loss: 1.6467 - accuracy: 0.38 - ETA: 4s - loss: 1.6466 - accuracy: 0.38 - ETA: 4s - loss: 1.6460 - accuracy: 0.38 - ETA: 3s - loss: 1.6455 - accuracy: 0.38 - ETA: 3s - loss: 1.6459 - accuracy: 0.38 - ETA: 2s - loss: 1.6454 - accuracy: 0.38 - ETA: 2s - loss: 1.6442 - accuracy: 0.38 - ETA: 1s - loss: 1.6432 - accuracy: 0.38 - ETA: 1s - loss: 1.6423 - accuracy: 0.38 - ETA: 1s - loss: 1.6420 - accuracy: 0.38 - ETA: 0s - loss: 1.6420 - accuracy: 0.38 - ETA: 0s - loss: 1.6418 - accuracy: 0.38 - 34s 965us/step - loss: 1.6421 - accuracy: 0.3874 - val_loss: 1.6702 - val_accuracy: 0.3706\n",
      "Epoch 29/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.7476 - accuracy: 0.321 - ETA: 23s - loss: 1.7059 - accuracy: 0.355 - ETA: 23s - loss: 1.6930 - accuracy: 0.363 - ETA: 23s - loss: 1.6554 - accuracy: 0.378 - ETA: 22s - loss: 1.6436 - accuracy: 0.380 - ETA: 22s - loss: 1.6377 - accuracy: 0.380 - ETA: 22s - loss: 1.6332 - accuracy: 0.381 - ETA: 21s - loss: 1.6295 - accuracy: 0.382 - ETA: 21s - loss: 1.6264 - accuracy: 0.384 - ETA: 21s - loss: 1.6298 - accuracy: 0.382 - ETA: 20s - loss: 1.6296 - accuracy: 0.384 - ETA: 20s - loss: 1.6301 - accuracy: 0.385 - ETA: 19s - loss: 1.6389 - accuracy: 0.381 - ETA: 19s - loss: 1.6382 - accuracy: 0.380 - ETA: 18s - loss: 1.6373 - accuracy: 0.379 - ETA: 18s - loss: 1.6361 - accuracy: 0.379 - ETA: 17s - loss: 1.6349 - accuracy: 0.381 - ETA: 17s - loss: 1.6349 - accuracy: 0.382 - ETA: 17s - loss: 1.6315 - accuracy: 0.382 - ETA: 16s - loss: 1.6296 - accuracy: 0.384 - ETA: 16s - loss: 1.6275 - accuracy: 0.384 - ETA: 15s - loss: 1.6258 - accuracy: 0.385 - ETA: 15s - loss: 1.6254 - accuracy: 0.386 - ETA: 14s - loss: 1.6245 - accuracy: 0.386 - ETA: 14s - loss: 1.6235 - accuracy: 0.387 - ETA: 13s - loss: 1.6255 - accuracy: 0.386 - ETA: 13s - loss: 1.6323 - accuracy: 0.385 - ETA: 13s - loss: 1.6342 - accuracy: 0.385 - ETA: 12s - loss: 1.6358 - accuracy: 0.386 - ETA: 12s - loss: 1.6374 - accuracy: 0.385 - ETA: 11s - loss: 1.6374 - accuracy: 0.385 - ETA: 11s - loss: 1.6355 - accuracy: 0.385 - ETA: 10s - loss: 1.6336 - accuracy: 0.386 - ETA: 10s - loss: 1.6332 - accuracy: 0.386 - ETA: 10s - loss: 1.6318 - accuracy: 0.386 - ETA: 9s - loss: 1.6298 - accuracy: 0.387 - ETA: 9s - loss: 1.6294 - accuracy: 0.38 - ETA: 8s - loss: 1.6281 - accuracy: 0.38 - ETA: 8s - loss: 1.6287 - accuracy: 0.38 - ETA: 7s - loss: 1.6280 - accuracy: 0.38 - ETA: 7s - loss: 1.6295 - accuracy: 0.38 - ETA: 7s - loss: 1.6303 - accuracy: 0.38 - ETA: 6s - loss: 1.6309 - accuracy: 0.38 - ETA: 6s - loss: 1.6301 - accuracy: 0.38 - ETA: 5s - loss: 1.6279 - accuracy: 0.38 - ETA: 5s - loss: 1.6271 - accuracy: 0.38 - ETA: 4s - loss: 1.6258 - accuracy: 0.38 - ETA: 4s - loss: 1.6247 - accuracy: 0.38 - ETA: 4s - loss: 1.6230 - accuracy: 0.39 - ETA: 3s - loss: 1.6242 - accuracy: 0.39 - ETA: 3s - loss: 1.6278 - accuracy: 0.38 - ETA: 2s - loss: 1.6302 - accuracy: 0.38 - ETA: 2s - loss: 1.6304 - accuracy: 0.38 - ETA: 1s - loss: 1.6301 - accuracy: 0.38 - ETA: 1s - loss: 1.6298 - accuracy: 0.38 - ETA: 1s - loss: 1.6305 - accuracy: 0.38 - ETA: 0s - loss: 1.6303 - accuracy: 0.38 - ETA: 0s - loss: 1.6301 - accuracy: 0.38 - 34s 969us/step - loss: 1.6300 - accuracy: 0.3887 - val_loss: 1.5649 - val_accuracy: 0.4036\n",
      "Epoch 30/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 1.6293 - accuracy: 0.381 - ETA: 25s - loss: 1.6055 - accuracy: 0.397 - ETA: 24s - loss: 1.5884 - accuracy: 0.405 - ETA: 24s - loss: 1.5997 - accuracy: 0.405 - ETA: 23s - loss: 1.5931 - accuracy: 0.407 - ETA: 22s - loss: 1.5928 - accuracy: 0.405 - ETA: 22s - loss: 1.5880 - accuracy: 0.406 - ETA: 21s - loss: 1.5861 - accuracy: 0.404 - ETA: 21s - loss: 1.5823 - accuracy: 0.404 - ETA: 20s - loss: 1.5801 - accuracy: 0.406 - ETA: 20s - loss: 1.5790 - accuracy: 0.405 - ETA: 19s - loss: 1.5870 - accuracy: 0.401 - ETA: 19s - loss: 1.6031 - accuracy: 0.396 - ETA: 19s - loss: 1.6182 - accuracy: 0.391 - ETA: 18s - loss: 1.6216 - accuracy: 0.390 - ETA: 18s - loss: 1.6242 - accuracy: 0.387 - ETA: 17s - loss: 1.6209 - accuracy: 0.389 - ETA: 17s - loss: 1.6209 - accuracy: 0.390 - ETA: 16s - loss: 1.6240 - accuracy: 0.388 - ETA: 16s - loss: 1.6218 - accuracy: 0.389 - ETA: 15s - loss: 1.6239 - accuracy: 0.389 - ETA: 15s - loss: 1.6252 - accuracy: 0.389 - ETA: 15s - loss: 1.6268 - accuracy: 0.389 - ETA: 14s - loss: 1.6262 - accuracy: 0.389 - ETA: 14s - loss: 1.6264 - accuracy: 0.390 - ETA: 13s - loss: 1.6252 - accuracy: 0.390 - ETA: 13s - loss: 1.6247 - accuracy: 0.389 - ETA: 12s - loss: 1.6224 - accuracy: 0.390 - ETA: 12s - loss: 1.6211 - accuracy: 0.391 - ETA: 12s - loss: 1.6207 - accuracy: 0.391 - ETA: 11s - loss: 1.6200 - accuracy: 0.391 - ETA: 11s - loss: 1.6176 - accuracy: 0.392 - ETA: 10s - loss: 1.6181 - accuracy: 0.392 - ETA: 10s - loss: 1.6168 - accuracy: 0.392 - ETA: 10s - loss: 1.6155 - accuracy: 0.392 - ETA: 9s - loss: 1.6134 - accuracy: 0.394 - ETA: 9s - loss: 1.6118 - accuracy: 0.39 - ETA: 8s - loss: 1.6111 - accuracy: 0.39 - ETA: 8s - loss: 1.6114 - accuracy: 0.39 - ETA: 8s - loss: 1.6110 - accuracy: 0.39 - ETA: 7s - loss: 1.6113 - accuracy: 0.39 - ETA: 7s - loss: 1.6136 - accuracy: 0.39 - ETA: 6s - loss: 1.6159 - accuracy: 0.39 - ETA: 6s - loss: 1.6171 - accuracy: 0.39 - ETA: 5s - loss: 1.6176 - accuracy: 0.39 - ETA: 5s - loss: 1.6170 - accuracy: 0.39 - ETA: 4s - loss: 1.6167 - accuracy: 0.39 - ETA: 4s - loss: 1.6162 - accuracy: 0.39 - ETA: 4s - loss: 1.6140 - accuracy: 0.39 - ETA: 3s - loss: 1.6132 - accuracy: 0.39 - ETA: 3s - loss: 1.6130 - accuracy: 0.39 - ETA: 2s - loss: 1.6150 - accuracy: 0.39 - ETA: 2s - loss: 1.6159 - accuracy: 0.39 - ETA: 1s - loss: 1.6149 - accuracy: 0.39 - ETA: 1s - loss: 1.6152 - accuracy: 0.39 - ETA: 1s - loss: 1.6160 - accuracy: 0.39 - ETA: 0s - loss: 1.6159 - accuracy: 0.39 - ETA: 0s - loss: 1.6159 - accuracy: 0.39 - 34s 979us/step - loss: 1.6160 - accuracy: 0.3942 - val_loss: 1.5621 - val_accuracy: 0.4067\n",
      "Epoch 31/45\n",
      "35000/35000 [==============================] - ETA: 24s - loss: 1.5724 - accuracy: 0.396 - ETA: 24s - loss: 1.5646 - accuracy: 0.404 - ETA: 23s - loss: 1.5757 - accuracy: 0.398 - ETA: 23s - loss: 1.5688 - accuracy: 0.400 - ETA: 22s - loss: 1.5705 - accuracy: 0.406 - ETA: 22s - loss: 1.5764 - accuracy: 0.406 - ETA: 22s - loss: 1.5865 - accuracy: 0.406 - ETA: 21s - loss: 1.5822 - accuracy: 0.408 - ETA: 21s - loss: 1.5900 - accuracy: 0.402 - ETA: 20s - loss: 1.5929 - accuracy: 0.399 - ETA: 20s - loss: 1.5955 - accuracy: 0.400 - ETA: 19s - loss: 1.5937 - accuracy: 0.400 - ETA: 19s - loss: 1.5922 - accuracy: 0.400 - ETA: 19s - loss: 1.5931 - accuracy: 0.400 - ETA: 18s - loss: 1.5992 - accuracy: 0.399 - ETA: 18s - loss: 1.6079 - accuracy: 0.397 - ETA: 17s - loss: 1.6148 - accuracy: 0.393 - ETA: 17s - loss: 1.6152 - accuracy: 0.393 - ETA: 16s - loss: 1.6156 - accuracy: 0.393 - ETA: 16s - loss: 1.6155 - accuracy: 0.392 - ETA: 15s - loss: 1.6160 - accuracy: 0.391 - ETA: 15s - loss: 1.6117 - accuracy: 0.394 - ETA: 15s - loss: 1.6085 - accuracy: 0.395 - ETA: 14s - loss: 1.6070 - accuracy: 0.397 - ETA: 14s - loss: 1.6040 - accuracy: 0.398 - ETA: 13s - loss: 1.6048 - accuracy: 0.398 - ETA: 13s - loss: 1.6050 - accuracy: 0.398 - ETA: 13s - loss: 1.6069 - accuracy: 0.398 - ETA: 12s - loss: 1.6069 - accuracy: 0.398 - ETA: 12s - loss: 1.6060 - accuracy: 0.398 - ETA: 11s - loss: 1.6053 - accuracy: 0.398 - ETA: 11s - loss: 1.6050 - accuracy: 0.400 - ETA: 10s - loss: 1.6066 - accuracy: 0.398 - ETA: 10s - loss: 1.6082 - accuracy: 0.398 - ETA: 10s - loss: 1.6119 - accuracy: 0.397 - ETA: 9s - loss: 1.6129 - accuracy: 0.396 - ETA: 9s - loss: 1.6127 - accuracy: 0.39 - ETA: 8s - loss: 1.6117 - accuracy: 0.39 - ETA: 8s - loss: 1.6110 - accuracy: 0.39 - ETA: 7s - loss: 1.6087 - accuracy: 0.39 - ETA: 7s - loss: 1.6086 - accuracy: 0.39 - ETA: 7s - loss: 1.6075 - accuracy: 0.39 - ETA: 6s - loss: 1.6063 - accuracy: 0.39 - ETA: 6s - loss: 1.6049 - accuracy: 0.39 - ETA: 5s - loss: 1.6041 - accuracy: 0.39 - ETA: 5s - loss: 1.6050 - accuracy: 0.39 - ETA: 4s - loss: 1.6038 - accuracy: 0.39 - ETA: 4s - loss: 1.6039 - accuracy: 0.39 - ETA: 4s - loss: 1.6042 - accuracy: 0.39 - ETA: 3s - loss: 1.6041 - accuracy: 0.39 - ETA: 3s - loss: 1.6037 - accuracy: 0.39 - ETA: 2s - loss: 1.6032 - accuracy: 0.39 - ETA: 2s - loss: 1.6021 - accuracy: 0.39 - ETA: 1s - loss: 1.6007 - accuracy: 0.40 - ETA: 1s - loss: 1.6015 - accuracy: 0.39 - ETA: 0s - loss: 1.6029 - accuracy: 0.39 - ETA: 0s - loss: 1.6034 - accuracy: 0.39 - ETA: 0s - loss: 1.6034 - accuracy: 0.39 - 34s 960us/step - loss: 1.6034 - accuracy: 0.3979 - val_loss: 1.5530 - val_accuracy: 0.4061\n",
      "Epoch 32/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.5616 - accuracy: 0.395 - ETA: 22s - loss: 1.5545 - accuracy: 0.400 - ETA: 22s - loss: 1.5821 - accuracy: 0.391 - ETA: 22s - loss: 1.5961 - accuracy: 0.386 - ETA: 22s - loss: 1.6053 - accuracy: 0.385 - ETA: 21s - loss: 1.6065 - accuracy: 0.388 - ETA: 21s - loss: 1.6106 - accuracy: 0.387 - ETA: 21s - loss: 1.6102 - accuracy: 0.389 - ETA: 20s - loss: 1.6059 - accuracy: 0.391 - ETA: 20s - loss: 1.5985 - accuracy: 0.396 - ETA: 20s - loss: 1.5960 - accuracy: 0.398 - ETA: 19s - loss: 1.5901 - accuracy: 0.400 - ETA: 19s - loss: 1.5863 - accuracy: 0.400 - ETA: 18s - loss: 1.5850 - accuracy: 0.399 - ETA: 18s - loss: 1.5913 - accuracy: 0.397 - ETA: 18s - loss: 1.5983 - accuracy: 0.395 - ETA: 17s - loss: 1.6107 - accuracy: 0.391 - ETA: 17s - loss: 1.6121 - accuracy: 0.391 - ETA: 16s - loss: 1.6092 - accuracy: 0.392 - ETA: 16s - loss: 1.6081 - accuracy: 0.393 - ETA: 15s - loss: 1.6076 - accuracy: 0.393 - ETA: 15s - loss: 1.6054 - accuracy: 0.395 - ETA: 15s - loss: 1.6033 - accuracy: 0.396 - ETA: 14s - loss: 1.6016 - accuracy: 0.398 - ETA: 14s - loss: 1.5990 - accuracy: 0.398 - ETA: 13s - loss: 1.5999 - accuracy: 0.396 - ETA: 13s - loss: 1.6041 - accuracy: 0.394 - ETA: 13s - loss: 1.6088 - accuracy: 0.393 - ETA: 12s - loss: 1.6115 - accuracy: 0.391 - ETA: 12s - loss: 1.6118 - accuracy: 0.391 - ETA: 11s - loss: 1.6111 - accuracy: 0.392 - ETA: 11s - loss: 1.6106 - accuracy: 0.392 - ETA: 10s - loss: 1.6075 - accuracy: 0.394 - ETA: 10s - loss: 1.6065 - accuracy: 0.395 - ETA: 10s - loss: 1.6050 - accuracy: 0.396 - ETA: 9s - loss: 1.6041 - accuracy: 0.397 - ETA: 9s - loss: 1.6027 - accuracy: 0.39 - ETA: 8s - loss: 1.6019 - accuracy: 0.39 - ETA: 8s - loss: 1.5997 - accuracy: 0.39 - ETA: 7s - loss: 1.5974 - accuracy: 0.39 - ETA: 7s - loss: 1.5973 - accuracy: 0.39 - ETA: 7s - loss: 1.5969 - accuracy: 0.39 - ETA: 6s - loss: 1.5984 - accuracy: 0.39 - ETA: 6s - loss: 1.6004 - accuracy: 0.39 - ETA: 5s - loss: 1.6024 - accuracy: 0.39 - ETA: 5s - loss: 1.6029 - accuracy: 0.39 - ETA: 4s - loss: 1.6029 - accuracy: 0.39 - ETA: 4s - loss: 1.6024 - accuracy: 0.39 - ETA: 3s - loss: 1.6027 - accuracy: 0.39 - ETA: 3s - loss: 1.6024 - accuracy: 0.39 - ETA: 3s - loss: 1.6020 - accuracy: 0.39 - ETA: 2s - loss: 1.6016 - accuracy: 0.39 - ETA: 2s - loss: 1.6009 - accuracy: 0.39 - ETA: 1s - loss: 1.6001 - accuracy: 0.39 - ETA: 1s - loss: 1.6001 - accuracy: 0.39 - ETA: 0s - loss: 1.5999 - accuracy: 0.39 - ETA: 0s - loss: 1.5997 - accuracy: 0.39 - ETA: 0s - loss: 1.5999 - accuracy: 0.39 - 34s 964us/step - loss: 1.5999 - accuracy: 0.3959 - val_loss: 1.5984 - val_accuracy: 0.3954\n",
      "Epoch 33/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5658 - accuracy: 0.425 - ETA: 23s - loss: 1.6327 - accuracy: 0.390 - ETA: 23s - loss: 1.6208 - accuracy: 0.398 - ETA: 22s - loss: 1.6196 - accuracy: 0.395 - ETA: 22s - loss: 1.6198 - accuracy: 0.395 - ETA: 22s - loss: 1.6145 - accuracy: 0.399 - ETA: 21s - loss: 1.6143 - accuracy: 0.400 - ETA: 21s - loss: 1.6085 - accuracy: 0.401 - ETA: 20s - loss: 1.6081 - accuracy: 0.399 - ETA: 20s - loss: 1.6043 - accuracy: 0.400 - ETA: 20s - loss: 1.6017 - accuracy: 0.400 - ETA: 19s - loss: 1.6041 - accuracy: 0.398 - ETA: 19s - loss: 1.6010 - accuracy: 0.400 - ETA: 18s - loss: 1.6031 - accuracy: 0.399 - ETA: 18s - loss: 1.6022 - accuracy: 0.400 - ETA: 17s - loss: 1.6010 - accuracy: 0.400 - ETA: 17s - loss: 1.6001 - accuracy: 0.400 - ETA: 17s - loss: 1.5991 - accuracy: 0.400 - ETA: 16s - loss: 1.5981 - accuracy: 0.400 - ETA: 16s - loss: 1.5973 - accuracy: 0.400 - ETA: 16s - loss: 1.5997 - accuracy: 0.401 - ETA: 15s - loss: 1.6075 - accuracy: 0.398 - ETA: 15s - loss: 1.6115 - accuracy: 0.395 - ETA: 14s - loss: 1.6096 - accuracy: 0.396 - ETA: 14s - loss: 1.6087 - accuracy: 0.396 - ETA: 13s - loss: 1.6072 - accuracy: 0.397 - ETA: 13s - loss: 1.6043 - accuracy: 0.398 - ETA: 13s - loss: 1.6033 - accuracy: 0.399 - ETA: 12s - loss: 1.6041 - accuracy: 0.398 - ETA: 12s - loss: 1.6023 - accuracy: 0.399 - ETA: 11s - loss: 1.5998 - accuracy: 0.400 - ETA: 11s - loss: 1.6013 - accuracy: 0.399 - ETA: 10s - loss: 1.6003 - accuracy: 0.400 - ETA: 10s - loss: 1.5993 - accuracy: 0.400 - ETA: 10s - loss: 1.5989 - accuracy: 0.400 - ETA: 9s - loss: 1.5999 - accuracy: 0.399 - ETA: 9s - loss: 1.5990 - accuracy: 0.39 - ETA: 8s - loss: 1.5982 - accuracy: 0.39 - ETA: 8s - loss: 1.5966 - accuracy: 0.40 - ETA: 7s - loss: 1.5968 - accuracy: 0.40 - ETA: 7s - loss: 1.5971 - accuracy: 0.39 - ETA: 7s - loss: 1.5971 - accuracy: 0.39 - ETA: 6s - loss: 1.5987 - accuracy: 0.39 - ETA: 6s - loss: 1.5997 - accuracy: 0.39 - ETA: 5s - loss: 1.5992 - accuracy: 0.39 - ETA: 5s - loss: 1.5982 - accuracy: 0.39 - ETA: 4s - loss: 1.5968 - accuracy: 0.40 - ETA: 4s - loss: 1.5972 - accuracy: 0.39 - ETA: 4s - loss: 1.5970 - accuracy: 0.39 - ETA: 3s - loss: 1.5975 - accuracy: 0.39 - ETA: 3s - loss: 1.5975 - accuracy: 0.39 - ETA: 2s - loss: 1.5985 - accuracy: 0.39 - ETA: 2s - loss: 1.5992 - accuracy: 0.39 - ETA: 1s - loss: 1.5990 - accuracy: 0.39 - ETA: 1s - loss: 1.5994 - accuracy: 0.39 - ETA: 1s - loss: 1.5990 - accuracy: 0.39 - ETA: 0s - loss: 1.5982 - accuracy: 0.39 - ETA: 0s - loss: 1.5983 - accuracy: 0.39 - 34s 968us/step - loss: 1.5988 - accuracy: 0.3984 - val_loss: 1.5652 - val_accuracy: 0.3995\n",
      "Epoch 34/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.5840 - accuracy: 0.385 - ETA: 23s - loss: 1.5957 - accuracy: 0.387 - ETA: 23s - loss: 1.5904 - accuracy: 0.392 - ETA: 22s - loss: 1.5760 - accuracy: 0.397 - ETA: 22s - loss: 1.5859 - accuracy: 0.393 - ETA: 22s - loss: 1.5870 - accuracy: 0.389 - ETA: 21s - loss: 1.5817 - accuracy: 0.393 - ETA: 21s - loss: 1.5788 - accuracy: 0.394 - ETA: 21s - loss: 1.5752 - accuracy: 0.399 - ETA: 20s - loss: 1.5783 - accuracy: 0.399 - ETA: 20s - loss: 1.5860 - accuracy: 0.397 - ETA: 19s - loss: 1.5893 - accuracy: 0.397 - ETA: 19s - loss: 1.5918 - accuracy: 0.395 - ETA: 19s - loss: 1.5883 - accuracy: 0.399 - ETA: 18s - loss: 1.5886 - accuracy: 0.398 - ETA: 18s - loss: 1.5880 - accuracy: 0.399 - ETA: 17s - loss: 1.5865 - accuracy: 0.400 - ETA: 17s - loss: 1.5857 - accuracy: 0.402 - ETA: 17s - loss: 1.5849 - accuracy: 0.403 - ETA: 16s - loss: 1.5862 - accuracy: 0.403 - ETA: 16s - loss: 1.5860 - accuracy: 0.403 - ETA: 15s - loss: 1.5875 - accuracy: 0.403 - ETA: 15s - loss: 1.5868 - accuracy: 0.401 - ETA: 14s - loss: 1.5859 - accuracy: 0.401 - ETA: 14s - loss: 1.5845 - accuracy: 0.401 - ETA: 14s - loss: 1.5833 - accuracy: 0.402 - ETA: 13s - loss: 1.5829 - accuracy: 0.403 - ETA: 13s - loss: 1.5839 - accuracy: 0.403 - ETA: 12s - loss: 1.5831 - accuracy: 0.404 - ETA: 12s - loss: 1.5813 - accuracy: 0.404 - ETA: 11s - loss: 1.5806 - accuracy: 0.404 - ETA: 11s - loss: 1.5806 - accuracy: 0.404 - ETA: 10s - loss: 1.5813 - accuracy: 0.403 - ETA: 10s - loss: 1.5819 - accuracy: 0.402 - ETA: 10s - loss: 1.5836 - accuracy: 0.402 - ETA: 9s - loss: 1.5864 - accuracy: 0.401 - ETA: 9s - loss: 1.5872 - accuracy: 0.40 - ETA: 8s - loss: 1.5869 - accuracy: 0.40 - ETA: 8s - loss: 1.5872 - accuracy: 0.40 - ETA: 7s - loss: 1.5876 - accuracy: 0.40 - ETA: 7s - loss: 1.5866 - accuracy: 0.40 - ETA: 6s - loss: 1.5870 - accuracy: 0.40 - ETA: 6s - loss: 1.5862 - accuracy: 0.40 - ETA: 6s - loss: 1.5859 - accuracy: 0.40 - ETA: 5s - loss: 1.5844 - accuracy: 0.40 - ETA: 5s - loss: 1.5846 - accuracy: 0.40 - ETA: 4s - loss: 1.5841 - accuracy: 0.40 - ETA: 4s - loss: 1.5829 - accuracy: 0.40 - ETA: 3s - loss: 1.5827 - accuracy: 0.40 - ETA: 3s - loss: 1.5816 - accuracy: 0.40 - ETA: 3s - loss: 1.5819 - accuracy: 0.40 - ETA: 2s - loss: 1.5815 - accuracy: 0.40 - ETA: 2s - loss: 1.5807 - accuracy: 0.40 - ETA: 1s - loss: 1.5809 - accuracy: 0.40 - ETA: 1s - loss: 1.5807 - accuracy: 0.40 - ETA: 0s - loss: 1.5799 - accuracy: 0.40 - ETA: 0s - loss: 1.5797 - accuracy: 0.40 - ETA: 0s - loss: 1.5796 - accuracy: 0.40 - 34s 960us/step - loss: 1.5798 - accuracy: 0.4030 - val_loss: 1.5995 - val_accuracy: 0.3951\n",
      "Epoch 35/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.6365 - accuracy: 0.361 - ETA: 22s - loss: 1.5886 - accuracy: 0.392 - ETA: 22s - loss: 1.5820 - accuracy: 0.397 - ETA: 22s - loss: 1.5932 - accuracy: 0.389 - ETA: 22s - loss: 1.6232 - accuracy: 0.382 - ETA: 21s - loss: 1.6514 - accuracy: 0.370 - ETA: 21s - loss: 1.6556 - accuracy: 0.366 - ETA: 21s - loss: 1.6472 - accuracy: 0.370 - ETA: 20s - loss: 1.6326 - accuracy: 0.377 - ETA: 20s - loss: 1.6246 - accuracy: 0.380 - ETA: 20s - loss: 1.6135 - accuracy: 0.385 - ETA: 19s - loss: 1.6121 - accuracy: 0.387 - ETA: 19s - loss: 1.6085 - accuracy: 0.387 - ETA: 19s - loss: 1.6043 - accuracy: 0.389 - ETA: 18s - loss: 1.5970 - accuracy: 0.393 - ETA: 18s - loss: 1.5892 - accuracy: 0.396 - ETA: 17s - loss: 1.5862 - accuracy: 0.398 - ETA: 17s - loss: 1.5853 - accuracy: 0.397 - ETA: 16s - loss: 1.5860 - accuracy: 0.397 - ETA: 16s - loss: 1.5899 - accuracy: 0.396 - ETA: 16s - loss: 1.5922 - accuracy: 0.395 - ETA: 15s - loss: 1.5911 - accuracy: 0.395 - ETA: 15s - loss: 1.5895 - accuracy: 0.397 - ETA: 14s - loss: 1.5869 - accuracy: 0.398 - ETA: 14s - loss: 1.5875 - accuracy: 0.399 - ETA: 13s - loss: 1.5911 - accuracy: 0.398 - ETA: 13s - loss: 1.5944 - accuracy: 0.397 - ETA: 12s - loss: 1.5947 - accuracy: 0.397 - ETA: 12s - loss: 1.5944 - accuracy: 0.397 - ETA: 12s - loss: 1.5940 - accuracy: 0.397 - ETA: 11s - loss: 1.5942 - accuracy: 0.397 - ETA: 11s - loss: 1.5944 - accuracy: 0.398 - ETA: 10s - loss: 1.5955 - accuracy: 0.397 - ETA: 10s - loss: 1.5945 - accuracy: 0.398 - ETA: 9s - loss: 1.5933 - accuracy: 0.398 - ETA: 9s - loss: 1.5933 - accuracy: 0.39 - ETA: 9s - loss: 1.5932 - accuracy: 0.39 - ETA: 8s - loss: 1.5919 - accuracy: 0.39 - ETA: 8s - loss: 1.5925 - accuracy: 0.39 - ETA: 7s - loss: 1.5942 - accuracy: 0.39 - ETA: 7s - loss: 1.5941 - accuracy: 0.39 - ETA: 6s - loss: 1.5940 - accuracy: 0.39 - ETA: 6s - loss: 1.5938 - accuracy: 0.39 - ETA: 6s - loss: 1.5919 - accuracy: 0.39 - ETA: 5s - loss: 1.5925 - accuracy: 0.39 - ETA: 5s - loss: 1.5920 - accuracy: 0.39 - ETA: 4s - loss: 1.5901 - accuracy: 0.39 - ETA: 4s - loss: 1.5900 - accuracy: 0.39 - ETA: 3s - loss: 1.5890 - accuracy: 0.39 - ETA: 3s - loss: 1.5877 - accuracy: 0.40 - ETA: 3s - loss: 1.5886 - accuracy: 0.40 - ETA: 2s - loss: 1.5892 - accuracy: 0.39 - ETA: 2s - loss: 1.5883 - accuracy: 0.40 - ETA: 1s - loss: 1.5872 - accuracy: 0.40 - ETA: 1s - loss: 1.5876 - accuracy: 0.40 - ETA: 1s - loss: 1.5870 - accuracy: 0.40 - ETA: 0s - loss: 1.5863 - accuracy: 0.40 - ETA: 0s - loss: 1.5855 - accuracy: 0.40 - 34s 961us/step - loss: 1.5848 - accuracy: 0.4017 - val_loss: 1.5255 - val_accuracy: 0.4134\n",
      "Epoch 36/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5613 - accuracy: 0.401 - ETA: 23s - loss: 1.5318 - accuracy: 0.421 - ETA: 23s - loss: 1.5271 - accuracy: 0.427 - ETA: 23s - loss: 1.5312 - accuracy: 0.424 - ETA: 23s - loss: 1.5424 - accuracy: 0.420 - ETA: 22s - loss: 1.5488 - accuracy: 0.416 - ETA: 22s - loss: 1.5691 - accuracy: 0.408 - ETA: 22s - loss: 1.5711 - accuracy: 0.407 - ETA: 22s - loss: 1.5750 - accuracy: 0.408 - ETA: 21s - loss: 1.5714 - accuracy: 0.410 - ETA: 21s - loss: 1.5679 - accuracy: 0.410 - ETA: 20s - loss: 1.5672 - accuracy: 0.411 - ETA: 20s - loss: 1.5637 - accuracy: 0.412 - ETA: 19s - loss: 1.5656 - accuracy: 0.412 - ETA: 19s - loss: 1.5741 - accuracy: 0.408 - ETA: 18s - loss: 1.5743 - accuracy: 0.409 - ETA: 18s - loss: 1.5717 - accuracy: 0.410 - ETA: 17s - loss: 1.5698 - accuracy: 0.410 - ETA: 17s - loss: 1.5667 - accuracy: 0.411 - ETA: 16s - loss: 1.5665 - accuracy: 0.411 - ETA: 16s - loss: 1.5641 - accuracy: 0.412 - ETA: 15s - loss: 1.5623 - accuracy: 0.413 - ETA: 15s - loss: 1.5629 - accuracy: 0.413 - ETA: 15s - loss: 1.5663 - accuracy: 0.412 - ETA: 14s - loss: 1.5667 - accuracy: 0.413 - ETA: 14s - loss: 1.5666 - accuracy: 0.413 - ETA: 13s - loss: 1.5662 - accuracy: 0.413 - ETA: 13s - loss: 1.5670 - accuracy: 0.412 - ETA: 12s - loss: 1.5667 - accuracy: 0.412 - ETA: 12s - loss: 1.5658 - accuracy: 0.412 - ETA: 11s - loss: 1.5643 - accuracy: 0.412 - ETA: 11s - loss: 1.5642 - accuracy: 0.411 - ETA: 11s - loss: 1.5662 - accuracy: 0.410 - ETA: 10s - loss: 1.5649 - accuracy: 0.410 - ETA: 10s - loss: 1.5646 - accuracy: 0.410 - ETA: 9s - loss: 1.5652 - accuracy: 0.409 - ETA: 9s - loss: 1.5658 - accuracy: 0.40 - ETA: 8s - loss: 1.5663 - accuracy: 0.40 - ETA: 8s - loss: 1.5655 - accuracy: 0.40 - ETA: 7s - loss: 1.5662 - accuracy: 0.40 - ETA: 7s - loss: 1.5643 - accuracy: 0.40 - ETA: 7s - loss: 1.5637 - accuracy: 0.40 - ETA: 6s - loss: 1.5636 - accuracy: 0.40 - ETA: 6s - loss: 1.5629 - accuracy: 0.40 - ETA: 5s - loss: 1.5630 - accuracy: 0.40 - ETA: 5s - loss: 1.5624 - accuracy: 0.40 - ETA: 4s - loss: 1.5624 - accuracy: 0.40 - ETA: 4s - loss: 1.5620 - accuracy: 0.40 - ETA: 4s - loss: 1.5616 - accuracy: 0.40 - ETA: 3s - loss: 1.5617 - accuracy: 0.40 - ETA: 3s - loss: 1.5619 - accuracy: 0.40 - ETA: 2s - loss: 1.5631 - accuracy: 0.40 - ETA: 2s - loss: 1.5644 - accuracy: 0.40 - ETA: 1s - loss: 1.5653 - accuracy: 0.40 - ETA: 1s - loss: 1.5649 - accuracy: 0.40 - ETA: 1s - loss: 1.5644 - accuracy: 0.40 - ETA: 0s - loss: 1.5641 - accuracy: 0.40 - ETA: 0s - loss: 1.5630 - accuracy: 0.40 - 34s 975us/step - loss: 1.5626 - accuracy: 0.4072 - val_loss: 1.5358 - val_accuracy: 0.4092\n",
      "Epoch 37/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5158 - accuracy: 0.415 - ETA: 24s - loss: 1.5390 - accuracy: 0.414 - ETA: 24s - loss: 1.5236 - accuracy: 0.421 - ETA: 23s - loss: 1.5298 - accuracy: 0.419 - ETA: 23s - loss: 1.5188 - accuracy: 0.419 - ETA: 23s - loss: 1.5148 - accuracy: 0.420 - ETA: 22s - loss: 1.5083 - accuracy: 0.424 - ETA: 22s - loss: 1.5020 - accuracy: 0.427 - ETA: 21s - loss: 1.5036 - accuracy: 0.427 - ETA: 21s - loss: 1.5137 - accuracy: 0.425 - ETA: 20s - loss: 1.5213 - accuracy: 0.422 - ETA: 20s - loss: 1.5241 - accuracy: 0.422 - ETA: 19s - loss: 1.5287 - accuracy: 0.417 - ETA: 19s - loss: 1.5340 - accuracy: 0.415 - ETA: 18s - loss: 1.5338 - accuracy: 0.415 - ETA: 18s - loss: 1.5367 - accuracy: 0.414 - ETA: 18s - loss: 1.5398 - accuracy: 0.413 - ETA: 17s - loss: 1.5408 - accuracy: 0.414 - ETA: 17s - loss: 1.5443 - accuracy: 0.412 - ETA: 16s - loss: 1.5458 - accuracy: 0.412 - ETA: 16s - loss: 1.5489 - accuracy: 0.411 - ETA: 15s - loss: 1.5535 - accuracy: 0.409 - ETA: 15s - loss: 1.5581 - accuracy: 0.407 - ETA: 14s - loss: 1.5580 - accuracy: 0.408 - ETA: 14s - loss: 1.5574 - accuracy: 0.407 - ETA: 14s - loss: 1.5580 - accuracy: 0.407 - ETA: 13s - loss: 1.5580 - accuracy: 0.408 - ETA: 13s - loss: 1.5564 - accuracy: 0.408 - ETA: 12s - loss: 1.5556 - accuracy: 0.409 - ETA: 12s - loss: 1.5554 - accuracy: 0.409 - ETA: 11s - loss: 1.5567 - accuracy: 0.408 - ETA: 11s - loss: 1.5554 - accuracy: 0.409 - ETA: 10s - loss: 1.5540 - accuracy: 0.410 - ETA: 10s - loss: 1.5539 - accuracy: 0.410 - ETA: 10s - loss: 1.5541 - accuracy: 0.409 - ETA: 9s - loss: 1.5538 - accuracy: 0.409 - ETA: 9s - loss: 1.5555 - accuracy: 0.40 - ETA: 8s - loss: 1.5561 - accuracy: 0.40 - ETA: 8s - loss: 1.5573 - accuracy: 0.40 - ETA: 7s - loss: 1.5578 - accuracy: 0.40 - ETA: 7s - loss: 1.5576 - accuracy: 0.40 - ETA: 7s - loss: 1.5578 - accuracy: 0.40 - ETA: 6s - loss: 1.5596 - accuracy: 0.40 - ETA: 6s - loss: 1.5602 - accuracy: 0.40 - ETA: 5s - loss: 1.5604 - accuracy: 0.40 - ETA: 5s - loss: 1.5594 - accuracy: 0.40 - ETA: 4s - loss: 1.5602 - accuracy: 0.40 - ETA: 4s - loss: 1.5598 - accuracy: 0.40 - ETA: 4s - loss: 1.5594 - accuracy: 0.40 - ETA: 3s - loss: 1.5580 - accuracy: 0.40 - ETA: 3s - loss: 1.5589 - accuracy: 0.40 - ETA: 2s - loss: 1.5603 - accuracy: 0.40 - ETA: 2s - loss: 1.5604 - accuracy: 0.40 - ETA: 1s - loss: 1.5597 - accuracy: 0.40 - ETA: 1s - loss: 1.5590 - accuracy: 0.40 - ETA: 1s - loss: 1.5585 - accuracy: 0.40 - ETA: 0s - loss: 1.5571 - accuracy: 0.40 - ETA: 0s - loss: 1.5569 - accuracy: 0.40 - 34s 968us/step - loss: 1.5567 - accuracy: 0.4090 - val_loss: 1.5208 - val_accuracy: 0.4142\n",
      "Epoch 38/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5357 - accuracy: 0.401 - ETA: 24s - loss: 1.5163 - accuracy: 0.401 - ETA: 24s - loss: 1.5125 - accuracy: 0.420 - ETA: 24s - loss: 1.5305 - accuracy: 0.413 - ETA: 23s - loss: 1.5413 - accuracy: 0.408 - ETA: 23s - loss: 1.5447 - accuracy: 0.409 - ETA: 23s - loss: 1.5491 - accuracy: 0.409 - ETA: 22s - loss: 1.5517 - accuracy: 0.405 - ETA: 22s - loss: 1.5580 - accuracy: 0.405 - ETA: 21s - loss: 1.5547 - accuracy: 0.407 - ETA: 21s - loss: 1.5555 - accuracy: 0.405 - ETA: 20s - loss: 1.5584 - accuracy: 0.402 - ETA: 20s - loss: 1.5599 - accuracy: 0.404 - ETA: 19s - loss: 1.5624 - accuracy: 0.404 - ETA: 19s - loss: 1.5601 - accuracy: 0.406 - ETA: 18s - loss: 1.5615 - accuracy: 0.407 - ETA: 18s - loss: 1.5593 - accuracy: 0.407 - ETA: 17s - loss: 1.5582 - accuracy: 0.408 - ETA: 17s - loss: 1.5548 - accuracy: 0.409 - ETA: 16s - loss: 1.5551 - accuracy: 0.409 - ETA: 16s - loss: 1.5561 - accuracy: 0.408 - ETA: 15s - loss: 1.5601 - accuracy: 0.407 - ETA: 15s - loss: 1.5640 - accuracy: 0.406 - ETA: 15s - loss: 1.5662 - accuracy: 0.406 - ETA: 14s - loss: 1.5675 - accuracy: 0.405 - ETA: 14s - loss: 1.5671 - accuracy: 0.405 - ETA: 13s - loss: 1.5654 - accuracy: 0.406 - ETA: 13s - loss: 1.5646 - accuracy: 0.406 - ETA: 12s - loss: 1.5648 - accuracy: 0.406 - ETA: 12s - loss: 1.5642 - accuracy: 0.406 - ETA: 11s - loss: 1.5638 - accuracy: 0.406 - ETA: 11s - loss: 1.5633 - accuracy: 0.407 - ETA: 11s - loss: 1.5637 - accuracy: 0.407 - ETA: 10s - loss: 1.5651 - accuracy: 0.407 - ETA: 10s - loss: 1.5686 - accuracy: 0.405 - ETA: 9s - loss: 1.5701 - accuracy: 0.405 - ETA: 9s - loss: 1.5701 - accuracy: 0.40 - ETA: 8s - loss: 1.5701 - accuracy: 0.40 - ETA: 8s - loss: 1.5701 - accuracy: 0.40 - ETA: 8s - loss: 1.5700 - accuracy: 0.40 - ETA: 7s - loss: 1.5691 - accuracy: 0.40 - ETA: 7s - loss: 1.5675 - accuracy: 0.40 - ETA: 6s - loss: 1.5672 - accuracy: 0.40 - ETA: 6s - loss: 1.5675 - accuracy: 0.40 - ETA: 5s - loss: 1.5674 - accuracy: 0.40 - ETA: 5s - loss: 1.5684 - accuracy: 0.40 - ETA: 4s - loss: 1.5712 - accuracy: 0.40 - ETA: 4s - loss: 1.5710 - accuracy: 0.40 - ETA: 4s - loss: 1.5703 - accuracy: 0.40 - ETA: 3s - loss: 1.5688 - accuracy: 0.40 - ETA: 3s - loss: 1.5681 - accuracy: 0.40 - ETA: 2s - loss: 1.5681 - accuracy: 0.40 - ETA: 2s - loss: 1.5668 - accuracy: 0.40 - ETA: 1s - loss: 1.5657 - accuracy: 0.40 - ETA: 1s - loss: 1.5661 - accuracy: 0.40 - ETA: 1s - loss: 1.5663 - accuracy: 0.40 - ETA: 0s - loss: 1.5663 - accuracy: 0.40 - ETA: 0s - loss: 1.5658 - accuracy: 0.40 - 34s 982us/step - loss: 1.5660 - accuracy: 0.4052 - val_loss: 1.5172 - val_accuracy: 0.4139\n",
      "Epoch 39/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 23s - loss: 1.4856 - accuracy: 0.440 - ETA: 23s - loss: 1.5238 - accuracy: 0.429 - ETA: 23s - loss: 1.5329 - accuracy: 0.418 - ETA: 23s - loss: 1.5276 - accuracy: 0.412 - ETA: 23s - loss: 1.5163 - accuracy: 0.417 - ETA: 22s - loss: 1.5205 - accuracy: 0.416 - ETA: 22s - loss: 1.5201 - accuracy: 0.416 - ETA: 22s - loss: 1.5185 - accuracy: 0.414 - ETA: 21s - loss: 1.5245 - accuracy: 0.413 - ETA: 21s - loss: 1.5397 - accuracy: 0.409 - ETA: 20s - loss: 1.5475 - accuracy: 0.407 - ETA: 20s - loss: 1.5506 - accuracy: 0.407 - ETA: 19s - loss: 1.5476 - accuracy: 0.408 - ETA: 19s - loss: 1.5451 - accuracy: 0.409 - ETA: 18s - loss: 1.5435 - accuracy: 0.410 - ETA: 18s - loss: 1.5420 - accuracy: 0.410 - ETA: 18s - loss: 1.5376 - accuracy: 0.413 - ETA: 17s - loss: 1.5399 - accuracy: 0.412 - ETA: 17s - loss: 1.5410 - accuracy: 0.410 - ETA: 16s - loss: 1.5472 - accuracy: 0.409 - ETA: 16s - loss: 1.5552 - accuracy: 0.407 - ETA: 15s - loss: 1.5568 - accuracy: 0.408 - ETA: 15s - loss: 1.5596 - accuracy: 0.407 - ETA: 15s - loss: 1.5601 - accuracy: 0.407 - ETA: 14s - loss: 1.5586 - accuracy: 0.408 - ETA: 14s - loss: 1.5566 - accuracy: 0.409 - ETA: 13s - loss: 1.5582 - accuracy: 0.409 - ETA: 13s - loss: 1.5588 - accuracy: 0.408 - ETA: 12s - loss: 1.5586 - accuracy: 0.409 - ETA: 12s - loss: 1.5586 - accuracy: 0.409 - ETA: 12s - loss: 1.5581 - accuracy: 0.409 - ETA: 11s - loss: 1.5573 - accuracy: 0.409 - ETA: 11s - loss: 1.5559 - accuracy: 0.410 - ETA: 10s - loss: 1.5554 - accuracy: 0.410 - ETA: 10s - loss: 1.5542 - accuracy: 0.409 - ETA: 9s - loss: 1.5528 - accuracy: 0.409 - ETA: 9s - loss: 1.5503 - accuracy: 0.41 - ETA: 9s - loss: 1.5503 - accuracy: 0.41 - ETA: 8s - loss: 1.5505 - accuracy: 0.41 - ETA: 8s - loss: 1.5506 - accuracy: 0.41 - ETA: 7s - loss: 1.5528 - accuracy: 0.41 - ETA: 7s - loss: 1.5533 - accuracy: 0.41 - ETA: 6s - loss: 1.5534 - accuracy: 0.41 - ETA: 6s - loss: 1.5535 - accuracy: 0.41 - ETA: 5s - loss: 1.5523 - accuracy: 0.41 - ETA: 5s - loss: 1.5519 - accuracy: 0.41 - ETA: 5s - loss: 1.5511 - accuracy: 0.41 - ETA: 4s - loss: 1.5500 - accuracy: 0.41 - ETA: 4s - loss: 1.5490 - accuracy: 0.41 - ETA: 3s - loss: 1.5492 - accuracy: 0.41 - ETA: 3s - loss: 1.5492 - accuracy: 0.41 - ETA: 2s - loss: 1.5499 - accuracy: 0.41 - ETA: 2s - loss: 1.5493 - accuracy: 0.41 - ETA: 1s - loss: 1.5495 - accuracy: 0.41 - ETA: 1s - loss: 1.5487 - accuracy: 0.41 - ETA: 1s - loss: 1.5477 - accuracy: 0.41 - ETA: 0s - loss: 1.5470 - accuracy: 0.41 - ETA: 0s - loss: 1.5469 - accuracy: 0.41 - 35s 1ms/step - loss: 1.5475 - accuracy: 0.4119 - val_loss: 1.5476 - val_accuracy: 0.4045\n",
      "Epoch 40/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5365 - accuracy: 0.440 - ETA: 23s - loss: 1.5484 - accuracy: 0.421 - ETA: 23s - loss: 1.5685 - accuracy: 0.413 - ETA: 22s - loss: 1.5813 - accuracy: 0.400 - ETA: 22s - loss: 1.5765 - accuracy: 0.399 - ETA: 22s - loss: 1.5744 - accuracy: 0.404 - ETA: 21s - loss: 1.5751 - accuracy: 0.408 - ETA: 21s - loss: 1.5773 - accuracy: 0.410 - ETA: 20s - loss: 1.5820 - accuracy: 0.409 - ETA: 20s - loss: 1.5792 - accuracy: 0.409 - ETA: 19s - loss: 1.5706 - accuracy: 0.411 - ETA: 19s - loss: 1.5653 - accuracy: 0.411 - ETA: 19s - loss: 1.5636 - accuracy: 0.411 - ETA: 18s - loss: 1.5566 - accuracy: 0.413 - ETA: 18s - loss: 1.5549 - accuracy: 0.413 - ETA: 17s - loss: 1.5541 - accuracy: 0.414 - ETA: 17s - loss: 1.5520 - accuracy: 0.413 - ETA: 17s - loss: 1.5604 - accuracy: 0.411 - ETA: 16s - loss: 1.5675 - accuracy: 0.407 - ETA: 16s - loss: 1.5700 - accuracy: 0.405 - ETA: 15s - loss: 1.5678 - accuracy: 0.407 - ETA: 15s - loss: 1.5676 - accuracy: 0.408 - ETA: 15s - loss: 1.5674 - accuracy: 0.408 - ETA: 14s - loss: 1.5660 - accuracy: 0.408 - ETA: 14s - loss: 1.5635 - accuracy: 0.409 - ETA: 13s - loss: 1.5642 - accuracy: 0.409 - ETA: 13s - loss: 1.5643 - accuracy: 0.409 - ETA: 13s - loss: 1.5659 - accuracy: 0.408 - ETA: 12s - loss: 1.5668 - accuracy: 0.407 - ETA: 12s - loss: 1.5662 - accuracy: 0.407 - ETA: 11s - loss: 1.5672 - accuracy: 0.407 - ETA: 11s - loss: 1.5675 - accuracy: 0.408 - ETA: 10s - loss: 1.5676 - accuracy: 0.408 - ETA: 10s - loss: 1.5679 - accuracy: 0.408 - ETA: 10s - loss: 1.5682 - accuracy: 0.408 - ETA: 9s - loss: 1.5680 - accuracy: 0.408 - ETA: 9s - loss: 1.5672 - accuracy: 0.40 - ETA: 8s - loss: 1.5654 - accuracy: 0.40 - ETA: 8s - loss: 1.5654 - accuracy: 0.40 - ETA: 7s - loss: 1.5668 - accuracy: 0.40 - ETA: 7s - loss: 1.5653 - accuracy: 0.40 - ETA: 7s - loss: 1.5645 - accuracy: 0.40 - ETA: 6s - loss: 1.5647 - accuracy: 0.40 - ETA: 6s - loss: 1.5644 - accuracy: 0.40 - ETA: 5s - loss: 1.5643 - accuracy: 0.40 - ETA: 5s - loss: 1.5654 - accuracy: 0.40 - ETA: 4s - loss: 1.5658 - accuracy: 0.40 - ETA: 4s - loss: 1.5654 - accuracy: 0.40 - ETA: 4s - loss: 1.5648 - accuracy: 0.40 - ETA: 3s - loss: 1.5633 - accuracy: 0.41 - ETA: 3s - loss: 1.5632 - accuracy: 0.41 - ETA: 2s - loss: 1.5626 - accuracy: 0.41 - ETA: 2s - loss: 1.5623 - accuracy: 0.41 - ETA: 1s - loss: 1.5621 - accuracy: 0.41 - ETA: 1s - loss: 1.5627 - accuracy: 0.41 - ETA: 1s - loss: 1.5632 - accuracy: 0.41 - ETA: 0s - loss: 1.5624 - accuracy: 0.41 - ETA: 0s - loss: 1.5616 - accuracy: 0.41 - 34s 964us/step - loss: 1.5612 - accuracy: 0.4113 - val_loss: 1.5796 - val_accuracy: 0.4015\n",
      "Epoch 41/45\n",
      "35000/35000 [==============================] - ETA: 23s - loss: 1.5649 - accuracy: 0.410 - ETA: 23s - loss: 1.6240 - accuracy: 0.395 - ETA: 23s - loss: 1.6355 - accuracy: 0.392 - ETA: 22s - loss: 1.6233 - accuracy: 0.394 - ETA: 22s - loss: 1.6075 - accuracy: 0.398 - ETA: 21s - loss: 1.6125 - accuracy: 0.395 - ETA: 21s - loss: 1.6209 - accuracy: 0.392 - ETA: 21s - loss: 1.6202 - accuracy: 0.395 - ETA: 20s - loss: 1.6077 - accuracy: 0.397 - ETA: 20s - loss: 1.6045 - accuracy: 0.397 - ETA: 19s - loss: 1.5979 - accuracy: 0.399 - ETA: 19s - loss: 1.5950 - accuracy: 0.397 - ETA: 18s - loss: 1.5891 - accuracy: 0.400 - ETA: 18s - loss: 1.5877 - accuracy: 0.400 - ETA: 18s - loss: 1.5830 - accuracy: 0.402 - ETA: 17s - loss: 1.5810 - accuracy: 0.403 - ETA: 17s - loss: 1.5820 - accuracy: 0.403 - ETA: 16s - loss: 1.5866 - accuracy: 0.400 - ETA: 16s - loss: 1.5890 - accuracy: 0.399 - ETA: 16s - loss: 1.5859 - accuracy: 0.400 - ETA: 15s - loss: 1.5828 - accuracy: 0.400 - ETA: 15s - loss: 1.5771 - accuracy: 0.401 - ETA: 15s - loss: 1.5737 - accuracy: 0.402 - ETA: 14s - loss: 1.5685 - accuracy: 0.404 - ETA: 14s - loss: 1.5655 - accuracy: 0.406 - ETA: 13s - loss: 1.5625 - accuracy: 0.407 - ETA: 13s - loss: 1.5616 - accuracy: 0.408 - ETA: 12s - loss: 1.5607 - accuracy: 0.407 - ETA: 12s - loss: 1.5606 - accuracy: 0.407 - ETA: 12s - loss: 1.5607 - accuracy: 0.408 - ETA: 11s - loss: 1.5633 - accuracy: 0.406 - ETA: 11s - loss: 1.5611 - accuracy: 0.407 - ETA: 10s - loss: 1.5600 - accuracy: 0.408 - ETA: 10s - loss: 1.5565 - accuracy: 0.409 - ETA: 9s - loss: 1.5550 - accuracy: 0.410 - ETA: 9s - loss: 1.5539 - accuracy: 0.41 - ETA: 9s - loss: 1.5540 - accuracy: 0.41 - ETA: 8s - loss: 1.5545 - accuracy: 0.41 - ETA: 8s - loss: 1.5557 - accuracy: 0.40 - ETA: 7s - loss: 1.5549 - accuracy: 0.40 - ETA: 7s - loss: 1.5537 - accuracy: 0.41 - ETA: 6s - loss: 1.5517 - accuracy: 0.41 - ETA: 6s - loss: 1.5504 - accuracy: 0.41 - ETA: 6s - loss: 1.5494 - accuracy: 0.41 - ETA: 5s - loss: 1.5482 - accuracy: 0.41 - ETA: 5s - loss: 1.5473 - accuracy: 0.41 - ETA: 4s - loss: 1.5461 - accuracy: 0.41 - ETA: 4s - loss: 1.5451 - accuracy: 0.41 - ETA: 3s - loss: 1.5447 - accuracy: 0.41 - ETA: 3s - loss: 1.5422 - accuracy: 0.41 - ETA: 3s - loss: 1.5421 - accuracy: 0.41 - ETA: 2s - loss: 1.5419 - accuracy: 0.41 - ETA: 2s - loss: 1.5434 - accuracy: 0.41 - ETA: 1s - loss: 1.5451 - accuracy: 0.41 - ETA: 1s - loss: 1.5458 - accuracy: 0.41 - ETA: 0s - loss: 1.5449 - accuracy: 0.41 - ETA: 0s - loss: 1.5459 - accuracy: 0.41 - ETA: 0s - loss: 1.5452 - accuracy: 0.41 - 33s 956us/step - loss: 1.5451 - accuracy: 0.4128 - val_loss: 1.5984 - val_accuracy: 0.3911\n",
      "Epoch 42/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 22s - loss: 1.6881 - accuracy: 0.366 - ETA: 22s - loss: 1.6435 - accuracy: 0.391 - ETA: 22s - loss: 1.6057 - accuracy: 0.394 - ETA: 22s - loss: 1.5908 - accuracy: 0.397 - ETA: 22s - loss: 1.5754 - accuracy: 0.399 - ETA: 21s - loss: 1.5605 - accuracy: 0.405 - ETA: 21s - loss: 1.5511 - accuracy: 0.407 - ETA: 20s - loss: 1.5413 - accuracy: 0.412 - ETA: 20s - loss: 1.5437 - accuracy: 0.412 - ETA: 20s - loss: 1.5383 - accuracy: 0.416 - ETA: 19s - loss: 1.5344 - accuracy: 0.416 - ETA: 19s - loss: 1.5388 - accuracy: 0.416 - ETA: 18s - loss: 1.5498 - accuracy: 0.410 - ETA: 18s - loss: 1.5549 - accuracy: 0.410 - ETA: 18s - loss: 1.5511 - accuracy: 0.411 - ETA: 17s - loss: 1.5438 - accuracy: 0.414 - ETA: 17s - loss: 1.5417 - accuracy: 0.413 - ETA: 17s - loss: 1.5400 - accuracy: 0.414 - ETA: 16s - loss: 1.5395 - accuracy: 0.416 - ETA: 16s - loss: 1.5376 - accuracy: 0.416 - ETA: 15s - loss: 1.5357 - accuracy: 0.417 - ETA: 15s - loss: 1.5386 - accuracy: 0.415 - ETA: 15s - loss: 1.5372 - accuracy: 0.416 - ETA: 14s - loss: 1.5378 - accuracy: 0.415 - ETA: 14s - loss: 1.5387 - accuracy: 0.415 - ETA: 13s - loss: 1.5415 - accuracy: 0.415 - ETA: 13s - loss: 1.5421 - accuracy: 0.414 - ETA: 12s - loss: 1.5448 - accuracy: 0.412 - ETA: 12s - loss: 1.5445 - accuracy: 0.413 - ETA: 12s - loss: 1.5440 - accuracy: 0.413 - ETA: 11s - loss: 1.5450 - accuracy: 0.413 - ETA: 11s - loss: 1.5453 - accuracy: 0.413 - ETA: 10s - loss: 1.5427 - accuracy: 0.414 - ETA: 10s - loss: 1.5429 - accuracy: 0.415 - ETA: 9s - loss: 1.5420 - accuracy: 0.415 - ETA: 9s - loss: 1.5406 - accuracy: 0.41 - ETA: 9s - loss: 1.5384 - accuracy: 0.41 - ETA: 8s - loss: 1.5366 - accuracy: 0.41 - ETA: 8s - loss: 1.5359 - accuracy: 0.41 - ETA: 7s - loss: 1.5357 - accuracy: 0.41 - ETA: 7s - loss: 1.5403 - accuracy: 0.41 - ETA: 6s - loss: 1.5418 - accuracy: 0.41 - ETA: 6s - loss: 1.5412 - accuracy: 0.41 - ETA: 6s - loss: 1.5407 - accuracy: 0.41 - ETA: 5s - loss: 1.5402 - accuracy: 0.41 - ETA: 5s - loss: 1.5385 - accuracy: 0.41 - ETA: 4s - loss: 1.5366 - accuracy: 0.41 - ETA: 4s - loss: 1.5362 - accuracy: 0.41 - ETA: 3s - loss: 1.5377 - accuracy: 0.41 - ETA: 3s - loss: 1.5387 - accuracy: 0.41 - ETA: 3s - loss: 1.5397 - accuracy: 0.41 - ETA: 2s - loss: 1.5398 - accuracy: 0.41 - ETA: 2s - loss: 1.5401 - accuracy: 0.41 - ETA: 1s - loss: 1.5386 - accuracy: 0.41 - ETA: 1s - loss: 1.5389 - accuracy: 0.41 - ETA: 0s - loss: 1.5373 - accuracy: 0.41 - ETA: 0s - loss: 1.5372 - accuracy: 0.41 - ETA: 0s - loss: 1.5372 - accuracy: 0.41 - 33s 955us/step - loss: 1.5373 - accuracy: 0.4168 - val_loss: 1.8501 - val_accuracy: 0.3341\n",
      "Epoch 43/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.8975 - accuracy: 0.340 - ETA: 23s - loss: 1.7587 - accuracy: 0.360 - ETA: 23s - loss: 1.6921 - accuracy: 0.380 - ETA: 23s - loss: 1.6461 - accuracy: 0.389 - ETA: 22s - loss: 1.6297 - accuracy: 0.395 - ETA: 22s - loss: 1.6102 - accuracy: 0.402 - ETA: 21s - loss: 1.5982 - accuracy: 0.403 - ETA: 21s - loss: 1.5874 - accuracy: 0.405 - ETA: 21s - loss: 1.5871 - accuracy: 0.406 - ETA: 20s - loss: 1.5814 - accuracy: 0.409 - ETA: 20s - loss: 1.5788 - accuracy: 0.408 - ETA: 20s - loss: 1.5714 - accuracy: 0.410 - ETA: 19s - loss: 1.5667 - accuracy: 0.411 - ETA: 19s - loss: 1.5644 - accuracy: 0.408 - ETA: 18s - loss: 1.5628 - accuracy: 0.408 - ETA: 18s - loss: 1.5625 - accuracy: 0.407 - ETA: 17s - loss: 1.5642 - accuracy: 0.407 - ETA: 17s - loss: 1.5605 - accuracy: 0.409 - ETA: 17s - loss: 1.5603 - accuracy: 0.408 - ETA: 16s - loss: 1.5573 - accuracy: 0.409 - ETA: 16s - loss: 1.5544 - accuracy: 0.410 - ETA: 15s - loss: 1.5505 - accuracy: 0.410 - ETA: 15s - loss: 1.5473 - accuracy: 0.410 - ETA: 14s - loss: 1.5497 - accuracy: 0.409 - ETA: 14s - loss: 1.5521 - accuracy: 0.409 - ETA: 13s - loss: 1.5555 - accuracy: 0.407 - ETA: 13s - loss: 1.5559 - accuracy: 0.407 - ETA: 12s - loss: 1.5551 - accuracy: 0.407 - ETA: 12s - loss: 1.5532 - accuracy: 0.407 - ETA: 12s - loss: 1.5516 - accuracy: 0.408 - ETA: 11s - loss: 1.5508 - accuracy: 0.408 - ETA: 11s - loss: 1.5492 - accuracy: 0.409 - ETA: 10s - loss: 1.5518 - accuracy: 0.408 - ETA: 10s - loss: 1.5531 - accuracy: 0.408 - ETA: 9s - loss: 1.5519 - accuracy: 0.409 - ETA: 9s - loss: 1.5510 - accuracy: 0.40 - ETA: 9s - loss: 1.5504 - accuracy: 0.40 - ETA: 8s - loss: 1.5484 - accuracy: 0.40 - ETA: 8s - loss: 1.5475 - accuracy: 0.41 - ETA: 7s - loss: 1.5463 - accuracy: 0.41 - ETA: 7s - loss: 1.5456 - accuracy: 0.41 - ETA: 6s - loss: 1.5456 - accuracy: 0.41 - ETA: 6s - loss: 1.5431 - accuracy: 0.41 - ETA: 6s - loss: 1.5433 - accuracy: 0.41 - ETA: 5s - loss: 1.5428 - accuracy: 0.41 - ETA: 5s - loss: 1.5433 - accuracy: 0.41 - ETA: 4s - loss: 1.5437 - accuracy: 0.41 - ETA: 4s - loss: 1.5439 - accuracy: 0.41 - ETA: 4s - loss: 1.5442 - accuracy: 0.41 - ETA: 3s - loss: 1.5441 - accuracy: 0.41 - ETA: 3s - loss: 1.5446 - accuracy: 0.41 - ETA: 2s - loss: 1.5432 - accuracy: 0.41 - ETA: 2s - loss: 1.5428 - accuracy: 0.41 - ETA: 1s - loss: 1.5422 - accuracy: 0.41 - ETA: 1s - loss: 1.5417 - accuracy: 0.41 - ETA: 1s - loss: 1.5415 - accuracy: 0.41 - ETA: 0s - loss: 1.5427 - accuracy: 0.41 - ETA: 0s - loss: 1.5428 - accuracy: 0.41 - 34s 975us/step - loss: 1.5432 - accuracy: 0.4123 - val_loss: 1.5060 - val_accuracy: 0.4179\n",
      "Epoch 44/45\n",
      "35000/35000 [==============================] - ETA: 22s - loss: 1.5085 - accuracy: 0.421 - ETA: 23s - loss: 1.5193 - accuracy: 0.423 - ETA: 23s - loss: 1.5344 - accuracy: 0.417 - ETA: 23s - loss: 1.5293 - accuracy: 0.419 - ETA: 23s - loss: 1.5410 - accuracy: 0.411 - ETA: 22s - loss: 1.5352 - accuracy: 0.412 - ETA: 22s - loss: 1.5271 - accuracy: 0.417 - ETA: 22s - loss: 1.5194 - accuracy: 0.420 - ETA: 21s - loss: 1.5174 - accuracy: 0.418 - ETA: 21s - loss: 1.5145 - accuracy: 0.417 - ETA: 20s - loss: 1.5116 - accuracy: 0.420 - ETA: 20s - loss: 1.5142 - accuracy: 0.420 - ETA: 19s - loss: 1.5218 - accuracy: 0.419 - ETA: 19s - loss: 1.5290 - accuracy: 0.415 - ETA: 19s - loss: 1.5273 - accuracy: 0.415 - ETA: 18s - loss: 1.5250 - accuracy: 0.417 - ETA: 18s - loss: 1.5272 - accuracy: 0.417 - ETA: 17s - loss: 1.5239 - accuracy: 0.418 - ETA: 17s - loss: 1.5238 - accuracy: 0.418 - ETA: 17s - loss: 1.5242 - accuracy: 0.418 - ETA: 16s - loss: 1.5239 - accuracy: 0.417 - ETA: 16s - loss: 1.5217 - accuracy: 0.418 - ETA: 15s - loss: 1.5215 - accuracy: 0.418 - ETA: 15s - loss: 1.5228 - accuracy: 0.419 - ETA: 14s - loss: 1.5242 - accuracy: 0.419 - ETA: 14s - loss: 1.5295 - accuracy: 0.416 - ETA: 13s - loss: 1.5303 - accuracy: 0.416 - ETA: 13s - loss: 1.5293 - accuracy: 0.417 - ETA: 12s - loss: 1.5282 - accuracy: 0.417 - ETA: 12s - loss: 1.5271 - accuracy: 0.417 - ETA: 12s - loss: 1.5271 - accuracy: 0.417 - ETA: 11s - loss: 1.5262 - accuracy: 0.417 - ETA: 11s - loss: 1.5264 - accuracy: 0.417 - ETA: 10s - loss: 1.5255 - accuracy: 0.417 - ETA: 10s - loss: 1.5287 - accuracy: 0.416 - ETA: 9s - loss: 1.5309 - accuracy: 0.416 - ETA: 9s - loss: 1.5319 - accuracy: 0.41 - ETA: 8s - loss: 1.5316 - accuracy: 0.41 - ETA: 8s - loss: 1.5309 - accuracy: 0.41 - ETA: 8s - loss: 1.5311 - accuracy: 0.41 - ETA: 7s - loss: 1.5335 - accuracy: 0.41 - ETA: 7s - loss: 1.5370 - accuracy: 0.41 - ETA: 6s - loss: 1.5394 - accuracy: 0.41 - ETA: 6s - loss: 1.5390 - accuracy: 0.41 - ETA: 5s - loss: 1.5385 - accuracy: 0.41 - ETA: 5s - loss: 1.5375 - accuracy: 0.41 - ETA: 5s - loss: 1.5373 - accuracy: 0.41 - ETA: 4s - loss: 1.5375 - accuracy: 0.41 - ETA: 4s - loss: 1.5382 - accuracy: 0.41 - ETA: 3s - loss: 1.5389 - accuracy: 0.41 - ETA: 3s - loss: 1.5403 - accuracy: 0.41 - ETA: 2s - loss: 1.5403 - accuracy: 0.41 - ETA: 2s - loss: 1.5406 - accuracy: 0.41 - ETA: 1s - loss: 1.5405 - accuracy: 0.41 - ETA: 1s - loss: 1.5391 - accuracy: 0.41 - ETA: 1s - loss: 1.5391 - accuracy: 0.41 - ETA: 0s - loss: 1.5394 - accuracy: 0.41 - ETA: 0s - loss: 1.5422 - accuracy: 0.41 - 35s 994us/step - loss: 1.5427 - accuracy: 0.4125 - val_loss: 1.6077 - val_accuracy: 0.3860\n",
      "Epoch 45/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 26s - loss: 1.5821 - accuracy: 0.401 - ETA: 25s - loss: 1.5750 - accuracy: 0.404 - ETA: 24s - loss: 1.5424 - accuracy: 0.412 - ETA: 23s - loss: 1.5360 - accuracy: 0.407 - ETA: 23s - loss: 1.5356 - accuracy: 0.408 - ETA: 22s - loss: 1.5271 - accuracy: 0.409 - ETA: 22s - loss: 1.5255 - accuracy: 0.414 - ETA: 21s - loss: 1.5335 - accuracy: 0.412 - ETA: 21s - loss: 1.5420 - accuracy: 0.412 - ETA: 21s - loss: 1.5500 - accuracy: 0.409 - ETA: 20s - loss: 1.5496 - accuracy: 0.409 - ETA: 20s - loss: 1.5460 - accuracy: 0.410 - ETA: 19s - loss: 1.5381 - accuracy: 0.412 - ETA: 19s - loss: 1.5391 - accuracy: 0.412 - ETA: 18s - loss: 1.5327 - accuracy: 0.414 - ETA: 18s - loss: 1.5276 - accuracy: 0.415 - ETA: 17s - loss: 1.5269 - accuracy: 0.417 - ETA: 17s - loss: 1.5234 - accuracy: 0.417 - ETA: 16s - loss: 1.5203 - accuracy: 0.418 - ETA: 16s - loss: 1.5200 - accuracy: 0.419 - ETA: 16s - loss: 1.5226 - accuracy: 0.418 - ETA: 15s - loss: 1.5279 - accuracy: 0.416 - ETA: 15s - loss: 1.5309 - accuracy: 0.416 - ETA: 14s - loss: 1.5313 - accuracy: 0.416 - ETA: 14s - loss: 1.5300 - accuracy: 0.417 - ETA: 13s - loss: 1.5257 - accuracy: 0.419 - ETA: 13s - loss: 1.5255 - accuracy: 0.419 - ETA: 12s - loss: 1.5248 - accuracy: 0.419 - ETA: 12s - loss: 1.5243 - accuracy: 0.419 - ETA: 12s - loss: 1.5232 - accuracy: 0.420 - ETA: 11s - loss: 1.5229 - accuracy: 0.420 - ETA: 11s - loss: 1.5221 - accuracy: 0.421 - ETA: 10s - loss: 1.5213 - accuracy: 0.421 - ETA: 10s - loss: 1.5193 - accuracy: 0.422 - ETA: 9s - loss: 1.5186 - accuracy: 0.423 - ETA: 9s - loss: 1.5180 - accuracy: 0.42 - ETA: 9s - loss: 1.5171 - accuracy: 0.42 - ETA: 8s - loss: 1.5165 - accuracy: 0.42 - ETA: 8s - loss: 1.5171 - accuracy: 0.42 - ETA: 7s - loss: 1.5160 - accuracy: 0.42 - ETA: 7s - loss: 1.5165 - accuracy: 0.42 - ETA: 6s - loss: 1.5176 - accuracy: 0.42 - ETA: 6s - loss: 1.5184 - accuracy: 0.42 - ETA: 6s - loss: 1.5192 - accuracy: 0.42 - ETA: 5s - loss: 1.5198 - accuracy: 0.42 - ETA: 5s - loss: 1.5192 - accuracy: 0.42 - ETA: 4s - loss: 1.5178 - accuracy: 0.42 - ETA: 4s - loss: 1.5171 - accuracy: 0.42 - ETA: 3s - loss: 1.5157 - accuracy: 0.42 - ETA: 3s - loss: 1.5151 - accuracy: 0.42 - ETA: 3s - loss: 1.5157 - accuracy: 0.42 - ETA: 2s - loss: 1.5159 - accuracy: 0.42 - ETA: 2s - loss: 1.5154 - accuracy: 0.42 - ETA: 1s - loss: 1.5156 - accuracy: 0.42 - ETA: 1s - loss: 1.5161 - accuracy: 0.42 - ETA: 0s - loss: 1.5172 - accuracy: 0.42 - ETA: 0s - loss: 1.5175 - accuracy: 0.42 - ETA: 0s - loss: 1.5183 - accuracy: 0.42 - 34s 958us/step - loss: 1.5191 - accuracy: 0.4229 - val_loss: 1.5271 - val_accuracy: 0.4125\n",
      "Test-Accuracy: 0.364080000254843\n",
      "Wall time: 25min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_rat=set_model(X_train_n_sparse,y_rat_train_n,X_test_n_sparse,y_rat_test_n,y_rat_train_n.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3iUVdr48e+ZSe89EJIQ0iiBhBJ6t4GiYAFFLFhRV99Fdy3rvu66ur9dd/W166rYu6KioGtB6UgNEEpCSQiBhIQUSO8zc35/JCCQhAyQZDKT+3NduZLnmTMzd04y95w5zylKa40QQgj7Z7B1AEIIIdqHJHQhhHAQktCFEMJBSEIXQggHIQldCCEchJOtnjgoKEhHRUXZ6umFEMIubdmypVhrHdzSbTZL6FFRUaSkpNjq6YUQwi4ppQ62dpt0uQghhIOQhC6EEA5CEroQQpyDJ79N57mf99k6jFPYrA9dCCFaUlpdz7ZDpWg0RoMBZ4PCaFA4GRVOBgP+Hi5EBnrYNEatNV+k5FDTYGbm0HCbx3OcJHQhRIdIyytj3gdb6OXvztiYIMbFBZIY7oezsXnHQF5pDUvTjrA0vYCNB45htpx5jamrhvTi0Uv7EeLj1lHhn1HOsRoq6kwAvLoik3/PTLT6vv/8fjeT+4YwOiaw3eOShC6EaHd5pTXc9t5mAGrqzbywbB/P/wKeLkZGRQcyJjaIgWE+bDpwjKXpBew8XAZAbIgXd02IZkJ8MG7ORswWCyazxmzRNFg0ZouFLQdLeHP1AZamHWH+RXHcMqYPLk6d23ucltcYb3Jvf77amsu9k2OtaqWv3FvIgtVZ+Lo7S0IXQrTNYtE0WCy4GA0opTr9+ctrG7j13c1U15n58p4x9O3hTUlVPRuyjrI2s5h1+4+ybE/hifJDIv14ZGo/LkkIJSbYq83Hv6BfKLOGRfD379L55/d7+HxzDn+bnsD4uBaHZneItLxyjAbFc9cO5qLnV1nVSq9tMPP4kjSigzy5Y3yfDonLqoSulJoKvAgYgbe01v9qpdxM4AtguNZaBpkL0cHqTRb2FVSQnldOWl4ZaXnl7M4vp6rejFLg6mTAzdl4yvcxMUE8NKUvnq7t355rMFu49+Ot7C+q5P3bRtC3hzcA/p4uXDqoJ5cO6gnA4dIa0g6XMTjC75y6TaKCPHn7luEs213Ak9+lc9Pbm7h0YA/+fFl/IgI6vj87Pb+c2GAvIgM9mDMiko82HGyzlf7m6iwOHq3mw9tH4Opk7JC42vyLKqWMwKvAxUAusFkptURrnX5aOW/g98DGjghUCPGbn9KO8NKyDPYVVNBgbuxv9nQx0r+nDzOHhRPi40Zdg5lak6Xxe4OFOpOZspoG3l+fzbI9BTx9TVK7fuzXWvPnRTtZk1HMMzMTGRsb1GrZXn7u9PJzP+/nvLB/KGNjg3hrTRavrMjkh11H6OXnTkKYDwPCfBjQs/F7Lz/3dv20kpZXxpiYxt/vnkkxfLLpEK+syODpmUktls85Vs0rKzKZNqhnh36SsOYtegSQqbXOAlBKfQbMANJPK/d34GngwXaNUAhxiup6E48u2omvuzO3j4smIcyHhDAfogI9MRjaTlqbs4/x4Bfbuf7NDdwyJoqHp/bFw+X8W+uvLM/kiy25/P7COGYlR5z341nLzdnIfRfEcdXQcBanHiY9r5z0/HJ+3l3A8f17fN2dGR0dyP9OO/8WfHFlHQXldSSE+QAQ6uPGnBGRfLjhIPdNjmuxlf7Et+kYDYrHLu9/Xs/dFmv+ir2AnJOOc4GRJxdQSg0BIrTW3ymlJKELh9VgtrQ4SuNMDh2t5u21WVyRFMaw3v7n3VL8dFMOx6rqWXDTMJKjAs76/sOjAvhh/nie/nEv763LZuXeQp6ZlcTw0x4r51g16/cf5df9xaRklxDq48rE+BAmxAeRGO6H8aQ3j6+35fLsz/u4emgvHrgo7rx+v3PVy8+d302KPXFcXW9id34F6fnlpOeVsSQ1j0ueL+LhqX25eXTUKfGfjbS8cgAGNCV0OHMrfdnuAn7ZXcCjl/ajp+/5fyo5E2sSeku/9YkxRUopA/A8cEubD6TUPGAeQGRkpHURCtFFrMko4vb3U/jkjpFnlUhfWZHBwpRc3l9/kEG9fLl1bBSXJ4ad08iM2gYzC1bvZ1R0wDkl8+M8XJz42/QEpiT04KEvt3PtG+u5fWwfBoX7si7zKOuyisk5VgNAsLcrI6ICOFxa0zRaZR9+Hs6MjwtmQlwQ3m5OPPzlDkZHB/KvqxNtciG2JR4uTgzr7c+w3v4A3HdBHH9etJMnvk3nux35/PuaRGJD2r4Ie7rjI1wSevqeONdaK722wczfvk0jNsSLW8d2zIXQk1mT0HOBkz8/hQN5Jx17AwOBlU1/yB7AEqXU9NMvjGqtFwALAJKTk2UzU2E3qupM/OmrndSbLLy7LtvqZFpR28C32/O5cnAYyVEBvPvrAf6wcDtP/bCHG0f25oZRkQR5uVodx5dbcikor+PZWYPP9Vc5xeiYQH66fwL//H43b609AICPmxOjogO5Y1w0Y2ICiQ3xOpGkS6rqWZNZzKq9RazOKOLb7Y2pIC7Ei9dvGtbpwwfPRi8/d967dTiLth7mye/SueylNcy/MI55E6LP6lNXel454f7u+Ho4n3L+d5Ni+PS0VvprK/eTc6yGT+4Y2Sl1Y01C3wzEKaX6AIeB2cCc4zdqrcuAE1c/lFIrgQdllIvoynbmlhEb4oW7i3WjDZ75aS95ZTWMjg5kadoRiivrrErE3+3Ip6bBzM1johga6c+cEZGsySzm3V8P8Pwv+3h1RSbXDg/nb1ck4NRGUmkwW3h91X4GR/gxNrb9LmZ6ujrxj6sGMWdkJGaLJiHMt9XuCH9PF6YnhTE9KQytNbvzK9h04ChTB/bE1925xft0JUoprhkWzoT4YB5fsotnftrL9zvz+b9ZSfTv6dP2A9CY0Ae0UDbEx405IyP5YH1jK12jeW3Vfq5ICmPMGS4Qt6c23zK01ibgPuAnYDewUGudppR6Uik1vaMDFKK9LU49zBWvrOXOD1KoN1naLJ+SfYz312czd3QUf78ygQaz5ouUXKue6/PNOcSFeDEkwg8Ag0ExMT6Y924dwS9/mMg1w3rx0YZDvLQsw4q488gtqeF/LojtkG6NhDDfZn3jZ6KUYkCYD7eM7UMPX9vM2DxXwd6u/OeGYbx+41AKyuu4/b3NaN12p0FVnYkDR6tICPNt8fZ7JsbgZFC8vDyDvy1Jw9mgeGxax14IPZlVnwG01t9rreO11jFa6380nfur1npJC2UnSetcdFXpeeU88tUOIgM8WJtZzJ8W7TjjC7m2wczDX+0gzNedh6b0JTbEmxF9Avh00yEsbUxP33ukgtScUq4bHtFiAo4N8eKpqxOZOSycl1dksjajuNXHMls0/1mRSf+ePlzQL8T6X1ic0dSBPXl4Sl/yymrZc6SizfK788vRmhMjXE53vJX+xZZcVuwt4oGL4wntxOUJum6HlxDtrKSqnrs+SsHP3YWv7hnDAxfFs2jrYZ7/pfXW8cvLM8gqquKpqwedmIhzw8hIDh2r5tf9rSdgaGydOxsVVw8NP2O5J2ckEBvsxf2fp1JYUdtime935pNVXMV9kzumdd6dTYhvHBe+el9Rm2XT8xtHuCT0ar175p6JMbg6GYgP9WLumKh2idFaktBFt2AyW/j9Z9soKKvjtRuHEuztyu8vjOXa5HBeWpbB55sPNbvPrsNlvL4qi5lNfa7HTUnogb+HM59sbH6f4+pMZhZty+WSAT0I8HQ5Y2weLk68esNQKusamP9parOFqSwWzasrMokJ9mTqwB5n+ZuLtvTwdaNvqDerrEjoaYfL8fdwpscZWt0hPm58cfdoPrx95FkPcT1fktBFl6a1ZtW+ItZmFLfZxXEmzyzdy5qMYv5+ZQJDIhuHsSml+MdVgxgfF8Sfv951ygu6wWzh4S934O/h0qwP1M3ZyDVDw/k5vaDVFvXStAJKqxu4brh1E2ziQ715csZA1mcd5eXlp35iWLankD1HKrh3cuw5j50WZzaxbzCbs49R1bSCYmvS8stICPNt81NSYrhfp3a1HCcJXXSaw6U1bMg62ubSqMetyyzmyld/Ze47m7jx7Y1MfnYlb6zaz7Gq+rN63u925PHGqixuHBXJdcNPnf/gbDTwnxuGEh/qze8+2nJijPGba7JIzy/n/12ZgJ9H8xb29SMjMVlavzi6MCWHXn7ujDuL0Q2zhoVz9dBevLgsg3VN3Tlaa15ZnkFEgDvTk8KsfixxdibGB9Ng1mzIOtpqmQazhX1HKlvtP+8KJKGLTrFybyFTnl/N7AUbGPOvZfz7xz1kFla2WHZHbik3vrWROW9tpKiijqdnJvLS9UMI9XHjqR/2MOqfy7j/s22kZB9rc2TCniPlPPTFDob19uevlye0WMbbzZl3bxmOj7szt767mTUZRbzwSwaXDuzB1IE9W7xPTLAXo6ID+Gxz84ujOceqWZNRzKzkcKum4h+nlOLvMwYSHeTJ/M9SKaqoY01GMdtzy7hnYmybwxrFuUuO8sfd2XjGbpfMwkrqzZZTZoh2NbJ8ruhw76/L5olv0+jbw4c7x/fhux35LFidxWsr9zMk0o9ZwyK4PKknRRV1PLt0L9/vPEKApwt/uXwAN4yMxM25caz49KQw9hVU8PGGgyzaephvUvPo18ObiX2DiQr0pHeAB5GBHvT0dcdoUJRW1zPvgy14uznx2g1Dzzixo4evG+/dOoKZr63jprc34evuzBMzWn4DOG7OyN78/tNtrMksZuJJfexfpOSgFOe0nomnqxP/uWEYM15dywOfp1JvstDDx41rhvU668cS1nN1MjI6JvCMCf34lP/Whix2BZLQRYcxmS08+V06H6w/yEX9Q3lx9mA8XZ24emg4heW1fJN6mC9Scvnz1zt54ts0TBaNm5OB+RfGccf4Pni7NZ+oEh/qzRMzBvLw1H4s2Z7HZ5tzeHdtNvXm38aTuxgNhAe4Y7Fo8stq+GzeaKuWaO3bw5s3bhrGPR9v5ckZCYR4n/k+UxJCCfB04ZONB08kdLNF88WWXMbHBZ/zaoJ9e3jzxPQEHvlqJwCPXzGgw5ZbFb+ZGB/M8j2FZBdXERXk2ez2tLwy3J2N9Gnhtq5CErroEOW1Ddz78VbWZBQzb0I0j0ztd8oFvRAfN+ZNiOHO8dHsPFzGoq2HcXU2cOf4aKtmYHq6OnH9iEiuH9E4uzG/rIZDR6s5eKya7KNVHDpaTX5ZLQ9cHH9iLQ9rjIkNYttfLraqq8TVycisYeG8tfYABeW1hPq4sTqjiPyyWv5y+QCrn7Ml1yZHsPVgKeuzjjJ7uKx71BmOvymvzihqJaGX06+nd5e+MC0JXbS7Q0eruf39zRworuJfVw9i9ojWE5JSisRwPxLD/c75+YwGRbi/B+H+How550f5zdn0e18/IpI3VmexcHMO/3NhHAs35xDg6cJF/UPPKwalFP+emXhOqzuKcxMV5EnvQA9W7S3i5tFRp9xmsWh255UzY0jXvjAtCV1YzWLRbMspZWnaEdLyynEyKlydDLg6GXFxMuDqZMDZaGDJ9jxMZgsf3Dai09awsJWoIE/Gxgby2eYcrhsewc/pBdwyJqrdFmKSZN65JsQF8+WWXOpM5lO6uXJLGjeF7sr95yAJXbShwWxhQ9ZRfko7wtK0Agor6nA2Kgb09EEDdQ0W6s2Nu+I0frcQHuDBq3OGEG3F/pCOYM6I3tz7yVb+sHA7Jou2euy56Homxgfz4YaDbMkuOaUxcnw4a0uLcnUlktBFi1Kyj/HJxkP8sruA8loT7s5GJvUNZkpCDyb3C7GLlfU6y8UDQgnycmFtZjFDI/2IC/W2dUjiHI2OCcTZqFi1r+i0hN64KfTxPVK7Kkno4gSLRbNsTyGvr9rPloMl+Lg5cfGAHkxJCGVCfPCJ4YPiVC5OBmYlR/Dayv1yAdPOebo6MTwqgFX7inj0st9mCKfllREb7NXlXwOS0AX1JguLUw+zYHUWGYWVhPu788T0BGYlh7fLXpPdwR3j+mBUiumDu/ZFM9G2CfHB/OuHPRwpqz2xLHB6fjljY7r+9SB5tTqwOpOZxdvy+HZHHk4GhY+7Mz5uzvi4OzV9d6akup4P1x8kv6yW/j19eHH2YKYN6imzEs9SoJcrD07pa+swRDuY2JTQV2cUcW1yxIlNobvyDNHjJKE7oLKaBj7ZeIh3fz1AYUUd0cGeeLk6caC4ivJaE2U1DaespzI6OpB/XZPIhLggWZpVdHv9engT4u3Kqn2NCb2lTaG7KknoDiS/rIZ3f83mk42HqKwzMS42iGevTWJc7KmJWmtNTYOZ8hoTFq0JO8cZjUI4IqUad5Vaml6A2aJb3BS6q5KE7gBKqur55/e7+XrbYTQwbVBP5k2IZmCvlv8BlVJ4uDhJ/7gQrZgQH8wXW3LZnlva6qbQXZG8ou1cak4p9368laKKOm4c1Zvbx/UhIsDD1mEJYdfGxQZhULBqbxHpeeVdesnck0lCt1Naaz7acJAnv0snxLtxh5SkiHOfPi+E+I2/pwtJEX78uOsIB45WceUQ+1jtUoYy2KHqehMPfJ7KXxanMTY2iO/+Z5wkcyHa2cT4YPYWVKB1158hepwkdDuTWVjJjFd+ZfH2PP54cTzvzB2Ofxt7Vgohzt7J+8ieaVPorkS6XOyE1ppvd+Tz6Fc7cHM28uFtIxkX1/UnOghhr5LC/fB1d8ZoUGfcFLorkYTexWmtWbm3iBeXZZCaU8qw3v68MmcIPX1lqKEQHcloUMwdE0Vtg9lu5mdIQu+itNYs213IS8sz2JFbRi8/d/5x1UCuTY6QJVWF6CR/uDje1iGcFUnoXYzFovl5dwEvLcsgLa+ciAB3/n3NIK4aEt5ua2wLIRyTJHQb0lqTW1LD3iMV7C2oYM+RCnbklnLwaDVRgR48MzORK4f0kha5EMIqktA7mcls4eXlmazJKGJfQSWVdaYTt/Xyc6dfD2/mXxjH9KQwWSBLCHFWJKF3oso6E/d+vJVV+4pI7u3P1UN70beHN/16eBMf6t3iLvdCCGEtSeid5EhZLbe9t5m9BRX886pBzBkpGyEIIdqXJPROsOdIObe+u5nymgbenpvMpL4htg5JCOGAJKF3sDUZRdzz0VY8XY0svHt0l981XAhhvyShd6CFm3P489c7iQ3x4t1bh8tkICFEh5KE3gGq60288EsGC1ZnMT4uiP/cMFQueAohOpwk9HaktWbJ9jye+n4PR8prmTMykiemJ8g4ciFEp7AqoSulpgIvAkbgLa31v067/W7gXsAMVALztNbp7Rxrl7Y9p5Qnv0tny8ESBvXy5eU5QxgeFWDrsIQQ3UibCV0pZQReBS4GcoHNSqklpyXsT7TWrzeVnw48B0ztgHi7nMLyWp7+aS9fbsklyMuVp2cmMnNoOAaDfSzmI4RwHNa00EcAmVrrLACl1GfADOBEQtdal59U3hPQOLg6k5l31mbzyvIM6s0W7poYzX2TY6WvXAhhM9Yk9F5AzknHucDI0wsppe4F/gC4ABe09EBKqXnAPIDISPucWKO1ZvmeQv7+XTrZR6u5qH8oj03rT1SQp61DE0J0c9Yk9Jb6Dpq1wLXWrwKvKqXmAI8Bc1soswBYAJCcnGx3rfj9RZX8/bt0Vu4tIibYkw9uG3HKriZCCGFL1iT0XCDipONwIO8M5T8DXjufoLqaitoGXl6eyTtrD+DubOSxaf2ZOyZKRq8IIboUaxL6ZiBOKdUHOAzMBuacXEApFae1zmg6nAZkYEcKK2rZerCE6noz1fVmaurN1DSYm45NfL/zCEer6pg1LJyHpvQj2NvV1iELIUQzbSZ0rbVJKXUf8BONwxbf0VqnKaWeBFK01kuA+5RSFwENQAktdLd0RUcr63hjdRYfrM+mtsHS7HYXowF3FyP9enjz9txkkiL8Oj9IIYSwklXj0LXW3wPfn3buryf9PL+d4+pQZdUNvLkmi3d/PUBNg5krB/fiptG98fdwwcPFiLuLEXdno6xHLoSwK91qpmhlnYl31x5gwZosKmpNTEvsyQMXxREb4m3r0IQQ4rx1m4S+OfsY8z5IoaS6gYsHhPLARfEMCPOxdVhCCNFuuk1Cf+/XbAxKsfjesdIXLoRwSN2ik9hs0azNLOaCfiGSzIUQDqtbJPTtuaWU1TTIJCAhhEPrFgl9zb5ilIJxsUG2DkUIITpMt0joqzOKSOzli7+ni61DEUKIDuPwCb2spoHUnFLpbhFCODyHT+jr9xdjtmhJ6EIIh+fwCX3VvmK8XJ0YLKNbhBAOzqETutaa1fuKGBMTKCsjCiEcnkNnuQPFVRwurZHuFiFEt+DQCX31viIAJsRJQhdCOD7HTugZxUQFehAZ6GHrUIQQosM5bEKvM5lZv/8o46V1LoToJhw2oW85WEJNg1n6z4UQ3YbDJvTV+4pxMihGxwTaOhQhhOgUDpvQ12QUMbS3P16u3WaFYCFEN+eQCb2ooo60vHImSneLEKIbcciEvjZThisKIbofh0zoa/YVE+DpQoJsMSeE6EYcLqFbLJrVGcWMiw3CYFC2DkcIITqNwyX03UfKKa6sk+GKQohux+ES+pqMYgDGx8nuREKI7sXhEvrqfUX06+FNqI+brUMRQohO5VAJvbreREp2ibTOhRDdkkMl9J/TC6g3W6T/XAjRLTlMQv9ww0H+uHA78aFeDI8KsHU4QgjR6ex+Xny9ycLfvk3jk42HmNw3mBevH4Kbs9HWYQkhRKez64ReXFnH7z7ayqbsY9wzKYYHL+mLUcaeCyG6KbtN6LsOl3HXh1sorqzjxdmDmTG4l61DEkIIm7LLhP7t9jwe+nI7/h4ufHn3GAaF+9o6JCGEsDm7S+hfb8vlgc+3k9zbn9duHEawt6utQxJCiC7B7hL6hf1D+f0Fsdx7QSyuTnLxUwghjrO7hO7j5swfLulr6zCEEKLLcZhx6EII0d1JQhdCCAehtNa2eWKlioCD53j3IKC4HcNxFFIvzUmdNCd10pw91UlvrXWL65vYLKGfD6VUitY62dZxdDVSL81JnTQnddKco9SJdLkIIYSDkIQuhBAOwl4T+gJbB9BFSb00J3XSnNRJcw5RJ3bZhy6EEKI5e22hCyGEOI0kdCGEcBB2l9CVUlOVUnuVUplKqT/ZOh5bUEq9o5QqVErtOulcgFLqZ6VURtN3f1vG2NmUUhFKqRVKqd1KqTSl1Pym8922XpRSbkqpTUqp7U118kTT+T5KqY1NdfK5UsrF1rF2NqWUUSm1TSn1XdOxQ9SJXSV0pZQReBW4FBgAXK+UGmDbqGziPWDqaef+BCzTWscBy5qOuxMT8EetdX9gFHBv0/9Gd66XOuACrXUSMBiYqpQaBfwbeL6pTkqA220Yo63MB3afdOwQdWJXCR0YAWRqrbO01vXAZ8AMG8fU6bTWq4Fjp52eAbzf9PP7wJWdGpSNaa3ztdZbm36uoPHF2otuXC+6UWXToXPTlwYuAL5sOt+t6gRAKRUOTAPeajpWOEid2FtC7wXknHSc23ROQKjWOh8akxsQYuN4bEYpFQUMATbSzeulqWshFSgEfgb2A6Vaa1NTke74GnoBeBiwNB0H4iB1Ym8JvaUNQ2XcpThBKeUFfAXcr7Uut3U8tqa1NmutBwPhNH7C7d9Ssc6NynaUUpcDhVrrLSefbqGoXdaJva2HngtEnHQcDuTZKJaupkAp1VNrna+U6klji6xbUUo505jMP9ZaL2o63e3rBUBrXaqUWknj9QU/pZRTU4u0u72GxgLTlVKXAW6AD40tdoeoE3troW8G4pquSLsAs4ElNo6pq1gCzG36eS6w2IaxdLqmftC3gd1a6+dOuqnb1otSKlgp5df0sztwEY3XFlYAM5uKdas60Vo/qrUO11pH0Zg/lmutb8BB6sTuZoo2vbO+ABiBd7TW/7BxSJ1OKfUpMInGJT8LgMeBb4CFQCRwCJiltT79wqnDUkqNA9YAO/mtb/TPNPajd8t6UUol0niBz0hj422h1vpJpVQ0jQMKAoBtwI1a6zrbRWobSqlJwINa68sdpU7sLqELIYRomb11uQghhGiFJHQhhHAQktCFEMJB2GzYYlBQkI6KirLV0wshhF3asmVLcWt7itosoUdFRZGSkmKrpxdCCLuklDrY2m3S5SKEEA6izYTe2rKkp5W5QSm1o+lrnVIqqWPChXqThcWph5HhlkIIcSprulyOL0u6VSnlDWxRSv2stU4/qcwBYKLWukQpdSmN+/ON7IB4WbQ1lz8t2olSiulJYR3xFEIIYZfabKGfYVnSk8us01qXNB1uoHEthA4xKzmCpAg//rYkjaOVdjeRSwghOsxZ9aGftixpa24Hfmjl/vOUUilKqZSioqKzeeoTjAbFMzMTqaw18dclaef0GEII4YisTujWLEuqlJpMY0J/pKXbtdYLtNbJWuvk4OAWR91YJT7Um99fGMt/d+Tz4678c34cIYRwJFYl9FaWJT29TCKNO4DM0Fofbb8QW3bXxBgSwnx47Js0SqrqO/rphBCiy7NmlEtry5KeXCYSWATcpLXe174htszZaOCZmUmUVtfz9+/S276DEEI4OGta6GOBm4ALlFKpTV+XKaXuVkrd3VTmrzRu4/Sfpts7ZcbQgDAffjc5lkXbDrN8T0FnPKUQQnRZNls+Nzk5WbfHTNF6k4UrXl5LaU09Sx+YiK+7cztEJ4QQXZNSaovWOrml2+x+pqiLk4FnZiVSXFnPU9/vtnU4QghhM3af0AESw/2YNyGazzbnsCbj3IZDCiGEvXOIhA4w/8I4YoI9+dNXOykor7V1OEII0ekcJqG7ORt57trBlFbXc+0b68ktqbZ1SEII0akcJqEDJEX48dEdIympqmfW6+vJKqq0dUhCCNFpHCqhAwyJ9OezeaOpN1m49o0N7DnS4qRWIYRwOA6X0KFxfPrnd43GaIDZCzawI7fU1iEJIUSHc8iEDhAb4sUXd43B282JOW9uZNOBY7YOSQghOpTDJnSAyEAPFt41mhAfV25+ZyOr98mQRiGE43LohA7Q09edhXeNJr/wYcQAABvQSURBVCrQk9vf38yH67NltyMhhENy+IQOEOTlyud3jWZ8XDB/WZzGH7/YTm2D2dZhCSFEu+oWCR3A192Zt25O5v6L4li09TDXvLaOnGMyVl0I4Ti6TUIHMBgU918Uz9tzkzl0rJorXlkr/epCCIfRrRL6cRf2D+Xb+8YR6u3G3Hc38eqKTOlXF0LYvW6Z0AGigjz5+t4xXJ4YxjM/7eWO91OkC0YIYde6bUIH8HBx4qXZg/nr5QP4dX8xFz63iqd/3ENlncnWoQkhxFmzZgu6CKXUCqXUbqVUmlJqfgtllFLqJaVUplJqh1JqaMeE2/6UUtw2rg8rHpzEtEE9+c/K/Ux6ZiULN+dgtkg3jBDCfljTQjcBf9Ra9wdGAfcqpQacVuZSIK7pax7wWrtG2Ql6+rrz/HWD+ebesUQGuPPwVzuY/spaNmR1+H7XQgjRLtpM6FrrfK311qafK4DdQK/Tis0APtCNNgB+Sqme7R5tJxgc4cdX94zhpeuHUFJVz+wFG7jzgxRZD0YI0eU5nU1hpVQUMATYeNpNvYCck45zm87ln3b/eTS24ImMjDy7SDuRUorpSWFcMiCUN1dn8eaaLH5OL2B8XBD3TY5lRJ8AlFK2DlMIIU5h9UVRpZQX8BVwv9b69DVpW8puzTqgtdYLtNbJWuvk4ODgs4vUBtycjfzPhXH8+qcLeGRqP3bnl3Pdgg3Men09K/YUylBHIUSXYlVCV0o505jMP9ZaL2qhSC4QcdJxOJB3/uF1Dd5uztwzKYa1j1zAkzMSyC+r5db3NjPtpbX8uOuIJHYhRJdgzSgXBbwN7NZaP9dKsSXAzU2jXUYBZVrr/FbK2i03ZyM3j45ixYOTeGZmIrUNZu7+aAvXvLaOlGxZnlcIYVuqrdalUmocsAbYCViaTv8ZiATQWr/elPRfAaYC1cCtWuuUMz1ucnKyTkk5Y5Euz2S28OWWXJ77eR+FFXVMSQjl4an9iAn2snVoQggHpZTaorVObvE2W3UXOEJCP6663sTbaw7wxuosahrMXD8igvkXxhPs7Wrr0IQQDkYSeicprqzj5WUZfLzxEC5OBm4c1ZvrhkdIi10I0W4koXeyA8VVPLt0Lz/uOoLJoknu7c+1wyOYNqgnnq5nNVJUCCFOIQndRooq6li0NZfPU3LIKqrC08XIFUlhXDs8giERfjKWXQhx1iSh25jWmpSDJXy+OYf/7sinpsHMqOgA/nHVIOmOEUKcFUnoXUhFbQNfNY2MqW2wcO/kWO6eFI2rk9HWoQkh7MCZEnq3Xj7XFrzdnLllbB9++eNELkkI5flf9jHtpbVslnHsQojzJAndRkK83XhlzlDevXU4NfVmZr2+nkcX7aSsusHWoQkh7JQkdBub3DeEn/8wgTvH9+HzzYe48LlVfLg+m+p62WRDCHF2pA+9C9l1uIy/Lt7F1kOl+Lg5cf3ISOaOjiLMz93WoQkhugi5KGpHtNZsPVTCO2uz+WFXPkopLh3Yg9vG9WFopL+twxNC2NiZErrMculilFIM6x3AsN4B5JZU88H6g3y66RDf7chnSKQf8y+MY1LfEFuHKYTogqSFbgeq6kx8tTWXN9dkkXOshonxwfzvtP7Eh3rbOjQhRCeTLhcHUW+y8MH6bF5alkFlnYnrR0TywMXxBHnJImBCdITC8lrMWtPTt+tcx5Jx6A7CxcnAHeOjWfXQZG4eHcXnm3OY9MxKXlu5n9oGs63DE8LhPLAwlXs/3mrrMKwmCd0O+Xu68LfpCfz0wARGRQfy7x/3cOGzq3hj1X4KymttHZ4QDsFktrD1YCl7jlRgsdjHrmSS0O1YTLAXb81N5pM7RtLD142nftjD6KeWcfM7m1icepiaemm1C3Gu9hZUUNNgprrezBE7aSjJKBcHMCY2iDGxQWQVVfL1tsMs2nqY+Z+l4uXqxGWDenDN0HBG9AmQ1R2FOAupOaUnft5fVGkX80Gs2VP0HaVUoVJqVyu3+yqlvlVKbVdKpSmlbm3/MIU1ooO9+OMlfVnz8GQ+vXMUlw7swX935HPdgg3c8X4KeaU1tg5RCLuReqgUd+fGRfMyCyttHI11rOlyeY/GvUJbcy+QrrVOAiYBzyqlXM4/NHGuDAbF6JhAnpmVRMpjF/PYtP6s23+Ui59bxfvrsjHbSX+gELaUmlPK6JhAfNyc2F/kIAlda70aONNSgBrwbtoo2quprCxE0kW4uxi5Y3w0Sx+YwLCoAB5fksbM19exr6DC1qEJ0WWV1zaQWVTJ4Ag/YkK82F9YZeuQrNIeF0VfAfoDecBOYL7W2tJSQaXUPKVUilIqpaioqB2eWlgrIsCD928dzgvXDebg0WqmvbSG55buleGOQrRgR04ZWsOQSD9ig73IdJQWuhWmAKlAGDAYeEUp5dNSQa31Aq11stY6OTg4uB2eWpwNpRRXDunFL3+YyBWJYby0PJPLXlzD4tTDmMwtvgcL0S2l5pQAkBje2EIvqqijrKbrL23dHgn9VmCRbpQJHAD6tcPjig4S4OnCc9cN5oPbRmA0KOZ/lsqk/1vJB+uzZaijEDT2n8cEe+Lr7nxim0h76Edvj4R+CLgQQCkVCvQFstrhcUUHmxAfzE/3T+Ctm5MJ8Xblr4vTGPvv5by8LIPS6npbhyeETWitSc0pZXBE4+qmsSFNCd0ORrq0OQ5dKfUpjaNXgpRSucDjgDOA1vp14O/Ae0qpnYACHtFaF3dYxKJdGQyKiwaEcmH/EDZnl/D6qv08+/M+Xlu1n1nDwhkfF8zQ3v4EeMrAJdE95JbUUFxZz+BIPwAi/N1xNir2F3X9C6NtJnSt9fVt3J4HXNJuEQmbUEoxok8AI/oEsDu/nDdW7eeTTYd4f/1BAKKDPUnu7c+w3v4M6x1ATLCnTFQSDmlb04SiIRGNCd3JaCAq0NMuxqLLTFHRTP+ePrwwewj/uiaRHbllpBw8xpbsEpamF7AwJReAIC8XLuofypSBPRgTE4irk9HGUQvRPlIPleLmbKBvj9+Wp44N8WLvka4/1FcSumiVm7PxRKsdGvsWs4qr2JJdwprMYr7bkc9nm3PwcnXign4hTEnowaS+wXi6yr+VsF+pOSUM6uWLs/G3S4wxwV4sTS+g3mTBxanrLoElrzxhNaUUMcFexAR7ce3wCOpMZtZlHuWntCMsTS9gyfY8XJwMTEnowf9e1p8evm62DlmIs1JvsrArr5y5o3ufcj42xAuzRXPwaBVxXXhjGUno4py5OhmZ3C+Eyf1C+MdVmpTsY/yw6wifbT7Eyj2F/Hlaf2YPj5C+dmE39hwpp95kOTHC5biThy525YTedT87CLtiNChGRgc2rtN+/wQG9vLl0UU7mfPmRrKLu/7oACHgtxUWj49wOS462BOgy490kYQu2l3vQE8+uXMk/7p6ELsOlzH1xdW8uTpLZqOKLm/boVKCvV0JO6270NPViTBfty4/0kW6XESHUEoxe0Qkk/qG8Ng3u/jH97v5bkcev78wDk9XJ5yNCieDASejwtlowMmgcHM24uFixMPFqUtfeGqJ2aLZnV9OQpiPdDHZsdScUoZE+LX4N4wJ8erys0UloYsO1cPXjTdvHsZ3O/L525I0bn/fuo3BnQzqRHL3cDXSv4cPVw7pxcT4YKuTvdmiMRo6J7k+/dMe3liVxfPXJXHVkPBOeU7Rvkqr6zlQXMWs5Jb/fjHBXnyRkoPWusu+aUtCFx1OKcUVSWFM7BvMnvwKTGYLDRbd+N2sMVksNJgt1DVYqK43U11voqreTE29mao6E5V1JjZkHeW/O/Px83Dm8sSeXDWkF0Mj/U95YVXVmUg5WML6/UfZkHWUtLwyLhvUk2dmJnVoi39z9jEWrM7C2aj45/d7uKh/KN5uzh32fKJjnOg/j/Br8faYEC+qmraj6+nbNXcvkoQuOo2Pm/OJMe1nq8FsYW1GMV9vO8yXW3L5aMMhIgM8mDE4DLNFsz7rKDtzyzBZNM5GRVK4H1ckhrFo22FKqxt4/cZhuLu0Pfkps7CCl5dncse4aAaF+7ZZvrLOxB8WphLh78G/rhnEDW9t5IVfMvjL5QPO6fc82dHKOvLLahnYq+04xPlLzSlFqcYVFlsSc/zCaGGVJHQhzoez0XBiiGRlnYmfdh3hm9TDvLoiE4NSJIb7ctfEaEZFBzKstz8eLo3/2iP6BPDo1zu56e2NvH3LcHzdW245a635cksuf12cRk2DmdX7ivji7tHEhpx5iNo//ptObkkNC+8azfCoAGYPj+S9ddlcmxxxykzDs1VZZ+K6BRvILq5ixYOTiAjwOOfHEtZJzSklPsQbr1Ymxh1fpCuzsIJxcUGdGZrV7OvKkxCAl6sT1wwL58PbR7LlsYvZ/vglLPrdWB6a0o/xccEnkjnA7BGRvHL9ULbnlnL9gg0UVdQ1e7zGVvZ2HvpyB0kRviy8azRGg4Gb3t5Ebkl1q3Es31PAp5tymDchmuFRjZ88Hp7SF283J/6yeBdan9tWfxaL5o8LUzlQXIVBKV5alnFOjyOsd3yFxSGRLbfOAYK9XPF2c+rSQxcloQu75u/p0uZSA9MSe/LW3OEcKK7i2jfWc/ikzbLT8sqY/vJaFqce5oGL4vn4jlGM6BPAh7ePoKrOxE1vb6K4svmbwLGqeh7+cid9Q735w8Xxp8Tz8JR+bDpwjCXb887pd/rPykx+Sivgz5f158ZRvflqay5ZXXx0hb3LPlpNaXVDq/3n8NtM6a480kUSuugWJsYH89EdIyiurGPma+vILKzkg/XZXPWfdVTVm/jkzlHMvyjuxKiY/j19eOeW4eSX1TD3nU2U1/62W43Wmse+2UlZTT3PXze42cJk1w2PIDHcl//3391U1J7dLjcr9hTy7M/7uHJwGLeNjeKeSTG4Ohl5UVrpHer4DkWnTyg6XWyIV5ceiy4JXXQbw3oH8Pm80TSYNZe9uKZxQ4+YQL7//XhGRQc2K58cFcDrNw5jX0EFd7yfcmL/1SXb8/h+5xEeuDieAWHNd1s0GhRPzhhIcWUdL/5ifSLOLq7i959to38PH566OhGlFMHertwyNool2/PsYrU/e5V6qBRPFyNxbVwziQn2orCi7pQ3+K5EErroVgaE+fDF3aNJivDlfy/rz9tzhxPo5dpq+Ul9Q3ju2sFszj7GfZ9sJedYNX/5ZhfDevtz14SYVu83OMKP2cMjeHddtlWJuKrOxLwPUzAaFG/cdOqInLsmROPl4sTzP+87u19WWC01p5RB4b5tzlv4baRL12ylt5nQlVLvKKUKlVK7zlBmklIqVSmVppRa1b4hCtG++gR58sXdY7hzQjQGKyYeXZEUxt9nDOSX3YVc9uIaGsyaZ2cltfnif2hKP7zdnPhrGxdItdY89OV2MgsreeX6oc1GtPh5uHDbuD78mHaEXYfLrPslhdVqG8yk55c3W5CrJSe2o+uiF0ataaG/B0xt7UallB/wH2C61joBmNU+oQnRddw4qjcPTelLRZ2Jxy7vT1SQZ5v3CfB04cFL+rKxjQukr6/K4vudR/jTpf1aHQ53+/g++Lo785y00ttdWl45DWZ9xhEux0UGeDRtR3fmFvquw2UUlNe2V4hWs2YLutVKqagzFJkDLNJaH2oqX9g+oQnRtdw7OZaZw8IJ9bF+nffrR0Ty+eYcHvt6F++ty8aoFAaDwqAa+9oNSvFrZjGXJ/bkzvHRrT6Oj5szd02M5ukf97LlYAnDerfdmjxbBeW1eLo6tToOuyVaa9btP0pShN9Z3a8r2Xao8YLokDOMcDnOmu3o8stquOa1dfTyd+f734/HzbnzdvNqjz70eMBfKbVSKbVFKXVzawWVUvOUUilKqZSioqJ2eGohOtfZJHNoTNrPXpvE6JhAvFwbFx1TgMUCtQ0WKmpNTEsM4+mZiW2uD3LLmCiCvFx47ue95/EbtOz7nflMfGYFk55ZyeLUw1aNod9fVMnsBRu44a2N/M8nW8953L0tVNQ28NWWXOa+s4mnfthDnyBPQqz827Y1dPHFXzIwWzRZRVU8u7T9/1Zn0h5vqU7AMOBCwB1Yr5TaoLVu9tlQa70AWACQnJxsP399Ic5DfKg3C25OPu/H8XBx4p5Jsfz9u3TW7S9mTMz5z1bUWvPK8kye/XkfQyL9sFg08z9LZWFKDk/OGHhiY4eT1ZssvL5qP68sz8TN2cCVg8P4JjWPt9ce4I4zfMqwtdoGMyv2FLJkex7L9xRSZ7LQy8+dO8dHM2dEpNWPExPiyS+7C2gwW07Zpg4gs7CShSk5zB0TRZ3JwltrDzAloQfJUee25MXZao+EngsUa62rgCql1GogCZDOPiHa2Q0jI1mwej/PLd3H6LsDz2vVv9oGM498tYPFqXlcNaQXT109CGejgU82HeLpH/dw6QtruGtiNPdOjj3RbZCSfYxHF+0ko7CSyxN78tcrBhDs5UpVvZl//7iHkX0CrVoDpzNV1pl4eXkGH284RGWdiSAvV64fEckVSWEMjWx5qdwziQ3xwtS0Hd3pS0M8u3Qv7s5G7psci6uzkdX7injwi+18P3/8KTOYO0p7dLksBsYrpZyUUh7ASGB3OzyuEOI0bs5G7rsgjpSDJazOKD7nxymsqGX2gg0sTs3joSl9ee7aJNycjRgNiptG9Wb5HydxeWJPXl6eySXPr+bHXfn879c7mfn6eqrrzbx7y3BemTOUEG83lFI8MzORIC9X7vt061lPpuooWmu+3pbLBf+3kjdWZXFh/xA+un0kGx69gL9NT2BYb/9zekM8/qkls/DUkS6pOaX8sOsId06IJtDLFS9XJ56emUj20Wqe/rFzul7afMtQSn0KTAKClFK5wOOAM4DW+nWt9W6l1I/ADsACvKW1bnWIoxDi/FyXHMHrK/cz74MUApuWPvB0dcLbzQlPFye83JwI8XYlLtSLuBBvYkO8Trkwl55Xzh3vb6akuoHXbxzK1IE9mz1HsLcrz103mFnJEfxl8S7u/mgrBgW3j+vDHy6Ob7bcgp+HCy/OHsLsBev5yze7eP66wTZdM3zX4TIeX5LGloMlJIX78sZNwxgS2T4XkqNP2l/0OK01//5hD4GeLqd0O42JCWLu6N68ty6bqQN7tDiBrT0pW13ISE5O1ikp1m12IIQ4VVpeGV+k5FJZZzqxZnxlnYnK2sbjwoo6TJbG17ZBNQ63iwv1JsLfg882H8LHzZm35iZbtTRvvcnCV1tzGRjm22Z3ykvLMnju533836wkZg7r/I0+jlXV839L9/LppkMEeLjwyNR+zBwWbtV8g7Mx+qlljI4O5LnrBgOwel8RN7+zicevGMCtY/ucUra63sSlL67BojU/zp/Q5tpDbVFKbdFat3hRxj7HGQnRzSWE+ZIwvfXkWm+ykH20ioyCSvYVVJBRWMG+gkpW7CkkMdyX128cZvWoDhcnA9dbedHw3smxrNtfzF++2cWQSL8WL6p2hAazhY83HOT5XzKorDNxy5go7r8ovtXlks/XySNdLBbN0z/tIdzfnTkjm9eTh4sTz8xM4roF63nqh938vysHdUhMIAldCIfk4mQgPtSb+FBvpvFbl4rJbMFoUB3WHWI0KF64bgiXvria+z7Zxte/G9PiOOzS6noOHq0mPtTbqo1HWqO15uf0Av71wx6yiqsYExPI41cknNda9NaICfbkq62Nwzv/uzOfXYfLef66pGYLtR03ok8At43tw9trDzA1oWeHracuCV2IbsTJ2PHLN/XwdePZa5O47b0Unvp+N/MmxpB2uIy0vHLS88tJzys/sYSxi5OBEVEBTIgPYkJ8MH1Dva1+s9l1uIz/9990NmQdIybYk7fnJnNBv5BO6buPDfGiss5EbkkNzy7dS78e3kxP6nXG+zw0pS8r9hTyyFc7+PH+8R2yTaH0oQshOsST36bzzq8HThwrBdFBngwI8yUhzIcIfw+2HSphdUYR+woauy9CvF0ZHxfMmJhAQn3c8HV3PvHl5eaE0aA4UlbLMz/tZdG2XPw9XLj/ojiuHxHZbEx4R1qXWcyctzZy6cAe/LDrCO/ckswF/ULbvN/WQyXMfG0dN43qzRMzBp7Tc0sfuhCi0z1yaV/8PJzx93RhQE8f+vf0bjYWe1piY3dQflkNa/YVsyqjiF92F/DV1txmj6dU425VdQ0WAOZNaBwj72ODDbmPL9L1w64jDI/yZ3LfEKvuNzTSn6dnJjE2tmNGu0gLXQjRpZgtmgPFlRyraqCs5tSv8poGDEpx69gom+6zqrUm8W9Lqagz8eXdozttJihIC10IYUeMBtXm5ty2ppRiZHQg7i7GTk3mbZGELoQQ5+DNm4fZOoRmJKELIcQ5sOVM2NbIFnRCCOEgJKELIYSDsNkoF6VUEXDwHO8eBJz7UnOOS+qlOamT5qROmrOnOumttQ5u6QabJfTzoZRKaW3YTncm9dKc1ElzUifNOUqdSJeLEEI4CEnoQgjhIOw1oS+wdQBdlNRLc1InzUmdNOcQdWKXfehCCCGas9cWuhBCiNNIQhdCCAdhdwldKTVVKbVXKZWplPqTreOxBaXUO0qpQqXUrpPOBSilflZKZTR9b58dce2EUipCKbVCKbVbKZWmlJrfdL7b1otSyk0ptUkptb2pTp5oOt9HKbWxqU4+V0q52DrWzqaUMiqltimlvms6dog6sauErpQyAq8ClwIDgOuVUgNsG5VNvAdMPe3cn4BlWus4YFnTcXdiAv6ote4PjALubfrf6M71UgdcoLVOAgYDU5VSo4B/A8831UkJcLsNY7SV+cDuk44dok7sKqEDI4BMrXWW1roe+AyYYeOYOp3WejVw7LTTM4D3m35+H7iyU4OyMa11vtZ6a9PPFTS+WHvRjetFN6psOnRu+tLABcCXTee7VZ0AKKXCgWnAW03HCgepE3tL6L2AnJOOc5vOCQjVWudDY3IDrNtCxQEppaKAIcBGunm9NHUtpAKFwM/AfqBUa21qKtIdX0MvAA8DlqbjQBykTuwtobe0XqWMuxQnKKW8gK+A+7XW5baOx9a01mat9WAgnMZPuP1bKta5UdmOUupyoFBrveXk0y0Utcs6sbf10HOBiJOOw4E8G8XS1RQopXpqrfOVUj1pbJF1K0opZxqT+cda60VNp7t9vQBorUuVUitpvL7gp5RyamqRdrfX0FhgulLqMsAN8KGxxe4QdWJvLfTNQFzTFWkXYDawxMYxdRVLgLlNP88FFtswlk7X1A/6NrBba/3cSTd123pRSgUrpfyafnYHLqLx2sIKYGZTsW5VJ1rrR7XW4VrrKBrzx3Kt9Q04SJ3Y3UzRpnfWFwAj8I7W+h82DqnTKaU+BSbRuORnAfA48A2wEIgEDgGztNanXzh1WEqpccAaYCe/9Y3+mcZ+9G5ZL0qpRBov8BlpbLwt1Fo/qZSKpnFAQQCwDbhRa11nu0htQyk1CXhQa325o9SJ3SV0IYQQLbO3LhchhBCtkIQuhBAOQhK6EEI4CEnoQgjhICShCyGEg5CELoQQDkISuhBCOIj/D6p3s91GS4z3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(model_rat.history.history['val_accuracy'])\n",
    "plt.subplot(212)\n",
    "plt.plot(model_rat.history.history['val_loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.2 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "y_pred_rat=model_rat.predict(X_test_n_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_confusion_matrix(y_t,y_p):\n",
    "    expected_outputs = np.argmax(np.array(y_t), axis=1)\n",
    "    predicted_outputs= np.argmax(y_p, axis=1)\n",
    "    predicted_confusion_matrix = confusion_matrix(expected_outputs, predicted_outputs)\n",
    "    return predicted_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2583,    1,  126,  219,    9,    1,    0,  112],\n",
       "       [ 898,    1,  133,  271,    7,    2,    0,   62],\n",
       "       [ 688,    1,  167,  526,   23,    3,    0,   85],\n",
       "       [ 486,    1,  170,  783,   52,   17,    0,  122],\n",
       "       [  97,    0,   24,  481,  264,  105,    0,  460],\n",
       "       [ 109,    0,   17,  320,  257,  146,    0,  922],\n",
       "       [  81,    0,    6,  163,  118,   77,    0,  926],\n",
       "       [ 238,    0,   10,  191,  109,   87,    0, 2243]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_confusion_matrix(y_rat_test_n,y_pred_rat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(cv_n, 'wordbag.pkl') ### сохранение модели предобработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_pos, 'positivmodel.pkl') ### сохранение модели определения настроения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_rat, 'ratingmodel.pkl') ### ### сохранение модели определения рейтинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}